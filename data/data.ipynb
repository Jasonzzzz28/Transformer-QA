{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import os\n",
    "import json\n",
    "import tokenize\n",
    "from io import BytesIO\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source code extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_comments(source_code):\n",
    "    comments = []\n",
    "    tokens = tokenize.tokenize(BytesIO(source_code.encode(\"utf-8\")).readline)\n",
    "    for toknum, tokval, _, _, _ in tokens:\n",
    "        if toknum == tokenize.COMMENT:\n",
    "            comments.append(tokval.strip(\"# \").strip())\n",
    "    return comments\n",
    "\n",
    "def extract_docstrings_and_defs(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        source = f.read()\n",
    "\n",
    "    tree = ast.parse(source)\n",
    "    results = []\n",
    "    module_docstring = ast.get_docstring(tree)\n",
    "    for node in ast.walk(tree):\n",
    "        if isinstance(node, (ast.FunctionDef, ast.ClassDef)):\n",
    "            name = node.name\n",
    "            docstring = ast.get_docstring(node)\n",
    "            node_type = \"function\" if isinstance(node, ast.FunctionDef) else \"class\"\n",
    "            source_lines = source.splitlines()\n",
    "            start_line = node.lineno - 1  # ast 行号从1开始，列表索引从0开始\n",
    "            end_line = node.end_lineno if hasattr(node, 'end_lineno') else start_line\n",
    "            source_code = '\\n'.join(source_lines[start_line:end_line])\n",
    "            results.append({\n",
    "                \"type\": node_type,\n",
    "                \"name\": name,\n",
    "                \"docstring\": docstring or \"\",\n",
    "                \"source_code\": source_code,\n",
    "                \"file_docstring\": module_docstring\n",
    "            })\n",
    "\n",
    "    comments = extract_comments(source)\n",
    "    return results, comments\n",
    "\n",
    "def generate_qa_from_entry(entry):\n",
    "    name = entry[\"name\"]\n",
    "    doc = entry[\"docstring\"]\n",
    "    if not doc:\n",
    "        return None\n",
    "\n",
    "    # question = f\"What does the {entry['type']} `{name}` do?\"\n",
    "    # answer = doc.strip()\n",
    "    source_code = entry.get(\"source_code\", \"\")\n",
    "    file_docstring = entry.get(\"file_docstring\", \"\")\n",
    "\n",
    "    return {\n",
    "        \"name\": name,\n",
    "        \"docstring\": doc.strip(),\n",
    "        \"file_docstring\": file_docstring,\n",
    "        \"source\": \"source_code\",\n",
    "        \"type\": entry[\"type\"],\n",
    "        \"code\": source_code\n",
    "    }\n",
    "\n",
    "def process_directory(dir_path):\n",
    "    qa_pairs = []\n",
    "    for root, _, files in tqdm(os.walk(dir_path)):\n",
    "        for file in tqdm(files):\n",
    "            if file.endswith(\".py\"):\n",
    "                full_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    entries, comments = extract_docstrings_and_defs(full_path)\n",
    "                    for entry in entries:\n",
    "                        qa = generate_qa_from_entry(entry)\n",
    "                        if qa:\n",
    "                            qa[\"file\"] = full_path\n",
    "                            qa_pairs.append(qa)\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to parse {full_path}: {e}\")\n",
    "    return qa_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:02<00:00, 25.14it/s]\n",
      "100%|██████████| 21/21 [00:00<00:00, 117.14it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 93.05it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1956.30it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 19222.29it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 46776.62it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 99.02it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 66974.91it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 27413.75it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 22733.36it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 49636.73it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 92.11it/s]\n",
      "100%|██████████| 36/36 [00:00<00:00, 67.93it/s]\n",
      "100%|██████████| 31/31 [00:00<00:00, 89.41it/s]\n",
      "100%|██████████| 29/29 [00:00<00:00, 76.03it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 101.47it/s]\n",
      "100%|██████████| 14/14 [00:00<00:00, 19.61it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 338.86it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 116.90it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 60.39it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 27.76it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 43.38it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 33.64it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 81.73it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 43.78it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 85.50it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 55.56it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 68.52it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 29.33it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 46.03it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 26.51it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 72.89it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 75.98it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 338.63it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 34.62it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 186.04it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 56.72it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 19.51it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 109.53it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 54.30it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 61.92it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 70.71it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 25.79it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 84.67it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 65.89it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 267.80it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 92.55it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 70.89it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 170.04it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 129.13it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 107.98it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 31.29it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 38.66it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 43.40it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 82.62it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 44.31it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 30.75it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 45.47it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 32.03it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 39.47it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 144.02it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 61.56it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 77.59it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 43.14it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 23.33it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 55.43it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 70.95it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 210.63it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 43.64it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 84.72it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 189.42it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 66.99it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 78.88it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 55.93it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 86.79it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 124.53it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 85.29it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 98.56it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 45.23it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 60.80it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 37.74it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 93.87it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 52.61it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 137.52it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 161.29it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 47.27it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 179.90it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 29.66it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 57.45it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 24.03it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 46.26it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 26.18it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 102.00it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 71.45it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 196.15it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 107.70it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 44.61it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 26.41it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 91.99it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 48.17it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 42.84it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 77.46it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 44.40it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 72.35it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 59.93it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 82.02it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 67.07it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 37.59it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 65.18it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 61.97it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 115.09it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 173.85it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 38.47it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 32.75it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 52.73it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 40.46it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 39.56it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 73.51it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 17.53it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 67.62it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 43.60it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 101.52it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 76.75it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 45.77it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 92.29it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 34.11it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 95.15it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 57.16it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 950.98it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 46.42it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 37.74it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 38.93it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 75.79it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 67.31it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 95.42it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 94.43it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 75.82it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 53.74it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 53.39it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 73.03it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 97.19it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 122.36it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 38.38it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 102.37it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 40.66it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 62.15it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 36.65it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 47.54it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 97.21it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 75.41it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 60.18it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 29.20it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 51.06it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 90.56it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 110.72it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 66.27it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 31.51it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 60.97it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 96.82it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 158.64it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 48.19it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 75.24it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 129.45it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 35.02it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 100.02it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 80.95it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 39.25it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 63.22it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 72.49it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 38.11it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 33.89it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00,  8.20it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 132.58it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 49.08it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 25.09it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 38.37it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 87.31it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 34.89it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 79.05it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 39.66it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 59.41it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 124.75it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 63.97it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 30.84it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 41.77it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 33.35it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 59.27it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 50.01it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 88.59it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 70.71it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 68.07it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 222.82it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 42.53it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 26.93it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 28.24it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 67.93it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 54.51it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 138.34it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 46.15it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 44.95it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 171.51it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 86.35it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 45.67it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 40.27it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 81.77it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 34.66it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 46.34it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 65.96it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 65.69it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 61.17it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 50.81it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 93.55it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 55.77it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 67.45it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 39.37it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 127.76it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 30.73it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 55.53it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 47.89it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 113.20it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 139.35it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 65.37it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 25.57it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 13.99it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 32.46it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 65.17it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 10.91it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 58.55it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 109.24it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 57.74it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 48.18it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 217.20it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 21.24it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 37.48it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 35.40it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 96.70it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 70.56it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 48.06it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 74.12it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 31.62it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 87.70it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 127.69it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 114.56it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 43.16it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 26.33it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 116.86it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 49.93it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 40.96it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 148.96it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 29.43it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 45.98it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 32.22it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 53.93it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 27.52it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 65.75it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 88.42it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 65.09it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 76.34it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 56.88it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 108.95it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 56.65it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 73.56it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 29.33it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 50.42it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 15.81it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 39.90it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 66.13it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 42.16it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 65.57it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 223.99it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 44.71it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 43.47it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 207.31it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 31.42it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 47.87it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 105.75it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 34.56it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 61.04it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 80.58it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 32.49it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 36.35it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 37.88it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 113.21it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 133.26it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 31.00it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 49.09it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 887.87it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 64.03it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 67.96it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 136.04it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 70.98it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 116.84it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 86.07it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 40.07it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 103.13it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 42.66it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 20.83it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 139.74it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 118.91it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 46.43it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 236.87it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 57.43it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 43.14it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 45.01it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 82.36it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 24.25it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 66.45it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 78.65it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 259.31it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 47.68it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 100.92it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 30.94it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 62.82it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 139.30it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 105.38it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 55.92it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 32.04it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 97.48it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 36.42it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 55.11it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 115.44it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 67.28it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 71.67it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 93.62it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 51.84it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 74.42it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 26.30it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 45.68it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 50.86it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 107.10it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 45.13it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 89.71it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 136.21it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 43.21it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 203.91it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 82.18it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 73.54it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 92.64it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 88.75it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 39.74it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 34.70it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 50.24it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 54.96it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 132.83it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 67.33it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 50.58it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 92.11it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 44.12it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 33.41it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 83.63it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 51.97it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 36.24it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 61.78it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 57.88it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 81.12it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 45.30it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 23.20it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 163.79it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 38.09it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 35.44it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 24.48it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 22.11it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 138.26it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 77.65it/s]\n",
      "364it [00:44,  8.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 12678 QA pairs.\n"
     ]
    }
   ],
   "source": [
    "directory = \"/home/cc/transformers/src/transformers\"\n",
    "qa_data = process_directory(directory)\n",
    "\n",
    "# 保存结果为 JSONL 文件\n",
    "with open(\"source_code_qa.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(qa_data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"Extracted {len(qa_data)} QA pairs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import time\n",
    "# api_key = input(\"Enter your openai api key: \")\n",
    "# api_base = input(\"Enter your openai api base: \")\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"EMPTY\"\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"http://localhost:6006/v1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_docstring(name, type, docstring):\n",
    "    # 使用 GPT 模型生成摘要\n",
    "    while True:\n",
    "        try:\n",
    "            prompt = \"\"\"\n",
    "            Summarize the following docstring, tell me what does the {type} `{name}` do.\n",
    "            Docstring:\n",
    "            {docstring}\n",
    "            \"\"\"\n",
    "            question_pool = [\n",
    "                \"What does the {type} {name} do?\"\n",
    "                \"What is the function of the {type} {name}?\",\n",
    "                \"How does the {type} {name} work?\",\n",
    "                \"What role does the {type} {name} play?\",\n",
    "                \"What is the purpose of the {type} {name}?\",\n",
    "                \"What does the {type} {name} accomplish?\",\n",
    "                \"Can you explain what the {type} {name} is used for?\",\n",
    "                \"Why do we need the {type} {name}?\",\n",
    "                \"What is the {type} {name} responsible for?\",\n",
    "                \"What task does the {type} {name} perform?\",\n",
    "                \"What kind of behavior does the {type} {name} define?\"\n",
    "            ]\n",
    "            question = random.choice(question_pool)\n",
    "            question = question.format(name=name, type=type)\n",
    "            client = OpenAI(\n",
    "                api_key=\"EMPTY\",\n",
    "                base_url=\"http://localhost:6006/v1\"\n",
    "            )\n",
    "            prompt = prompt.format(name=name, type=type, docstring=docstring)\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"mistralai/Ministral-8B-Instruct-2410\",\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "            )\n",
    "            # print(response.choices[0].message.content)\n",
    "            return question, response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            time.sleep(10)\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12678it [2:28:13,  1.43it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(\"source_code_qa.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    qa_data = json.load(f)\n",
    "\n",
    "qa_data_with_summary = []\n",
    "with ThreadPoolExecutor(max_workers=100) as executor:\n",
    "    futures = [executor.submit(summarize_docstring, qa[\"name\"], qa[\"type\"], qa[\"docstring\"]) for qa in qa_data]\n",
    "    for future in tqdm(as_completed(futures)):\n",
    "        qa = qa_data[futures.index(future)]\n",
    "        qa[\"question\"], qa[\"answer\"] = future.result()\n",
    "        qa_data_with_summary.append(qa)\n",
    "\n",
    "with open(\"source_code_qa_with_summary.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(qa_data_with_summary, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Git commit extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from git import Repo\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 仓库路径：替换为你本地 transformers 的路径\n",
    "REPO_PATH = \"/home/cc/transformers\"\n",
    "repo = Repo(REPO_PATH)\n",
    "\n",
    "output = []\n",
    "\n",
    "# 遍历最近 N 个 commit（可调整）\n",
    "for commit in repo.iter_commits('main', max_count=10000):\n",
    "    commit_data = {\n",
    "        \"commit_hash\": commit.hexsha,\n",
    "        \"author\": commit.author.name,\n",
    "        \"date\": commit.committed_datetime.isoformat(),\n",
    "        \"message\": commit.message.strip()\n",
    "    }\n",
    "\n",
    "    # 获取 diff 的简要变化（可设置为 full_diff=True 看更多上下文）\n",
    "    diffs = commit.diff(commit.parents[0] if commit.parents else None, create_patch=True)\n",
    "\n",
    "    diff_texts = []\n",
    "    for diff in diffs:\n",
    "        try:\n",
    "            diff_texts.append(diff.diff.decode(\"utf-8\", errors=\"ignore\"))\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "    diff_summary = \"\\n\".join(diff_texts)\n",
    "    commit_data[\"diff_summary\"] = diff_summary\n",
    "\n",
    "    # 构造 QA 对\n",
    "    # qa_item = {\n",
    "    #     \"question\": f\"What changed in commit {commit.hexsha[:7]}?\",\n",
    "    #     \"answer\": f\"{commit.message.strip()}\\n\\nSummary of changes:\\n{diff_summary[:1000]}...\",\n",
    "    #     \"source\": \"git_commit\",\n",
    "    #     \"metadata\": commit_data\n",
    "    # }\n",
    "    question_pool = [\n",
    "        \"What changed in commit {hash}?\",\n",
    "        \"What modifications were introduced in commit {hash}?\",\n",
    "        \"Can you summarize the changes made in commit {hash}?\",\n",
    "        \"What updates does commit {hash} contain?\",\n",
    "        \"What's new in commit {hash}?\",\n",
    "        \"Describe the differences introduced by commit {hash}.\",\n",
    "        \"What was added, removed, or modified in commit {hash}?\",\n",
    "        \"What does commit {hash} change in the codebase?\",\n",
    "        \"Which files or functions were affected by commit {hash}?\",\n",
    "        \"What's the purpose of commit {hash}?\",\n",
    "        \"How does commit {hash} alter the existing implementation?\"\n",
    "    ]\n",
    "    question = random.choice(question_pool)\n",
    "    question = question.format(hash=commit.hexsha[:7])\n",
    "    qa_item = {\n",
    "        \"question\": question,\n",
    "        \"answer\": f\"{commit.message.strip()}\",\n",
    "        \"source\": \"git_commit\",\n",
    "        \"metadata\": commit_data\n",
    "    }\n",
    "\n",
    "    output.append(qa_item)\n",
    "\n",
    "    question_pool = [\n",
    "        \"Who is the author of the commit {hash}?\",\n",
    "        \"Who made the commit {hash}?\",\n",
    "        \"Who is responsible for commit {hash}?\",\n",
    "        \"Can you tell me who authored commit {hash}?\",\n",
    "        \"Who's the person behind commit {hash}?\",\n",
    "        \"Who committed {hash}?\",\n",
    "        \"Which developer authored commit {hash}?\",\n",
    "        \"Who was the contributor for commit {hash}?\",\n",
    "        \"Do you know who wrote commit {hash}?\",\n",
    "        \"Who pushed commit {hash} to the repository?\",\n",
    "        \"Whose work is represented by commit {hash}?\"\n",
    "    ]\n",
    "    question = random.choice(question_pool)\n",
    "    question = question.format(hash=commit.hexsha[:7])\n",
    "    qa_item = {\n",
    "        \"question\": question,\n",
    "        \"answer\": f\"{commit.author.name}\",\n",
    "        \"source\": \"git_commit\",\n",
    "        \"metadata\": commit_data\n",
    "    }\n",
    "    output.append(qa_item)\n",
    "\n",
    "    question_pool = [\n",
    "        \"When was the commit {hash} made?\",\n",
    "        \"What is the timestamp of commit {hash}?\",\n",
    "        \"When exactly did commit {hash} occur?\",\n",
    "        \"At what time was commit {hash} created?\",\n",
    "        \"Can you tell me the date of commit {hash}?\",\n",
    "        \"On what date was commit {hash} made?\",\n",
    "        \"Do you know when commit {hash} was pushed?\",\n",
    "        \"Any idea when commit {hash} happened?\",\n",
    "        \"When did commit {hash} go through?\",\n",
    "        \"What's the date on commit {hash}?\",\n",
    "        \"When did they make commit {hash}?\"\n",
    "    ]\n",
    "    question = random.choice(question_pool)\n",
    "    question = question.format(hash=commit.hexsha[:7])\n",
    "    qa_item = {\n",
    "        \"question\": question,\n",
    "        \"answer\": f\"{commit.committed_datetime.isoformat()}\",\n",
    "        \"source\": \"git_commit\",\n",
    "        \"metadata\": commit_data\n",
    "    }\n",
    "    output.append(qa_item)\n",
    "# 保存为 JSON\n",
    "with open(\"qa_from_commits.json\", \"w\") as f:\n",
    "    json.dump(output, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Github issue extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from github import Github\n",
    "g_token = input(\"Enter your github token: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Github(g_token)  # 用你的 GitHub Token\n",
    "\n",
    "repo = g.get_repo(\"huggingface/transformers\")\n",
    "issues = repo.get_issues(state=\"closed\")  # 可加过滤条件\n",
    "qa_pairs = []\n",
    "# issues[0].__dict__\n",
    "# real_issues = [i for i in all_issues if not i.pull_request]\n",
    "# for issue in issues[:2]:\n",
    "#     print(\"Title:\", issue.title)\n",
    "#     print(\"Body:\", issue.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Lock\n",
    "lock = Lock()\n",
    "def process_issue(issue):\n",
    "    global qa_pairs\n",
    "    while True:\n",
    "        try:\n",
    "            comments = issue.get_comments()\n",
    "            # print(\"********************Comments********************\")\n",
    "            comments_list = []\n",
    "            for comment in comments:\n",
    "                comments_list.append(\n",
    "                    {\n",
    "                        \"comment_author\": comment.user.login,\n",
    "                        \"comment_body\": comment.body\n",
    "                    }\n",
    "                )\n",
    "                # print(\"Comment by\", comment.user.login)\n",
    "                # print(comment.body)\n",
    "            print(issue.title)\n",
    "            qa_pair = {\n",
    "                \"Issue Title\": issue.title,\n",
    "                \"Issue Body\": issue.body,\n",
    "                \"Issue Comments\": comments_list,\n",
    "                \"source\": \"github_issue\",\n",
    "                \"metadata\": {\n",
    "                    \"issue_number\": issue.number,\n",
    "                    \"url\": issue.html_url,\n",
    "                    \"created_at\": str(issue.created_at)\n",
    "                }\n",
    "            }\n",
    "            lock.acquire()\n",
    "            qa_pairs.append(qa_pair)\n",
    "            with open(\"qa_from_issues.json\", \"w\") as f:\n",
    "                json.dump(qa_pairs, f, indent=4, ensure_ascii=False)\n",
    "            lock.release()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            time.sleep(10)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrOCR (image-to-text) produces incorrect output (':') on 12th Gen Intel CPU (i7-1260P) even with simple input\n",
      "name 'json' is not defined\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 38\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqa_from_issues.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 38\u001b[0m     \u001b[43mjson\u001b[49m\u001b[38;5;241m.\u001b[39mdump(qa_pairs, f, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, ensure_ascii\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m count \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m10000\u001b[39m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'json' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28mprint\u001b[39m(e)\n\u001b[0;32m---> 43\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "qa_pairs = []\n",
    "for issue in tqdm(issues):\n",
    "    if issue.pull_request:\n",
    "        continue\n",
    "    # print(\"********************Issue********************\")\n",
    "    # print(\"Title:\", issue.title)\n",
    "    # print(\"Body:\", issue.body)\n",
    "    try:\n",
    "        comments = issue.get_comments()\n",
    "        # print(\"********************Comments********************\")\n",
    "        comments_list = []\n",
    "        for comment in comments:\n",
    "            comments_list.append(\n",
    "                {\n",
    "                    \"comment_author\": comment.user.login,\n",
    "                    \"comment_body\": comment.body\n",
    "                }\n",
    "            )\n",
    "            # print(\"Comment by\", comment.user.login)\n",
    "            # print(comment.body)\n",
    "        print(issue.title)\n",
    "        qa_pair = {\n",
    "            \"Issue Title\": issue.title,\n",
    "            \"Issue Body\": issue.body,\n",
    "            \"Issue Comments\": comments_list,\n",
    "            \"source\": \"github_issue\",\n",
    "            \"metadata\": {\n",
    "                \"issue_number\": issue.number,\n",
    "                \"url\": issue.html_url,\n",
    "                \"created_at\": str(issue.created_at)\n",
    "            }\n",
    "        }\n",
    "        qa_pairs.append(qa_pair)\n",
    "        count += 1\n",
    "\n",
    "        with open(\"qa_from_issues.json\", \"w\") as f:\n",
    "            json.dump(qa_pairs, f, indent=4, ensure_ascii=False)\n",
    "        if count > 10000:\n",
    "            break\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        time.sleep(10)\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_comment(issue_title, issue_body, comments):\n",
    "    # 使用 GPT 模型生成摘要\n",
    "    prompt = \"\"\"\n",
    "    I will give you a github issue and its comments. Tell me how to resolve the issue based on the comments.\n",
    "    If the issue isn't resolved in the comments, please just respond with '<Unsolved>'.\n",
    "    Only give your response base on the comments, don't make up any solution.\n",
    "    Issue Title:\n",
    "    {issue_title}\n",
    "    Issue Body:\n",
    "    {issue_body}\n",
    "    Comments:\n",
    "    {comments}\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        try:\n",
    "            client = OpenAI(\n",
    "                        api_key=\"EMPTY\",\n",
    "                        base_url=\"http://localhost:6006/v1\"\n",
    "                    )\n",
    "            prompt = prompt.format(issue_title=issue_title, issue_body=issue_body, comments=comments)\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"mistralai/Ministral-8B-Instruct-2410\",\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "            )\n",
    "            print(response.choices[0].message.content)\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            time.sleep(10)\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Unsolved>\n",
      "<Unsolved>\n",
      "<Unsolved>\n",
      "<Unsolved>\n",
      "Based on the comments, it seems that the issue is due to a version mismatch between different dependencies, especially with NumPy. The suggested solution is to create a new conda environment or use another environment manager to manage package versions and install the required dependencies again.\n",
      "Based on the comments, it is not possible to add a Fast image processor for EfficientFormer because it is in the deprecated folder and contributions to this model are no longer accepted beyond version 4.40.2. So, the issue remains unsolved.\n",
      "The issue has been resolved with a PR made by `@gmlwns2000`. The mismatch between the default value of `attn_temperature_tuning` in the `transformers` library and the official implementation has been corrected by changing it to a boolean type.\n",
      "**Solution:**\n",
      "\n",
      "Change the dtype of `class_labels` from `torch.int64` to `torch.uint8`.\n",
      "\n",
      "```python\n",
      "\"class_labels\": torch.ones(n_objects).to(torch.uint8),  # ensure class_labels is of the same dtype as matcher expects\n",
      "```\n",
      "\n",
      "Then retry the `matcher(outputs, targets)` call.\n",
      "Based on the comments provided, the resolution to the issue involves ensuring the `hf_transfer` module is installed. The issue arises due to changes in the `transformers` library. Specifically, the error \"ValueError: Unrecognized model in Qwen/Qwen2.5-Coder-7B-Instruct\" can be resolved by ensuring the following steps are followed:\n",
      "\n",
      "1. Ensure that the `hf_transfer` module is installed.\n",
      "2. Set the environment variable `HF_HUB_ENABLE_HF_TRANSFER` if `hf_transfer` is not installed.\n",
      "\n",
      "To resolve the issue, you can modify the modal environment setup in your Reproduction section to include the installation of `hf_transfer`, as follows:\n",
      "\n",
      "```python\n",
      "training_image = modal.Image.from_registry(\"nvidia/cuda:12.4.0-devel-ubuntu22.04\", add_python=\"3.11\") \\\n",
      "    .pip_install(\n",
      "        \"transformers==4.51.2\",\n",
      "        \"hf_transfer\",  # Add this line to install hf_transfer\n",
      "    ) \\\n",
      "    .env({\"HF_HUB_ENABLE_HF_TRANSFER\": \"1\"})\n",
      "```\n",
      "\n",
      "This ensures that both `transformers` and `hf_transfer` are available and installed correctly. If the issue persists, you might need to manually raise exceptions as mentioned in the comment from `manueldeprada`.\n",
      "Based on the comments, you can resolve the issue as follows:\n",
      "\n",
      "1. **Sharing Environment Information**: Create a new, clean environment with `conda` and install the required packages into it.\n",
      "2. **Running on Another System**: Use platforms like Google Colab to run the code on a different system and see if the segmentation fault persists.\n",
      "3. **Updating Packages**: Considering trying older or newer versions of `transformers` if updating or using other versions hasn't helped.\n",
      "\n",
      "However, since it already worked on another Docker image and the issue was resolved there, trying the aforementioned steps might be redundant. Given your docker image worked, it implies the environment issue was identified without needing a significant debug.\n",
      "\n",
      "Here’s a summary of how to approach it:\n",
      "\n",
      "```markdown\n",
      "### Potential Steps to Debug Potential Environment Issues:\n",
      "1. Try updating to the latest version of `transformers`.\n",
      "2. Create a new environment (e.g., using `conda`) and install the packages afresh to ensure there are no corrupted files.\n",
      "3. Run the problematic code on another system to see if it's an environment-specific issue, like using Google Colab to access a free instance.\n",
      "4. If upgrading or using different versions of `transformers` didn't resolve the issue and the problem was found in your original setup, start migrating your working Docker image settings ensuring they reflect what was fixed.\n",
      "\n",
      "If none of these work or additional complexity is revealed, reopen the issue and provide a more detailed environment log where the issue can be narrowed down.\n",
      "```\n",
      "\n",
      "**For resolving redundancy, the comments suggest closing the issue after Docker proved reliable, confirming an environment-specific fix without needing further investigation.**\n",
      "After reviewing the provided comments and the error log, it appears the error is associated with a shape inconsistency after the tokenization step. Specifically, the issue seems to arise from the shape of the `hidden_states` from the `model.forward` method.\n",
      "\n",
      "Here are the key points:\n",
      "\n",
      "1. **Shape of `hidden_states`:** `torch.Size([1, 221, 1, 8, 128])`\n",
      "2. **Expected Shape:** The `repeat_kv` function expects a tensor with a shape like `[batch, num_heads, seq_len, head_dim]`.\n",
      "\n",
      "According to the comments, the issue can be traced back to a nested list format in the dataset. This nesting causes an extra dimension during tokenization, conflicting with the expected dimensions.\n",
      "\n",
      "To fix this:\n",
      "1. **Remove Nested List:** Change the structure of the dataset to avoid nested lists during tokenization.\n",
      "\n",
      "Here's an updated script removing the nested lists:\n",
      "\n",
      "```python\n",
      "from transformers import LlamaConfig, LlamaForCausalLM, AutoTokenizer\n",
      "from datasets import Dataset\n",
      "from transformers import TrainingArguments, Trainer\n",
      "\n",
      "# Load & Prepare Model and Tokenizer\n",
      "model_dir = '/work/AI/Models/NousResearch/Meta-Llama-3.1-8B'\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    ## Create some training data\n",
      "    datasets = {\n",
      "        'train': Dataset.from_dict({\n",
      "            \"text\": [\"The quick brown fox jumped over the lazy dog's back\" * 20]\n",
      "        }),\n",
      "        'test': Dataset.from_dict({\n",
      "            \"text\": [\"The quick brown fox jumped over the lazy dog's back\" * 20]\n",
      "        })\n",
      "    }\n",
      "\n",
      "    ## Create the tokenizer\n",
      "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
      "    if not tokenizer.pad_token:\n",
      "        tokenizer.pad_token = \"<|finetune_right_pad_id|>\"  # for Llama\n",
      "\n",
      "    ## Tokenize datasets\n",
      "    datasets['train'] = datasets['train'].map(lambda x: tokenizer(x['text'], padding=True))\n",
      "    datasets['test'] = datasets['train'].map(lambda x: tokenizer(x['text'], padding=True))\n",
      "\n",
      "    ## Load the empty model\n",
      "    config = LlamaConfig.from_pretrained(model_dir)\n",
      "    model = LlamaForCausalLM(config)\n",
      "    model.to('cuda')\n",
      "\n",
      "    ## Set up training\n",
      "    training_args = TrainingArguments(\n",
      "        output_dir=\"outputs/\" + model_dir.split('/')[-1],\n",
      "        per_device_train_batch_size=4,\n",
      "        per_device_eval_batch_size=4,\n",
      "        num_train_epochs=4,\n",
      "        eval_strategy=\"epoch\",\n",
      "        save_strategy=\"no\",  # no, epoch, steps, best\n",
      "        bf16=True,\n",
      "        push_to_hub=False,\n",
      "    )\n",
      "\n",
      "    trainer = Trainer(\n",
      "        model=model,\n",
      "        args=training_args,\n",
      "        train_dataset=datasets[\"train\"],\n",
      "        eval_dataset=datasets[\"test\"],\n",
      "        processing_class=tokenizer,\n",
      "    )\n",
      "\n",
      "    ## Train\n",
      "    trainer.train()\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "- The dataset structure is changed to ensure that each text entry is not nested within another list but rather a flat list containing multiple text samples.\n",
      "- We handle padding and trimming during the tokenization step to fit the expected input shape.\n",
      "- These changes should resolve the `too many values to unpack` error by ensuring the input tensor conforms to the expected dimensions.\n"
     ]
    }
   ],
   "source": [
    "with open(\"qa_from_issues.json\", \"r\") as f:\n",
    "    qa_data = json.load(f)\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    futures = [executor.submit(summarize_comment, qa[\"Issue Title\"], qa[\"Issue Body\"], qa[\"Issue Comments\"]) for qa in qa_data]\n",
    "    for future in as_completed(futures):\n",
    "        qa = qa_data[futures.index(future)]\n",
    "        qa[\"answer\"] = future.result()\n",
    "\n",
    "with open(\"qa_from_issues_with_summary.json\", \"w\") as f:\n",
    "    json.dump(qa_data, f, indent=4, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PR and Code Review Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_PRS = 10  # 设定最多提取几个 PR，避免 API rate limit\n",
    "OUTPUT_FILE = \"huggingface_pr_data.json\"\n",
    "\n",
    "\n",
    "for pr in repo.get_pulls(state=\"closed\", sort=\"created\", direction=\"desc\"):\n",
    "    if MAX_PRS <= 0:\n",
    "        break\n",
    "    MAX_PRS -= 1\n",
    "\n",
    "    pr_data = {\n",
    "        \"pr_number\": pr.number,\n",
    "        \"title\": pr.title,\n",
    "        \"body\": pr.body,\n",
    "        \"user\": pr.user.login,\n",
    "        \"created_at\": str(pr.created_at),\n",
    "        \"merged\": pr.merged,\n",
    "        \"merge_commit_sha\": pr.merge_commit_sha,\n",
    "        \"files\": [],\n",
    "        \"review_comments\": [],\n",
    "        \"general_comments\": [],\n",
    "    }\n",
    "\n",
    "    # PR 变更文件\n",
    "    try:\n",
    "        for file in pr.get_files():\n",
    "            pr_data[\"files\"].append({\n",
    "                \"filename\": file.filename,\n",
    "                \"status\": file.status,\n",
    "                \"patch\": file.patch if hasattr(file, \"patch\") else None\n",
    "            })\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch files for PR #{pr.number}: {e}\")\n",
    "\n",
    "    # Code Review 评论\n",
    "    try:\n",
    "        for comment in pr.get_review_comments():\n",
    "            pr_data[\"review_comments\"].append({\n",
    "                \"user\": comment.user.login,\n",
    "                \"path\": comment.path,\n",
    "                \"line\": comment.position,\n",
    "                \"body\": comment.body\n",
    "            })\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch comments for PR #{pr.number}: {e}\")\n",
    "    \n",
    "    try:\n",
    "        issue = repo.get_issue(number=pr.number)\n",
    "        for comment in issue.get_comments():\n",
    "            pr_data[\"general_comments\"].append({\n",
    "                \"user\": comment.user.login,\n",
    "                \"created_at\": str(comment.created_at),\n",
    "                \"body\": comment.body\n",
    "            })\n",
    "    except Exception as e:\n",
    "        print(f\"[通用评论出错] PR #{pr.number}: {e}\")\n",
    "\n",
    "with open(OUTPUT_FILE, \"w\") as f:\n",
    "    json.dump(pr_data, f, indent=4, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_pr(issue_title, issue_body, comments):\n",
    "    # 使用 GPT 模型生成摘要\n",
    "    prompt = \"\"\"\n",
    "    Summarize the following comments, tell me how to resolve issue `{issue_title}`.\n",
    "    If the issue isn't resolved, please just respond with '<Unsolved>'.\n",
    "    Issue Title:\n",
    "    {issue_title}\n",
    "    Issue Body:\n",
    "    {issue_body}\n",
    "    Comments:\n",
    "    {comments}\n",
    "    \"\"\"\n",
    "    client = OpenAI()\n",
    "    prompt = prompt.format(issue_title=issue_title, issue_body=issue_body, comments=comments)\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    print(response.choices[0].message.content)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38/38 [00:00<00:00, 3195.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers\n",
      "Agents & Tools\n",
      "Agents\n",
      "Agent\n",
      "class transformers.Agent\n",
      "CodeAgent\n",
      "class transformers.CodeAgent\n",
      "React agents\n",
      "class transformers.ReactAgent\n",
      "class transformers.ReactJsonAgent\n",
      "class transformers.ReactCodeAgent\n",
      "ManagedAgent\n",
      "class transformers.ManagedAgent\n",
      "Tools\n",
      "load_tool\n",
      "tool\n",
      "Tool\n",
      "class transformers.Tool\n",
      "Toolbox\n",
      "class transformers.Toolbox\n",
      "PipelineTool\n",
      "class transformers.PipelineTool\n",
      "launch_gradio_demo\n",
      "stream_to_gradio\n",
      "ToolCollection\n",
      "class transformers.ToolCollection\n",
      "Engines\n",
      "TransformersEngine\n",
      "class transformers.TransformersEngine\n",
      "HfApiEngine\n",
      "class transformers.HfApiEngine\n",
      "Agent Types\n",
      "AgentText\n",
      "class transformers.agents.agent_types.AgentText\n",
      "AgentImage\n",
      "class transformers.agents.agent_types.AgentImage\n",
      "AgentAudio\n",
      "class transformers.agents.agent_types.AgentAudio\n",
      "✅ Done! Extracted 37 QA pairs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from urllib.parse import urljoin\n",
    "import time\n",
    "\n",
    "def fetch_hf_doc(url):\n",
    "    html = requests.get(url).text\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    return soup\n",
    "\n",
    "def extract_sections(soup):\n",
    "    qa_pairs = []\n",
    "    headers = soup.find_all(['h1', 'h2', 'h3'])\n",
    "\n",
    "    for header in tqdm(headers):\n",
    "        title = header.get_text().strip()\n",
    "        print(title)\n",
    "        content = []\n",
    "        next_sibling = header.find_next_sibling()\n",
    "\n",
    "        # Collect paragraphs and lists until the next header\n",
    "        while next_sibling and next_sibling.name not in ['h1', 'h2', 'h3']:\n",
    "            # if next_sibling.name in ['p', 'ul', 'ol', 'pre', 'li']:\n",
    "            content.append(next_sibling.get_text().strip())\n",
    "            next_sibling = next_sibling.find_next_sibling()\n",
    "\n",
    "        text = \"\\n\".join(content).strip()\n",
    "        if text:\n",
    "            qa_pairs.append(\n",
    "                {\n",
    "                    \"header\": title,\n",
    "                    \"content\": text,\n",
    "                    \"source\": \"huggingface_doc\",\n",
    "                    \"url\": soup.title.string if soup.title else \"N/A\"\n",
    "                }\n",
    "            )\n",
    "            # question, answer = make_qa_from_section(title, text)\n",
    "\n",
    "            # if question:\n",
    "            #     qa_pairs.append({\n",
    "            #         \"question\": question,\n",
    "            #         \"answer\": answer,\n",
    "            #         \"source\": \"huggingface_doc\",\n",
    "            #         \"metadata\": {\n",
    "            #             \"section_title\": title,\n",
    "            #             \"url\": soup.title.string if soup.title else \"N/A\"\n",
    "            #         }\n",
    "            #     })\n",
    "\n",
    "    return qa_pairs\n",
    "\n",
    "def make_qa_from_section(title, text):\n",
    "    \"\"\"\n",
    "    基于标题和内容生成问题模板\n",
    "    \"\"\"\n",
    "    if len(text.split()) < 5:\n",
    "        return None, None\n",
    "\n",
    "    # 简单模板化问题构造\n",
    "    if \"tokenizer\" in title.lower():\n",
    "        q = f\"What is {title} in HuggingFace Transformers?\"\n",
    "    elif title.lower().startswith(\"how\"):\n",
    "        q = title + \"?\"\n",
    "    elif \"parameters\" in title.lower():\n",
    "        q = f\"What parameters does {title} include?\"\n",
    "    else:\n",
    "        q = f\"What does '{title}' refer to in Transformers?\"\n",
    "\n",
    "    return q, text\n",
    "\n",
    "def save_to_json(data, filename=\"hf_qa_output.json\"):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "url = \"https://huggingface.co/docs/transformers/en/main_classes/agent#transformers.Agent\"  # 你可以替换成其他页面\n",
    "soup = fetch_hf_doc(url)\n",
    "qa_pairs = extract_sections(soup)\n",
    "save_to_json(qa_pairs)\n",
    "print(f\"✅ Done! Extracted {len(qa_pairs)} QA pairs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_qa_from_hf_doc(header, content, few_shot_examples):\n",
    "    # 使用 GPT 模型生成摘要\n",
    "    prompt = \"\"\"\n",
    "    Generate a question and answer based on the following header and content.\n",
    "    {few_shot_examples}\n",
    "    Header:\n",
    "    {header}\n",
    "    Content:\n",
    "    {content}\n",
    "    \"\"\"\n",
    "    client = OpenAI()\n",
    "    prompt = prompt.format(few_shot_examples=few_shot_examples, header=header, content=content)\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    print(response.choices[0].message.content)\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_qa_pairs = [\n",
    "    {\n",
    "        \"Header\": \"Transformers\",\n",
    "        \"Content\": \"Transformers is a library for natural language processing tasks, including text classification, tokenization, and more.\",\n",
    "        \"Question\": \"What is Transformers?\",\n",
    "        \"Answer\": \"Transformers is a library for natural language processing tasks, including text classification, tokenization, and more.\"\n",
    "    },\n",
    "    {\n",
    "        \"Header\": \"Agents\",\n",
    "        \"Content\": \"We provide two types of agents, based on the main Agent class:\\nCodeAgent acts in one shot, generating code to solve the task, then executes it at once. ReactAgent acts step by step, each step consisting of one thought, then one tool call and execution. It has two classes:ReactJsonAgent writes its tool calls in JSON. ReactCodeAgent writes its tool calls in Python code.\",\n",
    "        \"Question\": \"What is the difference between CodeAgent and ReactAgent?\",\n",
    "        \"Answer\": \"CodeAgent acts in one shot, generating code to solve the task, then executes it at once. ReactAgent acts step by step, each step consisting of one thought, then one tool call and execution. It has two classes:ReactJsonAgent writes its tool calls in JSON. ReactCodeAgent writes its tool calls in Python code.\"\n",
    "    },\n",
    "    {\n",
    "        \"Header\": \"Agent Types\",\n",
    "        \"Content\": \"Agents can handle any type of object in-between tools; tools, being completely multimodal, can accept and return\\ntext, image, audio, video, among other types. In order to increase compatibility between tools, as well as to\\ncorrectly render these returns in ipython (jupyter, colab, ipython notebooks, …), we implement wrapper classes\\naround these types.\\nThe wrapped objects should continue behaving as initially; a text object should still behave as a string, an image\\nobject should still behave as a PIL.Image.\\nThese types have three specific purposes:\\nCalling to_raw on the type should return the underlying object Calling to_string on the type should return the object as a string: that can be the string in case of an AgentText\\nbut will be the path of the serialized version of the object in other instances Displaying it in an ipython kernel should display the object correctly\",\n",
    "        \"Question\": \"What is the purpose of wrapper classes for Agents?\",\n",
    "        \"Answer\": \"In order to increase compatibility between tools, as well as to correctly render these returns in ipython (jupyter, colab, ipython notebooks, …), we implement wrapper classes around these types.\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: &quot;Inconsistent Predictions in PyTorch Model: Single Image vs. Batch Processing&quot;\n",
      "Body: <p>I am noticing a significant difference in model predictions when running predictions on a single image versus the whole dataset. The model, which was trained using PyTorch, gives drastically differ...\n",
      "Link: https://stackoverflow.com/questions/79294216/inconsistent-predictions-in-pytorch-model-single-image-vs-batch-processing\n",
      "----------------------------------------\n",
      "Title: What is the proper way to fill a batch in training an LM all the way to the end eg how to correct my tokenize_and_group_texts_via_blocks?\n",
      "Body: <p>I’m preparing a text dataset for next-token language-model pre-training.\n",
      "Using HF datasets with batched=True, I wrote a helper that\n",
      "1.  prepends a BOS token (if the tokenizer has one),\n",
      "2.  appends ...\n",
      "Link: https://stackoverflow.com/questions/79615125/what-is-the-proper-way-to-fill-a-batch-in-training-an-lm-all-the-way-to-the-end\n",
      "----------------------------------------\n",
      "Title: How to Compute Teacher-Forced Accuracy (TFA) for Hugging Face Models While Handling EOS Tokens?\n",
      "Body: <p>I am trying to compute Teacher-Forced Accuracy (TFA) for Hugging Face models, ensuring the following:</p>\n",
      "<ol>\n",
      "<li>EOS Token Handling: The model should be rewarded for predicting the first EOS toke...\n",
      "Link: https://stackoverflow.com/questions/79209319/how-to-compute-teacher-forced-accuracy-tfa-for-hugging-face-models-while-handl\n",
      "----------------------------------------\n",
      "Title: How can I properly load a LoRA weight into a pretrained Stable Diffusion model on TorchServe and enable parallel inference?\n",
      "Body: <p>I'm attempting to serve a pretrained Stable Diffusion model with LoRA weights applied using TorchServe. However, the LoRA weights don't seem to load properly, and I'm not sure why. Could anyone hel...\n",
      "Link: https://stackoverflow.com/questions/79612497/how-can-i-properly-load-a-lora-weight-into-a-pretrained-stable-diffusion-model-o\n",
      "----------------------------------------\n",
      "Title: How to load a huggingface dataset from local path?\n",
      "Body: <p>Take a simple example in this website, <a href=\"https://huggingface.co/datasets/Dahoas/rm-static\" rel=\"nofollow noreferrer\">https://huggingface.co/datasets/Dahoas/rm-static</a>:</p>\n",
      "<p>if I want to...\n",
      "Link: https://stackoverflow.com/questions/77020278/how-to-load-a-huggingface-dataset-from-local-path\n",
      "----------------------------------------\n",
      "Title: ollama.generate raises model not found error: &quot;hf.co/mradermacher/Llama-3.2-3B-Instruct-uncensored-GGUF&quot;\n",
      "Body: <p>I'm trying to run a Python script that uses the ollama library to generate responses from a custom LLM model. My code attempts to call ollama.generate() using the following model name:</p>\n",
      "<pre><co...\n",
      "Link: https://stackoverflow.com/questions/79605566/ollama-generate-raises-model-not-found-error-hf-co-mradermacher-llama-3-2-3b-i\n",
      "----------------------------------------\n",
      "Title: Unable to connect to hugging face model\n",
      "Body: <pre class=\"lang-py prettyprint-override\"><code>from sentence_transformers import SentenceTransformer\n",
      "\n",
      "model = SentenceTransformer(&quot;BAAI/bge-small-en-v1.5&quot;)\n",
      "\n",
      "sentences = [\n",
      "    &quot;The weat...\n",
      "Link: https://stackoverflow.com/questions/79600374/unable-to-connect-to-hugging-face-model\n",
      "----------------------------------------\n",
      "Title: How to broadcast a tensor from main process using Accelerate?\n",
      "Body: <p>I want to do some computation in the main process and broadcast the tensor to other processes. Here is a sketch of what my code looks like currently:</p>\n",
      "<pre class=\"lang-py prettyprint-override\"><...\n",
      "Link: https://stackoverflow.com/questions/78165875/how-to-broadcast-a-tensor-from-main-process-using-accelerate\n",
      "----------------------------------------\n",
      "Title: LangChain: &#39;dict&#39; object has no attribute &#39;replace&#39; when using Chroma retriever\n",
      "Body: <p>I am working on a chatbot using LangChain, ChromaDB, and Hugging Face models. However, when I try to run my script, I get the error:</p>\n",
      "<pre><code>'dict' object has no attribute 'replace'\n",
      "</code><...\n",
      "Link: https://stackoverflow.com/questions/79450099/langchain-dict-object-has-no-attribute-replace-when-using-chroma-retriever\n",
      "----------------------------------------\n",
      "Title: Hugging Face Sentence Transformer API returning 400 error for embeddings with incorrect format\n",
      "Body: <pre><code>import { DataAPIClient } from &quot;@datastax/astra-db-ts&quot;;\n",
      "import { PuppeteerWebBaseLoader } from &quot;langchain/document_loaders/web/puppeteer&quot;;\n",
      "import axios from &quot;axios&q...\n",
      "Link: https://stackoverflow.com/questions/79601485/hugging-face-sentence-transformer-api-returning-400-error-for-embeddings-with-in\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def get_questions(tag='huggingface', page=1, pagesize=10):\n",
    "    url = \"https://api.stackexchange.com/2.3/questions\"\n",
    "    params = {\n",
    "        'order': 'desc',\n",
    "        'sort': 'activity',\n",
    "        'tagged': tag,\n",
    "        'site': 'stackoverflow',\n",
    "        'filter': 'withbody',\n",
    "        'page': page,\n",
    "        'pagesize': pagesize\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    data = response.json()\n",
    "    return data['items']\n",
    "\n",
    "questions = get_questions()\n",
    "for q in questions:\n",
    "    print(f\"Title: {q['title']}\")\n",
    "    print(f\"Body: {q['body'][:200]}...\")\n",
    "    print(f\"Link: {q['link']}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(questions[0]['answer_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: <p>I have run into the same issue myself and to my best understanding these tiny differences (≈2.3e-10) are due to floating point non-associativity or microkernel behavior in batched matrix ops. Apparently, It is expected in complex models with Attention layers.</p>\n",
      "<p>These differences are not a bug! They are not fixed by setting options like <strong><code>torch.use_deterministic_algorithms(True)</code></strong>, and they are not due to randomness, but due to numerical artifacts of batching.</p>\n",
      "\n",
      "Answer: <p>Try with eval mode.\n",
      "I have added it and updated the code.</p>\n",
      "<pre><code>from transformers import Trainer, TrainingArguments, PreTrainedModel, PretrainedConfig\n",
      "from torch.utils.data import Dataset\n",
      "import torch\n",
      "import torch.nn.functional as F\n",
      "import numpy as np\n",
      "\n",
      "# Number of Features\n",
      "num_of_features = 128\n",
      "\n",
      "# Dataset Class\n",
      "class SequenceDataset(Dataset):\n",
      "    def __init__(self, X, y):\n",
      "        self.X = torch.tensor(X, dtype=torch.float32)\n",
      "        self.y = torch.tensor(y, dtype=torch.long)\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.y)\n",
      "\n",
      "    def __getitem__(self, idx):\n",
      "        return {&quot;input_ids&quot;: self.X[idx], &quot;labels&quot;: self.y[idx]}\n",
      "\n",
      "# Configuration Class\n",
      "class SequenceConfig(PretrainedConfig):\n",
      "    model_type = &quot;sequence_transformer&quot;\n",
      "\n",
      "    def __init__(self, num_features=num_of_features, num_classes=3, d_model=1024, nhead=4, \n",
      "                 num_layers=4, dim_feedforward=512, **kwargs):\n",
      "        self.num_features = num_features\n",
      "        self.num_classes = num_classes\n",
      "        self.d_model = d_model\n",
      "        self.nhead = nhead\n",
      "        self.num_layers = num_layers\n",
      "        self.dim_feedforward = dim_feedforward\n",
      "        super().__init__(**kwargs)\n",
      "\n",
      "# Transformer Model\n",
      "class SequenceTransformer(PreTrainedModel):\n",
      "    config_class = SequenceConfig\n",
      "\n",
      "    def __init__(self, config):\n",
      "        super().__init__(config)\n",
      "        self.embedding = torch.nn.Linear(config.num_features, config.d_model)\n",
      "        self.positional_encoding = torch.nn.Parameter(torch.zeros(1, config.d_model))\n",
      "        \n",
      "        encoder_layer = torch.nn.TransformerEncoderLayer(\n",
      "            d_model=config.d_model, nhead=config.nhead, dim_feedforward=config.dim_feedforward, batch_first=True\n",
      "        )\n",
      "        self.transformer_encoder = torch.nn.TransformerEncoder(encoder_layer, num_layers=config.num_layers)\n",
      "        self.fc = torch.nn.Linear(config.d_model, config.num_classes)\n",
      "\n",
      "    def forward(self, input_ids, labels=None):\n",
      "        src = self.embedding(input_ids) + self.positional_encoding\n",
      "        output = self.transformer_encoder(src)\n",
      "        logits = self.fc(output)\n",
      "        probs = F.softmax(logits, dim=-1)\n",
      "        loss = None\n",
      "        if labels is not None:\n",
      "            loss_fct = torch.nn.CrossEntropyLoss()\n",
      "            loss = loss_fct(logits.view(-1, self.config.num_classes), labels.view(-1))\n",
      "        return {&quot;loss&quot;: loss, &quot;logits&quot;: logits, &quot;probs&quot;: probs} if labels is not None else logits\n",
      "\n",
      "# Assume `train_dataset`, `val_dataset`, `compute_metrics`, `path`, `train_image`, `numOfBreakpointsPerEpoch` are defined\n",
      "# Training Code\n",
      "config = SequenceConfig()\n",
      "model = SequenceTransformer(config)\n",
      "\n",
      "batchSize = 32\n",
      "numWarmUpSteps = int(np.shape(train_image)[0] / batchSize / numOfBreakpointsPerEpoch / 10)\n",
      "training_args = TrainingArguments(\n",
      "    output_dir=path,\n",
      "    num_train_epochs=1,\n",
      "    per_device_train_batch_size=batchSize,\n",
      "    per_device_eval_batch_size=320,\n",
      "    warmup_steps=numWarmUpSteps,\n",
      "    weight_decay=0.1,\n",
      "    logging_strategy='no',\n",
      "    eval_strategy=&quot;epoch&quot;,\n",
      "    save_strategy=&quot;epoch&quot;,\n",
      "    metric_for_best_model=&quot;accuracy&quot;,\n",
      "    save_only_model=True,\n",
      ")\n",
      "\n",
      "# Trainer Initialization\n",
      "trainer = Trainer(\n",
      "    model=model,\n",
      "    args=training_args,\n",
      "    train_dataset=train_dataset,\n",
      "    eval_dataset=val_dataset,\n",
      "    compute_metrics=compute_metrics\n",
      ")\n",
      "\n",
      "# Train the Model\n",
      "train_output = trainer.train()\n",
      "\n",
      "# Save Model and Training Arguments\n",
      "trainer.save_model(&quot;./SavedModels&quot;)\n",
      "torch.save(training_args, &quot;./SavedModels/training_args.bin&quot;)\n",
      "\n",
      "# Prediction Code\n",
      "training_args_loaded = torch.load(&quot;./SavedModels/training_args.bin&quot;)\n",
      "model_save_path = &quot;./SavedModels/&quot;\n",
      "model = SequenceTransformer(config).from_pretrained(model_save_path)\n",
      "model.eval()  # Ensure the model is in evaluation mode\n",
      "\n",
      "trainer = Trainer(model=model, compute_metrics=compute_metrics, args=training_args_loaded)\n",
      "\n",
      "test_data = np.random.rand(10, num_of_features)  # Example test data\n",
      "test_data_torch = torch.tensor(test_data, dtype=torch.float32)\n",
      "\n",
      "single_image = test_data_torch[0:1]\n",
      "batch_images = test_data_torch\n",
      "\n",
      "# Ensure no gradients are calculated during predictions\n",
      "with torch.no_grad():\n",
      "    # Process predictions for a single image\n",
      "    single_prediction = trainer.predict(single_image)\n",
      "    # Process predictions for a batch\n",
      "    batch_predictions = trainer.predict(batch_images)\n",
      "\n",
      "# Output Predictions\n",
      "print(&quot;Single Prediction:&quot;, single_prediction.predictions[0])\n",
      "print(&quot;Batch Prediction:&quot;, batch_predictions.predictions[0])\n",
      "</code></pre>\n",
      "<p><strong>The reason for using eval:</strong>\n",
      "while inference, batch normalisation layers apply the running estimates of mean and variance accumulated during training, ensuring that the input is Uniformly normalised, independent of the specific batch it's belong to.</p>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_answers(question_id):\n",
    "    url = f\"https://api.stackexchange.com/2.3/questions/{question_id}/answers\"\n",
    "    params = {\n",
    "        'order': 'desc',\n",
    "        'sort': 'activity',\n",
    "        'site': 'stackoverflow',\n",
    "        'filter': 'withbody'\n",
    "    }\n",
    "    res = requests.get(url, params=params)\n",
    "    data = res.json()\n",
    "    return data['items']\n",
    "\n",
    "# 示例：抓第一个问题的答案\n",
    "question_id = questions[0]['question_id']\n",
    "answers = get_answers(question_id)\n",
    "for ans in answers:\n",
    "    print(\"Answer:\", ans['body'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching page 1...\n",
      "Fetching page 2...\n",
      "Fetched 2 answers for question: &quot;Inconsistent Predictions in PyTorch Model: S...\n",
      "Fetched 1 answers for question: How to Compute Teacher-Forced Accuracy (TFA) for H...\n",
      "Fetched 4 answers for question: How to load a huggingface dataset from local path?...\n",
      "Fetched 1 answers for question: Unable to connect to hugging face model...\n",
      "Fetched 2 answers for question: How to broadcast a tensor from main process using ...\n",
      "Fetched 1 answers for question: Hugging Face Sentence Transformer API returning 40...\n",
      "Fetched 1 answers for question: Error while loading Deepseek using HuggingFace...\n",
      "Fetched 3 answers for question: ImportError: cannot import name &#39;HuggingFaceIn...\n",
      "Fetched 1 answers for question: How to convert safetensors model to onnx model?...\n",
      "Fetched 1 answers for question: Serving models using VLLM on Huggingface Spaces...\n",
      "✅ Done! Saved 100 Q&A items to huggingface_qa.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "\n",
    "def get_questions_with_answers(tag='huggingface', target_count=100):\n",
    "    collected = []\n",
    "    page = 1\n",
    "    page_size = 100\n",
    "    while len(collected) < target_count:\n",
    "        print(f\"Fetching page {page}...\")\n",
    "        url = \"https://api.stackexchange.com/2.3/questions\"\n",
    "        params = {\n",
    "            'order': 'desc',\n",
    "            'sort': 'activity',\n",
    "            'tagged': tag,\n",
    "            'site': 'stackoverflow',\n",
    "            'filter': 'withbody',\n",
    "            'pagesize': page_size,\n",
    "            'page': page\n",
    "        }\n",
    "        res = requests.get(url, params=params)\n",
    "        data = res.json()\n",
    "        items = data.get('items', [])\n",
    "\n",
    "        for item in items:\n",
    "            if item.get('answer_count', 0) > 0:\n",
    "                collected.append({\n",
    "                    'question_id': item['question_id'],\n",
    "                    'title': item['title'],\n",
    "                    'body': item['body'],\n",
    "                    'link': item['link']\n",
    "                })\n",
    "                if len(collected) >= target_count:\n",
    "                    break\n",
    "\n",
    "        if not data.get('has_more', False):\n",
    "            break  # 没有更多数据\n",
    "        page += 1\n",
    "        time.sleep(1)  # 避免触发速率限制\n",
    "    return collected\n",
    "\n",
    "def get_answers_for_question(qid):\n",
    "    url = f\"https://api.stackexchange.com/2.3/questions/{qid}/answers\"\n",
    "    params = {\n",
    "        'order': 'desc',\n",
    "        'sort': 'votes',\n",
    "        'site': 'stackoverflow',\n",
    "        'filter': 'withbody'\n",
    "    }\n",
    "    res = requests.get(url, params=params)\n",
    "    data = res.json()\n",
    "    return [ans['body'] for ans in data.get('items', [])]\n",
    "\n",
    "# 抓取前100个有答案的问题\n",
    "questions = get_questions_with_answers()\n",
    "\n",
    "# 获取每个问题的答案\n",
    "for q in questions:\n",
    "    q['answers'] = get_answers_for_question(q['question_id'])\n",
    "    print(f\"Fetched {len(q['answers'])} answers for question: {q['title'][:50]}...\")\n",
    "    time.sleep(0.5)\n",
    "\n",
    "# 保存为 JSON 文件\n",
    "with open('huggingface_qa.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(questions, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"✅ Done! Saved 100 Q&A items to huggingface_qa.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done! Saved 100 Q&A items to huggingface_qa_formatted.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('huggingface_qa.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "formatted_data = []\n",
    "for item in data:\n",
    "    for i in range(len(item['answers'])):\n",
    "        formatted_data.append({\n",
    "            \"question\": item['title'] + \"\\n\" + item['body'],\n",
    "            \"answer\": item['answers'][i],\n",
    "            \"source\": \"stackoverflow\",\n",
    "            \"metadata\": {\n",
    "            \"question_id\": item['question_id'],\n",
    "            \"link\": item['link']\n",
    "        }\n",
    "    })\n",
    "\n",
    "with open('huggingface_qa_formatted.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(formatted_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"✅ Done! Saved 100 Q&A items to huggingface_qa_formatted.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformerQA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
