[
    {
        "question": "SSLError: HTTPSConnectionPool(host=&#39;huggingface.co&#39;, port=443): Max retries exceeded with url: /dslim/bert-base-NER/resolve/main/tokenizer_config.json\n<p>I am facing below issue while loading the pretrained BERT model from HuggingFace due to SSL certificate error.</p>\n<h2>Error:</h2>\n<blockquote>\n<p>SSLError: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /dslim/bert-base-NER/resolve/main/tokenizer_config.json (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self signed certificate in certificate chain (_ssl.c:1108)')))</p>\n</blockquote>\n<h2>The line that is causing the issue is:</h2>\n<pre><code>tokenizer = AutoTokenizer.from_pretrained(&quot;dslim/bert-base-NER&quot;)\n</code></pre>\n<h2>Source code:</h2>\n<pre><code>from transformers import AutoTokenizer, AutoModelForTokenClassification\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(&quot;dslim/bert-base-NER&quot;)\nmodel = AutoModelForTokenClassification.from_pretrained(&quot;dslim/bert-base-NER&quot;)\n</code></pre>\n<p>I am expecting to download pre-trained models while running the code in jupyter lab on Windows.</p>\n",
        "answer": "<p>A working solution for this is <strong>Enable your device for development</strong>.</p>\n<blockquote>\n<p>if you're writing software with Visual Studio on a computer for the first time, you will need to enable Developer Mode on both the development PC and on any devices you'll use to test your code.</p>\n</blockquote>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 75110981,
            "link": "https://stackoverflow.com/questions/75110981/sslerror-httpsconnectionpoolhost-huggingface-co-port-443-max-retries-exce"
        }
    },
    {
        "question": "AttributeError: &#39;AcceleratorState&#39; object has no attribute &#39;distributed_type&#39;\n<pre><code>import transformers\nfrom datasets import load_dataset\nimport tensorflow as tf\n\ntokenizer = transformers.AutoTokenizer.from_pretrained('roberta-base')\n\ndf = load_dataset('csv', data_files={'train':'FinalDatasetTrain.csv', 'test':'FinalDatasetTest.csv'})\n\ndef tokenize_function(examples):\n    return tokenizer(examples[&quot;text&quot;], truncation=True)\n\ntokenized_datasets = df.map(tokenize_function, batched=True)\ndata_collator = transformers.DataCollatorWithPadding(tokenizer=tokenizer)\n\nmodel = transformers.AutoModelForSequenceClassification.from_pretrained('roberta-base', num_labels=7)\n\ntraining_args = transformers.TFTrainingArguments(\n    output_dir=&quot;./results&quot;,\n    num_train_epochs=2,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=16,\n    save_strategy='epoch',\n    evaluation_strategy=&quot;epoch&quot;,\n    logging_dir=&quot;./logs&quot;,\n)\n\ntrainer = transformers.Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets['train'],\n    eval_dataset=tokenized_datasets['test'],\n    data_collator=data_collator,\n    tokenizer=tokenizer\n)\n\ntrainer.train()\n</code></pre>\n<p>When I run this code I get an error saying:</p>\n<blockquote>\n<p>AttributeError: 'AcceleratorState' object has no attribute 'distributed_type'.</p>\n</blockquote>\n<p>How do I fix this (I tried both Jupyter notebook and Google Colab)?</p>\n",
        "answer": "<p>To temporarily solve the problem, I downgrade the <code>accelerate</code> and <code>transformers</code> to:</p>\n<ul>\n<li>accelerate 0.15.0</li>\n<li>transformers 4.28.1</li>\n<li>tokenizers 0.13.3</li>\n</ul>\n<p>Any major version higher than these will cause the error. I cannot find any official documentation about the change of model structure regarding <code>distributed_type</code> yet.</p>\n<p>Remember to restart the runtime after any version change.</p>\n<p>Note: downgrading is just a temp solution; I usually suggest upgrading to the latest version.</p>\n<hr />\n<p><strong>UPDATE</strong></p>\n<p>This is still valid in May 2024. The latest version of <code>tokenizers</code> and <code>accelerate</code> do not work:</p>\n<ul>\n<li>transformers 4.41.0</li>\n<li>accelerate 0.30.1</li>\n<li>tokenizers 0.19.1</li>\n</ul>\n<p>which will still give the captioned error.</p>\n<hr />\n<p><strong>UPDATE 2</strong> still not yet fixed in February 2025, with the following versions:</p>\n<ul>\n<li>transformers 4.49.0</li>\n<li>accelerate 1.4.0</li>\n<li>tokenizers 0.21.0</li>\n</ul>\n<p>Same error occurs.</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 76516579,
            "link": "https://stackoverflow.com/questions/76516579/attributeerror-acceleratorstate-object-has-no-attribute-distributed-type"
        }
    },
    {
        "question": "How to ensure last token in sequence is end-of-sequence token?\n<p>I am using the <code>gpt2</code> model from huggingface's <code>transformers</code> library. When tokenizing, I would like all sequences to end in the end-of-sequence (EOS) token.  How can I do this?</p>\n<p>An easy solution is to manually append the EOS token to each sequence in a batch prior to tokenization:</p>\n<pre><code>from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained('gpt2')\n\ntext = ['Hello world.', 'I am program.']\n\ntext = [el + tokenizer.eos_token for el in text]\n\ntokenized_text = tokenizer(text)\n\n</code></pre>\n<p>It seems like my solution is an inelegant solution for this task that is so commonplace that I expect there is some built-in way of doing this.  I haven't found anything about this in the documentation‚Äîis there a way?</p>\n<p>Edit: I want to do this in order to train a GPT-2 to generate specific kinds of responses to input sequences.  Without the end-of-sequence token during training, performance was poor, and the model generated much too much text.</p>\n",
        "answer": "<p>I would do this using f-strings.</p>\n<p><code>text = [f&quot;{el}{tokenizer.eos_token}&quot; for el in text]</code></p>\n<p>I also like using pandas:</p>\n<p><code>df[&quot;text_w_eos&quot;] = df.text.apply(lambda x: f&quot;{x}{tokenizer.eos_token}&quot;)</code></p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 75653539,
            "link": "https://stackoverflow.com/questions/75653539/how-to-ensure-last-token-in-sequence-is-end-of-sequence-token"
        }
    },
    {
        "question": "Error (&quot;bus error&quot;) running the simplest example on Hugging Face Transformers Pipeline (Macos M1)\n<p>I'm trying to follow the quick tour example here: <a href=\"https://huggingface.co/docs/transformers/quicktour\" rel=\"nofollow noreferrer\">https://huggingface.co/docs/transformers/quicktour</a></p>\n<p>and i'm getting a &quot;bus error&quot;.</p>\n<p>My env is:</p>\n<ul>\n<li>MacOS Sonoma 14.7, Apple M1 Max chip</li>\n<li>Python 3.11.5</li>\n<li>pip install transformers datasets evaluate accelerate</li>\n<li>pip install tf-keras</li>\n</ul>\n<p>Running this code:</p>\n<pre><code>from transformers import pipeline\n\nclassifier = pipeline(&quot;sentiment-analysis&quot;)\nresult = classifier(&quot;We are very happy to show you the ü§ó Transformers library.&quot;)\nprint(result)\n</code></pre>\n<p>And getting this result:</p>\n<blockquote>\n<p>No model was supplied, defaulted to\ndistilbert/distilbert-base-uncased-finetuned-sst-2-english and\nrevision 714eb0f\n(<a href=\"https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english\" rel=\"nofollow noreferrer\">https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english</a>).</p>\n<p>Using a pipeline without specifying a model name and revision in\nproduction is not recommended. [1]</p>\n<p>92883 bus error  python main.py</p>\n</blockquote>\n<p>Any ideas?</p>\n<p>Thanks for your help.</p>\n",
        "answer": "<p>using device=0 (the first GPU) solved this:</p>\n<p><code>classifier = pipeline(&quot;sentiment-analysis&quot;, device=0)</code></p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 79184146,
            "link": "https://stackoverflow.com/questions/79184146/error-bus-error-running-the-simplest-example-on-hugging-face-transformers-pi"
        }
    },
    {
        "question": "HuggingFace model loaded from the disk generates gibberish\n<p>I trained a LongT5 model using Huggingface's tooling.</p>\n<p>When I use the trained model directly after training the inference works as expected, I get good quality output, as expected from the training metrics. However if I save the model and load it from the disk, the output is gibberish. I can't figure out why.</p>\n<p>Code producing good output:</p>\n<pre class=\"lang-py prettyprint-override\"><code>text = dataset['test'][0]['from']\ninputs = tokenizer(text, return_tensors=&quot;pt&quot;).input_ids\ninputs = inputs.to('cuda:0')\n\nmodel.eval()\n\nwith torch.no_grad():\n    model.to('cuda:0')\n    model.generation_config = generation_config\n    outputs = model.generate(inputs)\n\ntranslation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# Prints correct output\nprint(translation)\n</code></pre>\n<p>How I save the model:</p>\n<pre class=\"lang-py prettyprint-override\"><code>trainer.save_model(os.path.join(model_output_dir, &quot;final&quot;))\ntokenizer.save_pretrained(os.path.join(model_output_dir, &quot;final&quot;))\n</code></pre>\n<p>How I load the model:</p>\n<pre class=\"lang-py prettyprint-override\"><code>model = LongT5ForConditionalGeneration.from_pretrained(os.path.join(model_output_dir, &quot;final&quot;))\nmodel.to('cuda:0')\nmodel.generation_config = generation_config\n\noutputs = model.generate(inputs)\n\ntranslation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# Prints random garbage, like:\n# pamper verre195188 albums188 albums188188 albums188 albums188 albums188 albums; unterschiedlich188 albums188 albums188 albums ...\n\nprint(translation)\n</code></pre>\n<p>In both cases, the tokenizer is the instance that already exists in memory during the training, but it doesn't make a difference whether I load it from the disk or not -- same result.</p>\n<p>The <code>generation_config</code> variable looks like this and it's also set in the training arguments:</p>\n<pre class=\"lang-py prettyprint-override\"><code>generation_config = GenerationConfig.from_model_config(model.config)\ngeneration_config._from_model_config = False\ngeneration_config.max_new_tokens = 512\n</code></pre>\n<p>It makes no difference whether it's set in the inference code or not, I still get gibberish.</p>\n",
        "answer": "<p>So, it turns out some strange bug in the current stable version of safetensors. It doesn't save the <code>encoder.embed_tokens.weight</code> and <code>decoder.embed_tokens.weight</code> state, so when the model is loaded again, these layers are initialized with random numbers.</p>\n<p>There are two workarounds:</p>\n<ol>\n<li>Use the latest version of safetensors where this seems to be fixed:</li>\n</ol>\n<pre><code>!pip install -U git+https://github.com/huggingface/safetensors.git\n</code></pre>\n<ol start=\"2\">\n<li>Don't use safetensors to save your model at all. You can set <code>save_safetensors=False</code> in the training arguments, so that HF will use pickle to save your model instead of safetensors.</li>\n</ol>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 79180415,
            "link": "https://stackoverflow.com/questions/79180415/huggingface-model-loaded-from-the-disk-generates-gibberish"
        }
    },
    {
        "question": "How to Log Training Loss at Step Zero in Hugging Face Trainer or SFT Trainer?\n<p>I'm using the Hugging Face <code>Trainer</code> (or <code>SFTTrainer</code>) for fine-tuning, and I want to log the training loss at step 0 (before any training steps are executed). I know there's an <code>eval_on_start</code> option for evaluation, but I couldn't find a direct equivalent for training loss logging at the beginning of training.</p>\n<p>Is there a way to log the initial training loss at step zero (before any updates) using <code>Trainer</code> or <code>SFTTrainer</code>? Ideally, I'd like something similar to <code>eval_on_start</code>.</p>\n<p>Here's what I've tried so far:</p>\n<h4>Solution 1: Custom Callback</h4>\n<p>I implemented a custom callback to log the training loss at the start of training:</p>\n<pre class=\"lang-py prettyprint-override\"><code>from transformers import TrainerCallback\n\nclass TrainOnStartCallback(TrainerCallback):\n    def on_train_begin(self, args, state, control, logs=None, **kwargs):\n        # Log training loss at step 0\n        logs = logs or {}\n        logs[&quot;train/loss&quot;] = None  # Replace None with an initial value if available\n        logs[&quot;train/global_step&quot;] = 0\n        self.log(logs)\n\n    def log(self, logs):\n        print(f&quot;Logging at start: {logs}&quot;)\n        wandb.log(logs)\n\n# Adding the callback to the Trainer\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    args=training_args,\n    optimizers=(optimizer, scheduler),\n    callbacks=[TrainOnStartCallback()],\n)\n</code></pre>\n<p>This works but feels a bit overkill. It logs metrics at the start of training before any steps.</p>\n<h4>Solution 2: Manual Logging</h4>\n<p>Alternatively, I manually log the training loss before starting training:</p>\n<pre class=\"lang-py prettyprint-override\"><code>wandb.log({&quot;train/loss&quot;: None, &quot;train/global_step&quot;: 0})\ntrainer.train()\n</code></pre>\n<h3>Question:</h3>\n<p>Are there any built-in features in <code>Trainer</code> or <code>SFTTrainer</code> to log training loss at step zero? Or is a custom callback or manual logging the best solution here? If so, are there better ways to implement this functionality? similar to the <code>eval_on_start</code> but <code>train_on_start</code>?</p>\n<p>cross:</p>\n<ul>\n<li><a href=\"https://discuss.huggingface.co/t/how-to-log-training-loss-at-step-zero-in-hugging-face-trainer-or-sft-trainer/128188\" rel=\"nofollow noreferrer\">discuss.huggingface</a></li>\n<li><a href=\"https://github.com/huggingface/transformers/issues/34981\" rel=\"nofollow noreferrer\">github/huggingface</a></li>\n</ul>\n",
        "answer": "<p>When using huggingface <code>transformer</code> library, the output returned by the model includes the model loss. Before starting the training, simply perform a forward pass on the dataset and obtain the model loss.</p>\n<pre class=\"lang-py prettyprint-override\"><code>with torch.no_grad():\n    outputs = model(**inputs, labels=labels)\n\nloss = outputs.loss\nprint(f&quot;Loss: {loss.item()}&quot;)\n</code></pre>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 79232257,
            "link": "https://stackoverflow.com/questions/79232257/how-to-log-training-loss-at-step-zero-in-hugging-face-trainer-or-sft-trainer"
        }
    },
    {
        "question": "Huggingface library not being able to replace separators in create_documents: &quot;AttributeError: &#39;dict&#39; object has no attribute &#39;replace&#39;&quot;\n<p>I'm a beginner in the chatbot developer world and currently building a rag code to create a context based chatbot, but I keep getting this error, I believe it happens when the text is being split, because even after the function is called, the text remains with the &quot;\\n&quot; separators.\nThe last line of the traceback occurs in the huggingface library.</p>\n<p>The traceback:</p>\n<pre><code>Traceback (most recent call last):\n  File &quot;c:\\Users\\sophi\\miniconda3\\envs\\ambiente3.9\\lib\\runpy.py&quot;, line 197, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File &quot;c:\\Users\\sophi\\miniconda3\\envs\\ambiente3.9\\lib\\runpy.py&quot;, line 87, in _run_code\n    exec(code, run_globals)\n  File &quot;c:\\Users\\sophi\\.vscode\\extensions\\ms-python.debugpy-2024.6.0-win32-x64\\bundled\\libs\\debugpy\\__main__.py&quot;, line 39, in &lt;module&gt;\n    cli.main()\n  File &quot;c:\\Users\\sophi\\.vscode\\extensions\\ms-python.debugpy-2024.6.0-win32-x64\\bundled\\libs\\debugpy/..\\debugpy\\server\\cli.py&quot;, line 430, in main\n    run()\n  File &quot;c:\\Users\\sophi\\.vscode\\extensions\\ms-python.debugpy-2024.6.0-win32-x64\\bundled\\libs\\debugpy/..\\debugpy\\server\\cli.py&quot;, line 284, in run_file\n    runpy.run_path(target, run_name=&quot;__main__&quot;)\n  File &quot;c:\\Users\\sophi\\.vscode\\extensions\\ms-python.debugpy-2024.6.0-win32-x64\\bundled\\libs\\debugpy\\_vendored\\pydevd\\_pydevd_bundle\\pydevd_runpy.py&quot;, line 321, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File &quot;c:\\Users\\sophi\\.vscode\\extensions\\ms-python.debugpy-2024.6.0-win32-x64\\bundled\\libs\\debugpy\\_vendored\\pydevd\\_pydevd_bundle\\pydevd_runpy.py&quot;, line 135, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File &quot;c:\\Users\\sophi\\.vscode\\extensions\\ms-python.debugpy-2024.6.0-win32-x64\\bundled\\libs\\debugpy\\_vendored\\pydevd\\_pydevd_bundle\\pydevd_runpy.py&quot;, line 124, in _run_code\n    exec(code, run_globals)\n  File &quot;c:\\Users\\sophi\\Documents\\ProjetosdePesquisa\\Projeto-de-Pesquisa-SOLIRIS\\llm_rag _ver4\\utils\\rag.py&quot;, line 130, in &lt;module&gt;\n    main()\n  File &quot;c:\\Users\\sophi\\Documents\\ProjetosdePesquisa\\Projeto-de-Pesquisa-SOLIRIS\\llm_rag _ver4\\utils\\rag.py&quot;, line 126, in main\n    response = qa.invoke({&quot;input&quot;: {&quot;context&quot;: context, &quot;question&quot;: question}})\n  File &quot;c:\\Users\\sophi\\miniconda3\\envs\\ambiente3.9\\lib\\site-packages\\langchain_core\\runnables\\base.py&quot;, line 4588, in invoke\n    return self.bound.invoke(\n  File &quot;c:\\Users\\sophi\\miniconda3\\envs\\ambiente3.9\\lib\\site-packages\\langchain_core\\runnables\\base.py&quot;, line 2505, in invoke\n    input = step.invoke(input, config, **kwargs)\n  File &quot;c:\\Users\\sophi\\miniconda3\\envs\\ambiente3.9\\lib\\site-packages\\langchain_core\\runnables\\passthrough.py&quot;, line 469, in invoke\n    return self._call_with_config(self._invoke, input, config, **kwargs)\n  File &quot;c:\\Users\\sophi\\miniconda3\\envs\\ambiente3.9\\lib\\site-packages\\langchain_core\\runnables\\base.py&quot;, line 1599, in _call_with_config       \n    context.run(\n  File &quot;c:\\Users\\sophi\\miniconda3\\envs\\ambiente3.9\\lib\\site-packages\\langchain_core\\runnables\\config.py&quot;, line 380, in call_func_with_variable_args\n    return func(input, **kwargs)  # type: ignore[call-arg]\n  File &quot;c:\\Users\\sophi\\miniconda3\\envs\\ambiente3.9\\lib\\site-packages\\langchain_core\\runnables\\passthrough.py&quot;, line 456, in _invoke\n    **self.mapper.invoke(\n  File &quot;c:\\Users\\sophi\\miniconda3\\envs\\ambiente3.9\\lib\\site-packages\\langchain_core\\runnables\\base.py&quot;, line 3152, in invoke\n    output = {key: future.result() for key, future in zip(steps, futures)}\n  File &quot;c:\\Users\\sophi\\miniconda3\\envs\\ambiente3.9\\lib\\site-packages\\langchain_core\\runnables\\base.py&quot;, line 3152, in &lt;dictcomp&gt;\n    output = {key: future.result() for key, future in zip(steps, futures)}\n  File &quot;c:\\Users\\sophi\\miniconda3\\envs\\ambiente3.9\\lib\\concurrent\\futures\\_base.py&quot;, line 446, in result\n    return self.__get_result()\n  File &quot;c:\\Users\\sophi\\miniconda3\\envs\\ambiente3.9\\lib\\concurrent\\futures\\_base.py&quot;, line 391, in __get_result\n    raise self._exception\n  File &quot;c:\\Users\\sophi\\miniconda3\\envs\\ambiente3.9\\lib\\concurrent\\futures\\thread.py&quot;, line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File &quot;c:\\Users\\sophi\\miniconda3\\envs\\ambiente3.9\\lib\\site-packages\\langchain_core\\runnables\\base.py&quot;, line 4588, in invoke\n    return self.bound.invoke(\n  File &quot;c:\\Users\\sophi\\miniconda3\\envs\\ambiente3.9\\lib\\site-packages\\langchain_core\\runnables\\base.py&quot;, line 2507, in invoke\n    input = step.invoke(input, config)\n  File &quot;c:\\Users\\sophi\\miniconda3\\envs\\ambiente3.9\\lib\\site-packages\\langchain_core\\retrievers.py&quot;, line 221, in invoke\n    raise e\n  File &quot;c:\\Users\\sophi\\miniconda3\\envs\\ambiente3.9\\lib\\site-packages\\langchain_core\\retrievers.py&quot;, line 214, in invoke\n    result = self._get_relevant_documents(\n  File &quot;c:\\Users\\sophi\\miniconda3\\envs\\ambiente3.9\\lib\\site-packages\\langchain_core\\vectorstores.py&quot;, line 797, in _get_relevant_documents    \n    docs = self.vectorstore.similarity_search(query, **self.search_kwargs)\n  File &quot;c:\\Users\\sophi\\miniconda3\\envs\\ambiente3.9\\lib\\site-packages\\langchain_community\\vectorstores\\chroma.py&quot;, line 349, in similarity_search\n    docs_and_scores = self.similarity_search_with_score(\n  File &quot;c:\\Users\\sophi\\miniconda3\\envs\\ambiente3.9\\lib\\site-packages\\langchain_community\\vectorstores\\chroma.py&quot;, line 438, in similarity_search_with_score\n    query_embedding = self._embedding_function.embed_query(query)\n  File &quot;c:\\Users\\sophi\\miniconda3\\envs\\ambiente3.9\\lib\\site-packages\\langchain_huggingface\\embeddings\\huggingface.py&quot;, line 102, in embed_query\n    return self.embed_documents([text])[0]\n  File &quot;c:\\Users\\sophi\\miniconda3\\envs\\ambiente3.9\\lib\\site-packages\\langchain_huggingface\\embeddings\\huggingface.py&quot;, line 81, in embed_documents\n    texts = list(map(lambda x: x.replace(&quot;\\n&quot;, &quot; &quot;), texts))\n  File &quot;c:\\Users\\sophi\\miniconda3\\envs\\ambiente3.9\\lib\\site-**packages\\langchain_huggingface\\embeddings\\huggingface.py&quot;, line 81, in &lt;lambda&gt;   \n    texts = list(map(lambda x: x.replace(&quot;\\n&quot;, &quot; &quot;), texts))\nAttributeError: 'dict' object has no attribute 'replace'**\n</code></pre>\n<h1>This is my entire code: (except for the groq api)</h1>\n<pre><code>\nimport sys\nimport os\nfrom langchain_core.prompts import PromptTemplate \nfrom langchain_groq import ChatGroq\nfrom langchain.chains import create_retrieval_chain\nfrom langchain_community.document_loaders import TextLoader\nfrom sentence_transformers import SentenceTransformer\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\nfrom langchain_community.vectorstores import Chroma\nfrom langchain.docstore.document import Document\nimport PyPDF2\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.llms import CTransformers\n\n# Caminho para o arquivo PDF\nPDF_PATH = 'pdf_handling/entrevistas.pdf'\n\n# Caminho para salvar os dados do ChromaDB\nCHROMA_DATA_PATH = &quot;chroma_data&quot;\n\n# Modelo de embeddings\nEMBED_MODEL = &quot;all-MiniLM-L6-v2&quot;\n\n# Nome da cole√ß√£o\nCOLLECTION_NAME = &quot;ruth_docs&quot;\n\ndef dict_to_string(input_dict):\n    # Convert the dictionary into a string representation\n    # This uses a list comprehension to create a list of &quot;key: value&quot; strings\n    # and then joins them with a comma and a space.\n    return ', '.join([f&quot;{key}: {value}&quot; for key, value in input_dict.items()])\n\n# Fun√ß√£o para extrair texto de um PDF e retornar uma lista de objetos Document\ndef extract_text_from_pdf(file_path):\n    try:\n        with open(file_path, 'rb') as pdf_file:\n            pdf = PyPDF2.PdfReader(pdf_file)\n            paginas = len(pdf.pages)\n            text = &quot;&quot;\n            for i in range(paginas):\n                page = pdf.pages[i]\n                text += page.extract_text()\n            # print(type(text))\n            text_splitter = RecursiveCharacterTextSplitter(\n                chunk_size=500,\n                chunk_overlap=50,\n                length_function=len,\n                separators=['\\n\\n\\n','\\n\\n','\\n', ' ', '']\n            )\n            documents = text_splitter.create_documents([text])\n            splitted_documents = text_splitter.split_documents(documents)\n            # print(documents)\n            # print(&quot;----------------------  vs  ---------------------&quot;)\n            # print(splitted_documents)\n            return splitted_documents\n        \n    except FileNotFoundError:\n        print(&quot;Arquivo n√£o encontrado&quot;)\n        return []\n\nclass criar_vectordb:\n\n    def save_db(self, documents, embeddings, db_path):\n        self.db_path = db_path\n        self.embeddings = embeddings\n        self.documents = documents\n        input=self.documents\n        vectordb = Chroma.from_documents(input, self.embeddings, persist_directory=self.db_path)\n        vectordb = None\n        vectordb = Chroma(db_path, embeddings)\n\n        return vectordb\n    \nembeddings = HuggingFaceEmbeddings(model_name=&quot;sentence-transformers/all-MiniLM-L6-v2&quot;, model_kwargs={'device':'cpu'})\n\n# Extraindo texto do PDF e criando a base de dados vetorial\ndocuments = extract_text_from_pdf(PDF_PATH)\n\nvectordb = criar_vectordb().save_db(documents, embeddings, CHROMA_DATA_PATH)\n\nos.environ[&quot;GROQ_API_KEY&quot;] = &quot;-&quot;\n\nruth_prompt_template = &quot;&quot;&quot;\n                            Voc√™ √© um assistente virtual de RH utilizando documentos para embasar sua resposta sempre em fatos,\n                            Use as informa√ß√µes presentes no documento para responder a resposta do candidato,\n                            sua resposta deve ser o mais semelhante poss√≠vel com a descri√ß√£o presente nos documentos\n                            \n                            contexto: {context}\n                            pergunta: {question}\n                            \n                            Apenas retorne as respostas √∫teis em ajudar na avalia√ß√£o e sele√ß√£o de candidatos e nada mais, usando uma linguagem gentil e emp√°tica.\n                            Sempre responda em portugu√™s, uma descri√ß√£o em texto cont√≠nua, al√©m disso adicione\n                            um ou mais emojis √†s vezes para demonstrar empatia e emo√ß√£o.\n                            \n                            \n                            &quot;&quot;&quot;\n\nprompt = PromptTemplate(template=ruth_prompt_template, input_variables=['context', 'question'])\n\n'''\nllm = CTransformers(\n        model = &quot;model/llama-2-7b-chat.ggmlv3.q8_0.bin&quot;,\n        model_type = &quot;llama&quot;,\n        config={'max_new_tokens': 512, \n                'temperature': 0.03,\n                'context_length': 1000,\n                'repetition_penalty': 1.15}\n        )\n'''\n\nllm = ChatGroq(model_name=&quot;llama3-70b-8192&quot;, api_key=os.environ[&quot;GROQ_API_KEY&quot;])\n\nretriever = vectordb.as_retriever(search_kwargs={&quot;k&quot;: 2})\ncombine_docs_chain = create_stuff_documents_chain(\n    llm, prompt\n)\n\nqa = create_retrieval_chain(retriever, combine_docs_chain)\n\n# Main\ndef main():\n    # Exemplo de uso\n    context = &quot;Feedback negativo&quot;\n    question = &quot;Como voc√™ lida com feedback negativo?&quot;\n    response = qa.invoke({&quot;input&quot;: {&quot;context&quot;: context, &quot;question&quot;: question}})\n    print(response)\n\nif __name__ == &quot;__main__&quot;:\n    main()\n</code></pre>\n<h1>This is the huggingface file:</h1>\n<pre><code>from typing import Any, Dict, List, Optional\n\nfrom langchain_core.embeddings import Embeddings\nfrom langchain_core.pydantic_v1 import BaseModel, Extra, Field\n\nDEFAULT_MODEL_NAME = &quot;sentence-transformers/all-mpnet-base-v2&quot;\n\n\nclass HuggingFaceEmbeddings(BaseModel, Embeddings):\n    &quot;&quot;&quot;HuggingFace sentence_transformers embedding models.\n\n    To use, you should have the ``sentence_transformers`` python package installed.\n\n    Example:\n        .. code-block:: python\n\n            from langchain_huggingface import HuggingFaceEmbeddings\n\n            model_name = &quot;sentence-transformers/all-mpnet-base-v2&quot;\n            model_kwargs = {'device': 'cpu'}\n            encode_kwargs = {'normalize_embeddings': False}\n            hf = HuggingFaceEmbeddings(\n                model_name=model_name,\n                model_kwargs=model_kwargs,\n                encode_kwargs=encode_kwargs\n            )\n    &quot;&quot;&quot;\n\n    client: Any  #: :meta private:\n    model_name: str = DEFAULT_MODEL_NAME\n    &quot;&quot;&quot;Model name to use.&quot;&quot;&quot;\n    cache_folder: Optional[str] = None\n    &quot;&quot;&quot;Path to store models. \n    Can be also set by SENTENCE_TRANSFORMERS_HOME environment variable.&quot;&quot;&quot;\n    model_kwargs: Dict[str, Any] = Field(default_factory=dict)\n    &quot;&quot;&quot;Keyword arguments to pass to the Sentence Transformer model, such as `device`,\n    `prompts`, `default_prompt_name`, `revision`, `trust_remote_code`, or `token`.\n    See also the Sentence Transformer documentation: https://sbert.net/docs/package_reference/SentenceTransformer.html#sentence_transformers.SentenceTransformer&quot;&quot;&quot;\n    encode_kwargs: Dict[str, Any] = Field(default_factory=dict)\n    &quot;&quot;&quot;Keyword arguments to pass when calling the `encode` method of the Sentence\n    Transformer model, such as `prompt_name`, `prompt`, `batch_size`, `precision`,\n    `normalize_embeddings`, and more.\n    See also the Sentence Transformer documentation: https://sbert.net/docs/package_reference/SentenceTransformer.html#sentence_transformers.SentenceTransformer.encode&quot;&quot;&quot;\n    multi_process: bool = False\n    &quot;&quot;&quot;Run encode() on multiple GPUs.&quot;&quot;&quot;\n    show_progress: bool = False\n    &quot;&quot;&quot;Whether to show a progress bar.&quot;&quot;&quot;\n\n    def __init__(self, **kwargs: Any):\n        &quot;&quot;&quot;Initialize the sentence_transformer.&quot;&quot;&quot;\n        super().__init__(**kwargs)\n        try:\n            import sentence_transformers  # type: ignore[import]\n\n        except ImportError as exc:\n            raise ImportError(\n                &quot;Could not import sentence_transformers python package. &quot;\n                &quot;Please install it with `pip install sentence-transformers`.&quot;\n            ) from exc\n\n        self.client = sentence_transformers.SentenceTransformer(\n            self.model_name, cache_folder=self.cache_folder, **self.model_kwargs\n        )\n\n    class Config:\n        &quot;&quot;&quot;Configuration for this pydantic object.&quot;&quot;&quot;\n\n        extra = Extra.forbid\n\n    def embed_documents(self, texts: List[str]) -&gt; List[List[float]]:\n        &quot;&quot;&quot;Compute doc embeddings using a HuggingFace transformer model.\n\n        Args:\n            texts: The list of texts to embed.\n\n        Returns:\n            List of embeddings, one for each text.\n        &quot;&quot;&quot;\n        import sentence_transformers  # type: ignore[import]\n\n        texts = list(map(lambda x: x.replace(&quot;\\n&quot;, &quot; &quot;), texts))\n        if self.multi_process:\n            pool = self.client.start_multi_process_pool()\n            embeddings = self.client.encode_multi_process(texts, pool)\n            sentence_transformers.SentenceTransformer.stop_multi_process_pool(pool)\n        else:\n            embeddings = self.client.encode(\n                texts, show_progress_bar=self.show_progress, **self.encode_kwargs\n            )\n\n        return embeddings.tolist()\n\n    def embed_query(self, text: str) -&gt; List[float]:\n        &quot;&quot;&quot;Compute query embeddings using a HuggingFace transformer model.\n\n        Args:\n            text: The text to embed.\n\n        Returns:\n            Embeddings for the text.\n        &quot;&quot;&quot;\n        return self.embed_documents([text])[0]\n</code></pre>\n<p>While debugging, I tried to use split_text() and split_documents() instead of create_documents() and it also didn't work, all of them give me the same output: this error, and my text still containing all of the &quot;\\n&quot;. I don't know if it could be something else in the code, as this is the only part that deals with separators.\nPlease help!\nThank you!</p>\n",
        "answer": "<p>I came across the same problem while implementing an evaluation using Ragas (<a href=\"https://docs.ragas.io/en/stable/howtos/integrations/langchain/\" rel=\"nofollow noreferrer\">https://docs.ragas.io/en/stable/howtos/integrations/langchain/</a>). The error occurred, in my case, due to difference in chain construction. Depending on that, we should have different implementations:</p>\n<p>In ragas official document:</p>\n<pre><code>def format_docs(relevant_docs):\n    return &quot;\\n&quot;.join(doc.page_content for doc in relevant_docs)\nprompt = ChatPromptTemplate.from_template(template)\nchain = prompt | llm | StrOutputParser()\ndef format_docs(relevant_docs):\n    return &quot;\\n&quot;.join(doc.page_content for doc in relevant_docs)\nquery = &quot;...?&quot;\nrelevant_docs = retriever.invoke(query)\nqa_chain.invoke({&quot;context&quot;: format_docs(relevant_docs), &quot;query&quot;: query})\n</code></pre>\n<p>In my previous code:</p>\n<pre><code>chain = (\n    {&quot;context&quot;: compression_retriever, &quot;question&quot;: RunnablePassthrough()}\n    | RunnableLambda(inspect)  # Add the inspector here to print the intermediate results\n    | prompt\n    | chat\n    | StrOutputParser()\n)\nresponse = chain.invoke(query)\n</code></pre>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 78704628,
            "link": "https://stackoverflow.com/questions/78704628/huggingface-library-not-being-able-to-replace-separators-in-create-documents-a"
        }
    },
    {
        "question": "How to broadcast a tensor from main process using Accelerate?\n<p>I want to do some computation in the main process and broadcast the tensor to other processes. Here is a sketch of what my code looks like currently:</p>\n<pre class=\"lang-py prettyprint-override\"><code>from accelerate.utils import broadcast\n\nx = None\nif accelerator.is_local_main_process:\n    x = &lt;do_some_computation&gt;\n    x = broadcast(x)  # I have even tried moving this line out of the if block\nprint(x.shape)\n</code></pre>\n<p>This gives me following error:\n<code>TypeError: Unsupported types (&lt;class 'NoneType'&gt;) passed to `_gpu_broadcast_one` . Only nested list/tuple/dicts of objects that are valid for `is_torch_tensor` s hould be passed.</code></p>\n<p>Which means that <code>x</code> is still <code>None</code> and is not really being broadcasted. How do I fix this?</p>\n",
        "answer": "<p>It might not be like that (about the position of <code>broadcast</code>). I've recently run into the same issue and figured out the solution as below:</p>\n<pre class=\"lang-py prettyprint-override\"><code>x = torch.zeros(shape0)\n\nif accelerator.is_main_process:\n    x = &lt;some computation here&gt;\n\nx = broadcast(x) # broadcast does a sending-or-receiving job. it should be out of the if block\n</code></pre>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 78165875,
            "link": "https://stackoverflow.com/questions/78165875/how-to-broadcast-a-tensor-from-main-process-using-accelerate"
        }
    },
    {
        "question": "Cannot load a gated model from hugginface despite having access and logging in\n<p>I am training a Llama-3.1-8B-Instruct model for a specific task.\nI have request the access to the huggingface repository, and got access, confirmed on the huggingface webapp dashboard.</p>\n<p>I tried calling the <code>huggingface_hub.login</code> function with the token to login and then download the model in the same script. I get an error, saying that I need to be logged in to access gated repositories.</p>\n<p>Then I tried loging in via the <code>huggingface-cli login</code> command, which succeeded. I got the same error after running the script.</p>\n<p>Then I tried the first approach again, but didn't pass the token, the documentation says I should get prompter for the token. The login function however seems to block after showing the HF logo, but does not show a prompt for the token.</p>\n<p>Is there something I'm missing here in order to access the models?</p>\n<p>My code:</p>\n<pre><code>hf_login()\n\nbase_model_name = 'meta-llama/Llama-3.1-8B-Instruct'\ntokenizer = AutoTokenizer.from_pretrained(base_model_name)  # this line causes error\nmodel = AutoModelForCausalLM.from_pretrained(base_model_name)\n</code></pre>\n<p>Error:</p>\n<pre><code>OSError: You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct.\n401 Client Error. (Request ID: Root=1-673f47aa-6b11aae44cd9c6523654070c;5816d1af-49a5-4262-bec0-dab7ecad66e4)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.\n</code></pre>\n<p>I'm sure I have access to the meta-llama/Llama-3.1 models. The <code>huggingface-cli whoami</code> command correctly returns my username, so I'm also logged in.</p>\n<p>My token is set to read access, I'm also trying with a write access one.</p>\n<p>EDIT: I generated a new write-access token. The login via the function <code>huggingface_hub.login</code> was successful. The models still weren't actually downloading. I tried from the windows terminal instead of the pycharm built-in terminal, and now it is working. Still don't know why it works now.</p>\n",
        "answer": "<p>you need to loging using huggingface acces token , befor getting to access of gated model, for get , if you have not any acces token, you can create from <strong>Access token</strong> section.</p>\n<p>from huggingface_hub import login</p>\n<p>login(token = &quot;hugging_face_access_token&quot;)</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 79211723,
            "link": "https://stackoverflow.com/questions/79211723/cannot-load-a-gated-model-from-hugginface-despite-having-access-and-logging-in"
        }
    },
    {
        "question": "Image segmentation ONNX from huggingface produces very diferent results when used in ML.Net\n<p>I have been trying to get an image segmentation model from huggingface (<a href=\"https://huggingface.co/briaai/RMBG-2.0\" rel=\"nofollow noreferrer\">RMBG-2.0</a>) to work for inference using ML.NET. After a lot of trial and error, I finally got the code to compile and produce an output but it is wildly different from the result i get from using the demo on huggingface.</p>\n<p>The code:</p>\n<pre><code>public static void RemoveGreenBackgroundAI2(string imagePath, string outputfile)\n{\n    string modelPath = Path.Combine( Application.StartupPath,&quot;ONNX&quot;,&quot;model.onnx&quot;); \n    MLContext mlContext = new MLContext();\n\n    var imageData = new ImageInputData\n    {\n        Image = MLImage.CreateFromFile (imagePath)\n    };\n\n\n    var imageDataView = mlContext.Data.LoadFromEnumerable(new[] { imageData });\n   \n   var pipeline = mlContext.Transforms.ResizeImages(\n                        outputColumnName: &quot;input&quot;,\n                        imageWidth: 1024,\n                        imageHeight: 1024,\n                        inputColumnName: nameof(ImageInputData.Image))\n                  .Append(mlContext.Transforms.ExtractPixels(\n                        outputColumnName: &quot;out1&quot;,\n                        inputColumnName: &quot;input&quot;,\n                        interleavePixelColors: true,\n                        scaleImage: 1f / 255f,\n                        offsetImage: 0,\n                        outputAsFloatArray: true))\n                    .Append(mlContext.Transforms.CustomMapping&lt;CustomMappingInput, CustomMappingOutput&gt;( \n                       mapAction: (input, output) =&gt;\n                        {\n                            output.pixel_values = new float[input.out1.Length];\n                            for (int i = 0; i &lt; input.out1.Length; i += 3)\n                            {\n                                // R\n                                output.pixel_values[i] = (input.out1[i] - 0.485f) / 0.229f;\n\n                                //G\n                                output.pixel_values[i + 1] = (input.out1[i + 1] - 0.456f) / 0.224f;\n\n                                //B\n                                output.pixel_values[i + 2] = (input.out1[i + 2] - 0.406f) / 0.225f;\n                            }\n                        }, contractName: null))\n                  .Append(mlContext.Transforms.ApplyOnnxModel(\n                        modelFile: modelPath,\n                        outputColumnNames: new[] { &quot;alphas&quot; },\n                        inputColumnNames: new[] { &quot;pixel_values&quot; },\n                        shapeDictionary: new Dictionary&lt;string, int[]&gt;\n                        {\n                            { &quot;pixel_values&quot;, new[] { 1, 3, 1024, 1024 } }\n\n                        },\n                        fallbackToCpu:true,\n                        gpuDeviceId:null\n                        ));\n\n    \n    var model = pipeline.Fit(imageDataView);\n    var predictionEngine = mlContext.Model.CreatePredictionEngine&lt;ImageInputData, ModelOutput&gt;(model);\n    var prediction = predictionEngine.Predict(imageData);\n    ApplyMaskAndSaveImage(imagePath, prediction, outputfile);\n\n}\n\npublic static void ApplyMaskAndSaveImage(string originalImagepath, ModelOutput prediction, string outputPath)\n{\n    int width = 1024;\n    int height = 1024;\n    float[] outputData = prediction.Output;\n\n    Bitmap originalImage = (Bitmap)Bitmap.FromFile(originalImagepath);\n    int originalWidth = originalImage.Width;\n    int originalHeight = originalImage.Height;\n\n    Bitmap resizedImage = new Bitmap(originalImage, new System.Drawing.Size(width, height));\n    Bitmap outputImage = new Bitmap(width, height, PixelFormat.Format32bppArgb);\n\n    for (int y = 0; y &lt; height; y++)\n    {\n        for (int x = 0; x &lt; width; x++)\n        {\n            float maskValue = outputData[y * width + x];\n            float threshold = 0.5f;\n            byte alpha = maskValue &gt;= threshold ? (byte)255 : (byte)0;\n            Color pixelColor = resizedImage.GetPixel(x, y);\n            Color newColor = Color.FromArgb(alpha, pixelColor.R, pixelColor.G, pixelColor.B);\n            outputImage.SetPixel(x, y, newColor);\n        }\n    }      \n    outputImage.Save(outputPath, ImageFormat.Png);\n}\n\npublic class ModelOutput\n{\n    [ColumnName(&quot;alphas&quot;)]\n    [VectorType(1, 1, 1024, 1024)]\n    public float[] Output { get; set; }\n}\npublic class ImageInputData\n{\n    [ColumnName(&quot;Image&quot;)]\n    [ImageType(1024, 1024)]\n    public MLImage Image { get; set; }\n}\npublic class CustomMappingInput\n{\n    [VectorType(3, 1024, 1024)]\n    public float[] out1 { get; set; }\n}\npublic class CustomMappingOutput\n{\n    [VectorType(3, 1024, 1024)]\n    public float[] pixel_values { get; set; } \n}\n</code></pre>\n<p>I know the code is far from optimal (<code>GetPixel()</code>and <code>SetPixel()</code>have to be replaced amongst other things), and that the aspect ratio of my result is wrong because I have not scaled the image back to the original dimensions. First I would like to get the background removal working correctly.</p>\n<p>Any advice or idea of what I might be doind incorrectly?</p>\n<p>BTW, the onnx file is available in the RMBG-2.0 link at the beginning. There is also a code snippet in python for using the model and that is why I am applying thosee transformations to the image in the pipeline.</p>\n<p><a href=\"https://i.sstatic.net/oTJSO60A.jpg\" rel=\"nofollow noreferrer\">Input Image</a></p>\n<p><a href=\"https://i.sstatic.net/ZLouZcpm.png\" rel=\"nofollow noreferrer\">Expected result</a></p>\n<p><a href=\"https://i.sstatic.net/pB1FKT6f.png\" rel=\"nofollow noreferrer\">Result I am getting</a></p>\n",
        "answer": "<p>I finally solved the problem going at it from another angle. Using the Ml.OnnxRuntime and ImageSharp greatly simplified the task.</p>\n<p>Here is the working code:</p>\n<pre><code>public class ImageSegmentationService : IDisposable\n{\n    private readonly InferenceSession _session;\n    private const int ImageSize = 1024;\n\n    public ImageSegmentationService(string modelPath)\n    {\n        _session = new InferenceSession(modelPath);\n    }\n\n    public float[] ProcessImage(string imagePath)\n    {\n        using var image = Image.Load&lt;Rgb24&gt;(imagePath);\n        image.Mutate(x =&gt; x.Resize(ImageSize, ImageSize));\n    \n        // Prepare input tensor (normalize to [0,1] and convert to NCHW)\n        var inputTensor = new DenseTensor&lt;float&gt;(new[] { 1, 3, ImageSize, ImageSize });\n    \n        for (int y = 0; y &lt; ImageSize; y++)\n        {\n            for (int x = 0; x &lt; ImageSize; x++)\n            {\n                var pixel = image[x, y];\n                inputTensor[0, 0, y, x] = pixel.R / 255f;\n                inputTensor[0, 1, y, x] = pixel.G / 255f;\n                inputTensor[0, 2, y, x] = pixel.B / 255f;\n            }\n        }\n\n        // Run inference\n        var inputs = new List&lt;NamedOnnxValue&gt; \n        { \n            NamedOnnxValue.CreateFromTensor(&quot;pixel_values&quot;, inputTensor) \n        };\n\n        using var outputs = _session.Run(inputs);\n        var alphas = outputs.First().AsTensor&lt;float&gt;();\n        \n        return alphas.ToArray();\n    }\n\n    public void Dispose()\n    {\n        _session?.Dispose();\n    }\n\n    public class RmbgInput\n    {\n        [VectorType(1, 3, 1024, 1024)]\n        public float[] pixel_values { get; set; }\n    }\n\n    public class RmbgOutput\n    {\n        [VectorType(1, 1024, 1024)]\n        public float[] alphas { get; set; }\n    }\n}\n</code></pre>\n<p>The result of <code>ProcessImage(string imagePath)</code> is the alpha mask that should be applied to the original image (in 1024*1024 dimensions) to remove the background.</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 79411192,
            "link": "https://stackoverflow.com/questions/79411192/image-segmentation-onnx-from-huggingface-produces-very-diferent-results-when-use"
        }
    },
    {
        "question": "Get attention masks from HF pipelines\n<p>How should returned attention masks be accessed from the FeatureExtractionPipeline in Huggingface?</p>\n<p>The code below takes an embedding model, distributes it and a huggingface dataset across 8 GPUs on a single node, and performs inference on the inputs. The code requires the attention masks for mean pooling.</p>\n<p>Code example:</p>\n<pre class=\"lang-py prettyprint-override\"><code>from accelerate import Accelerator\nfrom accelerate.utils import tqdm\nfrom transformers import AutoTokenizer, AutoModel\nfrom optimum.bettertransformer import BetterTransformer\n\nimport torch\n\nfrom datasets import load_dataset\n\nfrom transformers import pipeline\n\naccelerator = Accelerator()\n\nmodel_name = &quot;BAAI/bge-large-en-v1.5&quot;\n\ntokenizer = AutoTokenizer.from_pretrained(model_name,)\n\nmodel = AutoModel.from_pretrained(model_name,)\n\npipe = pipeline(\n    &quot;feature-extraction&quot;,\n    model=model,\n    tokenizer=tokenizer,\n    max_length=512,\n    truncation=True,\n    padding=True,\n    pad_to_max_length=True,\n    batch_size=256,\n    framework=&quot;pt&quot;,\n    return_tensors=True,\n    return_attention_mask=True,\n    device=(accelerator.device)\n)\n\ndataset = load_dataset(\n    &quot;wikitext&quot;,\n    &quot;wikitext-2-v1&quot;,\n    split=&quot;train&quot;,\n)\n\n#Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n\n# Assume 8 processes\n\nwith accelerator.split_between_processes(dataset[&quot;text&quot;]) as data:\n\n    for out in pipe(data):\n\n        sentence_embeddings = mean_pooling(out, out[&quot;attention_mask&quot;])\n</code></pre>\n<p>I need the attention maks from pipe to use for mean pooling.</p>\n<p>Best,</p>\n<p>Enrico</p>\n",
        "answer": "<p>You can often directly access the tokenizer from the pipe and call it with your string to get the attention mask:</p>\n<pre><code>&gt;&gt;&gt; pipe.tokenizer(&quot;Blah blah blah.&quot;)\n{'input_ids': [101, 27984, 27984, 27984, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1]}\n\n&gt;&gt;&gt; pipe.tokenizer(&quot;Blah blah blah.&quot;)['attention_mask']\n{'attention_mask': [1, 1, 1, 1, 1, 1]}\n</code></pre>\n<p>But even if that's not an option, it looks like you have access to the tokenizer at initialization.  Why not use that directly?</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 77603219,
            "link": "https://stackoverflow.com/questions/77603219/get-attention-masks-from-hf-pipelines"
        }
    },
    {
        "question": "Target modules for applying PEFT / LoRA on different models\n<p>I am looking at a few <a href=\"https://colab.research.google.com/drive/1BiQiw31DT7-cDp1-0ySXvvhzqomTdI-o#scrollTo=NuAx3zBeUL1q\" rel=\"noreferrer\">different</a> <a href=\"https://www.philschmid.de/fine-tune-flan-t5-peft\" rel=\"noreferrer\">examples</a> of using PEFT on different models. The <code>LoraConfig</code> object contains a <code>target_modules</code> array. In some examples, the target modules are <code>[&quot;query_key_value&quot;]</code>, sometimes it is <code>[&quot;q&quot;, &quot;v&quot;]</code>, sometimes something else.</p>\n<p>I don't quite understand where the values of the target modules come from. Where in the model page should I look to know what the LoRA adaptable modules are?</p>\n<p>One example (for the model Falcon 7B):</p>\n<pre><code>peft_config = LoraConfig(\n    lora_alpha=lora_alpha,\n    lora_dropout=lora_dropout,\n    r=lora_r,\n    bias=&quot;none&quot;,\n    task_type=&quot;CAUSAL_LM&quot;,\n    target_modules=[\n        &quot;query_key_value&quot;,\n        &quot;dense&quot;,\n        &quot;dense_h_to_4h&quot;,\n        &quot;dense_4h_to_h&quot;,\n    ]\n</code></pre>\n<p>Another example (for the model Opt-6.7B):</p>\n<pre><code>config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[&quot;q_proj&quot;, &quot;v_proj&quot;],\n    lora_dropout=0.05,\n    bias=&quot;none&quot;,\n    task_type=&quot;CAUSAL_LM&quot;\n)\n</code></pre>\n<p>Yet another (for the model Flan-T5-xxl):</p>\n<pre><code>lora_config = LoraConfig(\n r=16,\n lora_alpha=32,\n target_modules=[&quot;q&quot;, &quot;v&quot;],\n lora_dropout=0.05,\n bias=&quot;none&quot;,\n task_type=TaskType.SEQ_2_SEQ_LM\n)\n</code></pre>\n",
        "answer": "<p>I will add another answer as none of present ones feel complete/general for me.</p>\n<p>To solve the original question, getting a list of Lora compatible modules programmatically, I have tried using</p>\n<pre><code>target_modules = 'all-linear',\n</code></pre>\n<p>which seems available in latest PEFT versions.\nHowever, that would raise an error when applying to <code>google/gemma-2b</code> model.\n(dropout layers were for some reason added to the <code>target_modules</code>, see later for the layers supported by LORA).</p>\n<p>From documentation of the PEFT library:</p>\n<pre><code>only the following modules: `torch.nn.Linear`, `torch.nn.Embedding`, `torch.nn.Conv2d`, `transformers.pytorch_utils.Conv1D`.\n</code></pre>\n<p>I ended up creating this function for getting all Lora compatible modules from arbitrary models:</p>\n<pre><code>import torch\nfrom transformers import Conv1D\n\ndef get_specific_layer_names(model):\n    # Create a list to store the layer names\n    layer_names = []\n    \n    # Recursively visit all modules and submodules\n    for name, module in model.named_modules():\n        # Check if the module is an instance of the specified layers\n        if isinstance(module, (torch.nn.Linear, torch.nn.Embedding, torch.nn.Conv2d, Conv1D)):\n            # model name parsing \n\n            layer_names.append('.'.join(name.split('.')[4:]).split('.')[0])\n    \n    return layer_names\n\nlist(set(get_specific_layer_names(model)))\n</code></pre>\n<p>Which yields on gemma-2B</p>\n<pre><code>[\n 'down_proj',\n 'o_proj',\n 'k_proj',\n 'q_proj',\n 'gate_proj',\n 'up_proj',\n 'v_proj']\n</code></pre>\n<p>This list was valid for a target_modules selection</p>\n<pre><code>peft.__version__\n'0.10.1.dev0'\n\ntransformers.__version__\n'4.39.1'\n</code></pre>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 76768226,
            "link": "https://stackoverflow.com/questions/76768226/target-modules-for-applying-peft-lora-on-different-models"
        }
    },
    {
        "question": "Target modules for applying PEFT / LoRA on different models\n<p>I am looking at a few <a href=\"https://colab.research.google.com/drive/1BiQiw31DT7-cDp1-0ySXvvhzqomTdI-o#scrollTo=NuAx3zBeUL1q\" rel=\"noreferrer\">different</a> <a href=\"https://www.philschmid.de/fine-tune-flan-t5-peft\" rel=\"noreferrer\">examples</a> of using PEFT on different models. The <code>LoraConfig</code> object contains a <code>target_modules</code> array. In some examples, the target modules are <code>[&quot;query_key_value&quot;]</code>, sometimes it is <code>[&quot;q&quot;, &quot;v&quot;]</code>, sometimes something else.</p>\n<p>I don't quite understand where the values of the target modules come from. Where in the model page should I look to know what the LoRA adaptable modules are?</p>\n<p>One example (for the model Falcon 7B):</p>\n<pre><code>peft_config = LoraConfig(\n    lora_alpha=lora_alpha,\n    lora_dropout=lora_dropout,\n    r=lora_r,\n    bias=&quot;none&quot;,\n    task_type=&quot;CAUSAL_LM&quot;,\n    target_modules=[\n        &quot;query_key_value&quot;,\n        &quot;dense&quot;,\n        &quot;dense_h_to_4h&quot;,\n        &quot;dense_4h_to_h&quot;,\n    ]\n</code></pre>\n<p>Another example (for the model Opt-6.7B):</p>\n<pre><code>config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[&quot;q_proj&quot;, &quot;v_proj&quot;],\n    lora_dropout=0.05,\n    bias=&quot;none&quot;,\n    task_type=&quot;CAUSAL_LM&quot;\n)\n</code></pre>\n<p>Yet another (for the model Flan-T5-xxl):</p>\n<pre><code>lora_config = LoraConfig(\n r=16,\n lora_alpha=32,\n target_modules=[&quot;q&quot;, &quot;v&quot;],\n lora_dropout=0.05,\n bias=&quot;none&quot;,\n task_type=TaskType.SEQ_2_SEQ_LM\n)\n</code></pre>\n",
        "answer": "<p>Here method to get all linear.</p>\n<pre><code>import bitsandbytes as bnb\n\ndef find_all_linear_names(model):\n    lora_module_names = set()\n    for name, module in model.named_modules():\n        if isinstance(module, bnb.nn.Linear4bit):\n            names = name.split(&quot;.&quot;)\n            # model-specific\n            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n\n    if &quot;lm_head&quot; in lora_module_names:  # needed for 16-bit\n        lora_module_names.remove(&quot;lm_head&quot;)\n    return list(lora_module_names)\n</code></pre>\n<p>In a future release you can directly use <code>target_modules=&quot;all-linear&quot;</code> in your LoraConfig</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 76768226,
            "link": "https://stackoverflow.com/questions/76768226/target-modules-for-applying-peft-lora-on-different-models"
        }
    },
    {
        "question": "Unable to import transformers.models.bert.modeling_tf_bert on macOS?\n<p>As the title is self-descriptive, I'm not able to import the <code>BertTokenizer</code> and <code>TFBertModel</code> classes from the <code>transformers</code> package through the following code:</p>\n<pre><code>from transformers import BertTokenizer, TFBertModel\n\ntokenizer = BertTokenizer.from_pretrained(BERT_PATH)\nmodel = TFBertModel.from_pretrained(BERT_PATH)\ntext = &quot;Replace me by any text you'd like.&quot;\nencoded_input = tokenizer(text, return_tensors='tf')\nresp = model(encoded_input)\nprint(resp)\n</code></pre>\n<p>As a result, I'm getting the following error:</p>\n<pre><code>RuntimeError: Failed to import transformers.models.bert.modeling_tf_bert because of the following error (look up to see its traceback):\ndlopen(/Users/tk/miniforge3/envs/QA-benchmark/lib/python3.10/site-packages/tensorflow-plugins/libmetal_plugin.dylib, 0x0006): symbol not found in flat namespace '_TF_GetInputPropertiesList'\n</code></pre>\n<p>Here is my software stack:</p>\n<pre><code>OS: macOS Ventura 13.3.1\nPython: 3.10\nTensorFlow: macOS-tensorflow 2.9.0\nTransformers: 4.28.0\nBERT model: uncased_L-12_H-768_A-12\n</code></pre>\n<p>p.s. I've already posted this issue on the GitHub repository of transformers.</p>\n",
        "answer": "<p>This issue has happened to me several times. This could be a potential dependency issues. Some library may want numpy(or any other library) &gt;=x and some library want numpy &gt;=y so you need to find a common library version which supported by both.</p>\n<p><strong>(look up to see its traceback):</strong></p>\n<p>Please check full trace for exact error. Post full stack trace for more accurate dependency causing error</p>\n<p>OS : Mac 15\nVersion: Python 3.10</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 76025069,
            "link": "https://stackoverflow.com/questions/76025069/unable-to-import-transformers-models-bert-modeling-tf-bert-on-macos"
        }
    },
    {
        "question": "ModuleNotFoundError: No module named &#39;huggingface_hub.utils&#39; using Anaconda\n<p>I'm trying to execute the example code of the huggingface website:</p>\n<pre class=\"lang-py prettyprint-override\"><code>from transformers import GPTJTokenizer, TFGPTJModel\nimport tensorflow as tf\n\ntokenizer = GPTJTokenizer.from_pretrained(&quot;EleutherAI/gpt-j-6B&quot;)\nmodel = TFGPTJModel.from_pretrained(&quot;EleutherAI/gpt-j-6B&quot;)\n\ninputs = tokenizer(&quot;Hello, my dog is cute&quot;, return_tensors=&quot;tf&quot;)\noutputs = model(inputs)\n\nlast_hidden_states = outputs.last_hidden_state\n</code></pre>\n<p>I'm using anaconda and I installed the transformers package beforehand with <code>conda install -c huggingface transformers</code> as explained in the <a href=\"https://huggingface.co/docs/transformers/installation#install-with-conda\" rel=\"noreferrer\">documentation</a>. But I still get this error, when I'm trying to execute the code. Following error message pops up: <code>ModuleNotFoundError: No module named 'huggingface_hub.utils'</code></p>\n<p>How to resolve this error?</p>\n",
        "answer": "<p>This still happened to me today, so here is what I did.</p>\n<p>I looked up the path to my conda env with</p>\n<pre><code>conda info\n\n# output\n    active env location : /home/jovyan/my-conda-envs/myenv\n# more output\n</code></pre>\n<p>I located the <code>huggingface_hub</code> package at <code>/home/jovyan/my-conda-envs/myenv/lib/python3.10/site-packages/huggingface_hub</code> and saw that it contained a file named <code>LICENSE</code> only.</p>\n<p>So I force-reinstalled <code>huggingface_hub</code> with:</p>\n<pre><code>conda install --force-reinstall --update-deps huggingface_hub\n</code></pre>\n<p>The module now contains many more files, besides <code>LICENSE</code>, the error is gone.</p>\n<p>This solved the problem for me.</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 73960201,
            "link": "https://stackoverflow.com/questions/73960201/modulenotfounderror-no-module-named-huggingface-hub-utils-using-anaconda"
        }
    },
    {
        "question": "How can I adjust the performance of tokenizer?\n<p>Working with the tokenizer from the <code>transformers</code> library of Hugging Face. The tokenizer works fine in most cases, but in some cases, it does not.</p>\n<p>I'm wondering if I can <strong>&quot;adjust&quot;</strong> (not train a new tokenizer from scratch) the performance of the tokenizer to handle the bad cases while still maintaining good performance in most cases as it used to.</p>\n<p>To be more specific, the type of tokenizer is <code>transformers.XLMRobertaTokenizerFast</code>, which is a unigram tokenizer, and the model is <code>paraphrase-multilingual-mpnet-base-v2</code>.</p>\n",
        "answer": "<p>You can change the tokenizer's vocabulary:</p>\n<pre><code>tokenizer.add_tokens([&quot;asadaf&quot;, &quot;sdfsaf&quot;])\nmodel.resize_token_embeddings(len(tokenizer)) # change input embeddings size\ninput_text = &quot;This is asadaf and sdfsaf&quot;\nprint(tokenizer(input_text))\n</code></pre>\n<p>As a result, <em>asadaf</em> and <em>sdfsaf</em> would be tokenized as unique words.</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 79100835,
            "link": "https://stackoverflow.com/questions/79100835/how-can-i-adjust-the-performance-of-tokenizer"
        }
    },
    {
        "question": "How to configure HuggingFaceEndpoint in Langchain\n<p>I'm trying to use this model</p>\n<pre><code>from langchain_huggingface import HuggingFaceEndpoint\nrepo_id=&quot;google/flan-t5-large&quot;\nhuggingface_llm = HuggingFaceEndpoint(\nhuggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN,\nrepo_id=repo_id,\ntemperature=0,\nmax_new_tokens=200)\n\nfrom langchain.prompts import PromptTemplate\ndef flan_process(tema, pregunta):\ntemplate = &quot;Eres un experto asistente en {tema}. Responde a la siguiente pregunta: {pregunta}&quot;\nprompt=PromptTemplate(template=template,input_variables=[&quot;tema&quot;,&quot;pregunta&quot;])\n\nflan_chain = prompt | huggingface_llm\n\nrespuesta=flan_chain.invoke({&quot;tema&quot;:tema, &quot;pregunta&quot;:pregunta})\n\nreturn respuesta\n\ntema=input(&quot;Ingrese el tema: &quot;)\npregunta=input(&quot;Ingrese la pregunta: &quot;)\n\nflan_reply=flan_process(tema, pregunta)\nprint(f&quot;Respuesta Flan: {flan_reply}&quot;)\n</code></pre>\n<p>But I always get this error The following <code>model_kwargs</code> are not used by the model: ['return_full_text', 'watermark', 'stop_sequences', 'stop'] (note: typos in the generate arguments will also show up in this list)</p>\n<p>Any idea please?</p>\n<p>Thanks</p>\n",
        "answer": "<p><code>HuggingFaceEndpoint</code> used to work with <code>google/flan-t5-large</code> but not any more. In case, you're still looking for a free model to try, here's what I'm using</p>\n<pre class=\"lang-py prettyprint-override\"><code>from langchain_huggingface import HuggingFaceEndpoint\nfrom langchain.chains import LLMChain\nrepo_id = &quot;mistralai/Mistral-7B-Instruct-v0.2&quot;\n# repo_id = &quot;meta-llama/Llama-3.2-1B&quot;\nllm = HuggingFaceEndpoint(\n    repo_id = repo_id,\n    temperature = 0.5,\n    huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN,\n    max_new_tokens = 250,\n)\n</code></pre>\n<p>Mistral-7B and Llama3.2 are quite nice models and free for access through API</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 78617530,
            "link": "https://stackoverflow.com/questions/78617530/how-to-configure-huggingfaceendpoint-in-langchain"
        }
    },
    {
        "question": "Checkpoints ValueError with downloading HuggingFace models\n<p>I am having trouble downloading deepseek_vl_v2 into my computer.</p>\n<p>Here is the error in my terminal</p>\n<blockquote>\n<p>ValueError: The checkpoint you are trying to load has model type\n<code>deepseek_vl_v2</code> but Transformers does not recognize this\narchitecture. This could be because of an issue with the checkpoint,\nor because your version of Transformers is out of date.</p>\n<p>You can update Transformers with the command <code>pip install --upgrade transformers</code>. If this does not work, and the checkpoint is very new,\nthen there may not be a release version that supports this model yet.\nIn this case, you can get the most up-to-date code by installing\nTransformers from source with the command <code>pip install git+https://github.com/huggingface/transformers.git</code></p>\n</blockquote>\n<p>However, I have downloaded many HuggingFace models before and never seemed to have had this issue, which means I should have all the libraries all correctly downloaded. I have already tried updating the transformers library and installing from the direct source, but neither have resolved the issue.</p>\n<p>Please let me know if there any more information I can provide.</p>\n<p>Thanks so much in advance!</p>\n",
        "answer": "<p>Following up on this question I asked, I found the solution <a href=\"https://huggingface.co/deepseek-ai/deepseek-vl2/discussions/3\" rel=\"nofollow noreferrer\">here</a></p>\n<p>Basically, DeepSeek is not a model supported by HuggingFace's transformer library, so the only option for downloading this model is through importing the model source code directly as of now.</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 79401652,
            "link": "https://stackoverflow.com/questions/79401652/checkpoints-valueerror-with-downloading-huggingface-models"
        }
    },
    {
        "question": "How to force langchain to use HF_DATA environment variable to load the model from local disk instead of Internet\n<p>How to force langchain to use <code>HF_DATA</code> environment variable to load the model.</p>\n<p>The <code>Snowflake/snowflake-arctic-embed-l</code> model files have been downloaded to <code>$HF_HOME/Snowflake/snowflake-arctic-embed-l</code>.</p>\n<pre><code>$ echo $HF_HOME\n/tmp-data\n\n$ls /tmp-data/Snowflake/snowflake-arctic-embed-l\n1_Pooling    README.md    config_sentence_transformers.json  model.safetensors  sentence_bert_config.json  tokenizer.json         vocab.txt\n2_Normalize  config.json  hoge.tgz                           modules.json       special_tokens_map.json    tokenizer_config.json\n</code></pre>\n<p>Python runtime acknowledges <code>HF_DATA</code> environment variable.</p>\n<pre><code>&gt;&gt;&gt; import os\n&gt;&gt;&gt; os.getenv(&quot;HF_HOME&quot;)\n'/tmp-data'\n</code></pre>\n<p>However, it tries to download the model from the internet.</p>\n<pre><code>from langchain.text_splitter import SentenceTransformersTokenTextSplitter\n\nsplitter=SentenceTransformersTokenTextSplitter(\n  model_name=&quot;Snowflake/snowflake-arctic-embed-l&quot;, \n  tokens_per_chunk=500, \n  chunk_overlap=50\n)\n</code></pre>\n<pre><code>No sentence-transformers model found with name Snowflake/snowflake-arctic-embed-l. Creating a new one with mean pooling.\nTraceback (most recent call last):\n  File &quot;/usr/local/lib/python3.10/dist-packages/urllib3/connection.py&quot;, line 198, in _new_conn\n    sock = connection.create_connection(\n  File &quot;/usr/local/lib/python3.10/dist-packages/urllib3/util/connection.py&quot;, line 60, in create_connection\n    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n  File &quot;/usr/lib/python3.10/socket.py&quot;, line 955, in getaddrinfo\n    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\nsocket.gaierror: [Errno -2] Name or service not known\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File &quot;/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py&quot;, line 787, in urlopen\n    response = self._make_request(\n  File &quot;/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py&quot;, line 488, in _make_request\n    raise new_e\n  File &quot;/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py&quot;, line 464, in _make_request\n    self._validate_conn(conn)\n  File &quot;/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py&quot;, line 1093, in _validate_conn\n    conn.connect()\n  File &quot;/usr/local/lib/python3.10/dist-packages/urllib3/connection.py&quot;, line 704, in connect\n    self.sock = sock = self._new_conn()\n  File &quot;/usr/local/lib/python3.10/dist-packages/urllib3/connection.py&quot;, line 205, in _new_conn\n    raise NameResolutionError(self.host, self, e) from e\nurllib3.exceptions.NameResolutionError: &lt;urllib3.connection.HTTPSConnection object at 0x7d1562b96680&gt;: Failed to resolve 'huggingface.co' ([Errno -2] Name or service not known)\n</code></pre>\n<p>Giving the full path to the local model directory fixes the issue but need to utilise <code>HF_HOME</code>.</p>\n<pre><code>splitter=SentenceTransformersTokenTextSplitter(model_name=&quot;/tmp-data/Snowflake/snowflake-arctic-embed-l&quot;, tokens_per_chunk=500, chunk_overlap=50)\nYou try to use a model that was created with version 3.4.1, however, your version is 3.0.0. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n</code></pre>\n",
        "answer": "<p>There's no built-in support for <code>HF_HOME</code> in langchain (or it's dependencies - <a href=\"https://github.com/UKPLab/sentence-transformers\" rel=\"nofollow noreferrer\">https://github.com/UKPLab/sentence-transformers</a> in this case). You'll just have to set up a method to prefix your model-names with the <code>HF_HOME</code> variable on your end. Unless you're open to monkey patching the library, but I'm not suggesting that for something like this.</p>\n<p>Something like this perhaps:</p>\n<pre><code>from langchain.text_splitter import SentenceTransformersTokenTextSplitter\nimport os\n\n\nHF_HOME = os.environ.get(&quot;HF_HOME&quot;)\n\n\ndef prefix_hf_home(model_name):\n    return os.path.join(HF_HOME, model_name)\n\n\nsplitter=SentenceTransformersTokenTextSplitter(\n  model_name=prefix_hf_home(&quot;Snowflake/snowflake-arctic-embed-l&quot;), \n  tokens_per_chunk=500, \n  chunk_overlap=50\n)\n</code></pre>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 79469538,
            "link": "https://stackoverflow.com/questions/79469538/how-to-force-langchain-to-use-hf-data-environment-variable-to-load-the-model-fro"
        }
    },
    {
        "question": "How to quantize a HF safetensors model and save it to llama.cpp GGUF format with less than q8_0 quantization?\n<p>I'm developing LLM agents using llama.cpp as inference engine. Sometimes I want to use models in safetensors format and there is a python script (<a href=\"https://github.com/ggerganov/llama.cpp/blob/master/convert_hf_to_gguf.py\" rel=\"nofollow noreferrer\">https://github.com/ggerganov/llama.cpp/blob/master/convert_hf_to_gguf.py</a>) to convert.</p>\n<p>Script is awesome, but minimum number size is 8 bit (q8_0). Is there any other script or repo with other quantization formats?</p>\n",
        "answer": "<p>First step is convert huggingface model to gguf (16b float or 32b float is recommended) using <code>convert_hf_to_gguf.py</code> from llama.cpp repository.</p>\n<p>Second step is use compiled c++ code from <code>/examples/quantize/</code> subdirectory of llama.cpp (<a href=\"https://github.com/ggerganov/llama.cpp/tree/master/examples/quantize\" rel=\"nofollow noreferrer\">https://github.com/ggerganov/llama.cpp/tree/master/examples/quantize</a>)</p>\n<p>Process is pretty straightforward and well-documented.</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 78842047,
            "link": "https://stackoverflow.com/questions/78842047/how-to-quantize-a-hf-safetensors-model-and-save-it-to-llama-cpp-gguf-format-with"
        }
    }
]