[
    {
        "question": "Deconstructiong the Stable Diffusion 3.5 pipeline\n<p>I am trying to deconstruct the SD3.5 (specifically 3.5 medium) pipeline in order to have a controlled process over the denoising steps. I can't do callbacks because I need to modify the latent according to other pipelines.</p>\n<p>I am trying to perform the steps on the following huggingface guide:\n<a href=\"https://huggingface.co/docs/diffusers/en/using-diffusers/write_own_pipeline#deconstruct-the-stable-diffusion-pipeline\" rel=\"nofollow noreferrer\">https://huggingface.co/docs/diffusers/en/using-diffusers/write_own_pipeline#deconstruct-the-stable-diffusion-pipeline</a></p>\n<p>I modified my text encoding to fit SD3.5, I also tried to load the entire pipeline and just run the encode_prompt function on it to get the text embedding and pooled embeddings for both the prompt and negative prompt. When running the function and putting its outputs as input to the regular pipeline instead of the prompt and negative prompt it works properly so it seems like this is not what's causing the problem.</p>\n<p>I also changed the unet from the article to use the pre-trained transformer of the model. After that I adjusted the decoding to match the same decoding on the pipeline's source code on diffusers.</p>\n<p>the output images don't look same as they are looking when running the pipeline through diffusers. I'm not sure where I can find a similar implementation to deconstruction of the SD3 pipeline or what am I missing.</p>\n<p><a href=\"https://i.sstatic.net/xyW0hOiI.png\" rel=\"nofollow noreferrer\">enter image description here</a></p>\n",
        "answer": "<p>It looks like the issue might be with the noise scheduler or latent processing. Make sure your scheduler settings match the default in diffusers, and check if the UNet’s predicted noise aligns with the official pipeline. If the results are still off, compare the shape and scale of your latents.</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 79494728,
            "link": "https://stackoverflow.com/questions/79494728/deconstructiong-the-stable-diffusion-3-5-pipeline"
        }
    },
    {
        "question": "How does one set the pad token correctly (not to eos) during fine-tuning to avoid model not predicting EOS?\n<h2>tldr; what I really want to know is what is the official way to set pad token for <strong>fine tuning</strong> it wasn't set during original training, so that it doesn't not learn to predict EOS.</h2>\n<p>colab: <a href=\"https://colab.research.google.com/drive/1poFdFYmkR_rDM5U5Z2WWjTepMQ8hvzNc?usp=sharing\" rel=\"nofollow noreferrer\">https://colab.research.google.com/drive/1poFdFYmkR_rDM5U5Z2WWjTepMQ8hvzNc?usp=sharing</a></p>\n<hr />\n<p>The HF falcon tutorial has the following line:</p>\n<pre><code>tokenizer.pad_token = tokenizer.eos_token\n</code></pre>\n<p>it looks strange to me. It make sense pad and eos are the same but then why even make a difference between them in the first place in general?</p>\n<p>Note its wrong to do pad = eos. This means during fine-tuning the model will never be trained to output eos (most likely) since eos is treated as pad token and no back propagated:</p>\n<pre><code>I just observed that when I set tokenizer.pad_token = tokenizer.eos_token during training, the model won't stop generating during inference, since it was trained to not output the eos token (per discussions above).\n</code></pre>\n<p>I saw this (here <a href=\"https://github.com/huggingface/transformers/issues/22794\" rel=\"nofollow noreferrer\">https://github.com/huggingface/transformers/issues/22794</a>):</p>\n<pre><code>tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n</code></pre>\n<p>But this assumes the model has a pad_token. I think an additional check has to be done that it does have an embedding for pad_token so that there are no run time errors (~type errors in the matrix extraction from the embedding &quot;table&quot;/matrix).</p>\n<p>But if one does that some care might be needed to initialize the new token so that it dominates the generation: <a href=\"https://nlp.stanford.edu/%7Ejohnhew/vocab-expansion.html\" rel=\"nofollow noreferrer\">https://nlp.stanford.edu/~johnhew/vocab-expansion.html</a></p>\n<hr />\n<p>code:</p>\n<pre class=\"lang-py prettyprint-override\"><code>def get_model_tokenizer_qlora_falcon7b(model_name: str = &quot;ybelkada/falcon-7b-sharded-bf16&quot;,\n                                       config: wand.Config,  # todo\n                                       lora_alpha=16,  # todo\n                                       lora_dropout=0.1,  # todo\n                                       lora_r=64,  # todo\n                                       bnb_4bit_compute_dtype=torch.float16,  # changed it from Guanaco hf\n                                       ) -&gt; tuple:\n    &quot;&quot;&quot;\n    Load the Falcon 7B model, quantize it in 4bit and attach LoRA adapters on it.\n\n    bf16 = 1S, 7Exp, 8Mantissa\n\n    Do:\n        pip install bitsandbytes\n    ref:\n        - https://colab.research.google.com/drive/1DOi8MFv4SWN9NImVornZ7t6BgmLoPQO-#scrollTo=AjB0WAqFSzlD\n    &quot;&quot;&quot;\n    from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoTokenizer\n\n    # model_id = &quot;tiiuae/falcon-7b&quot;\n    # model_name: str = &quot;ybelkada/falcon-7b-sharded-bf16&quot;\n\n    # - get bnb config for bit-4 base model (bnb lib for using 4bit qlora quantization techniques by tim dettmers)\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,  # load (usually huge) base model in 4 bits\n        bnb_4bit_quant_type=&quot;nf4&quot;,  # normal float 4 for the (usually huge) base model. introduces error but fixed by ft\n        # ref: https://gist.github.com/pacman100/1731b41f7a90a87b457e8c5415ff1c14\n        bnb_4bit_compute_dtype=bnb_4bit_compute_dtype,\n    )\n\n    # - get falcon 4bit model\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        quantization_config=bnb_config,\n        trust_remote_code=True  # allows to execute custom code you download from the uploaded model code you are using\n    )\n    model.config.use_cache = False  # todo: why? https://stackoverflow.com/questions/76633335/why-does-hugging-face-falcon-model-use-mode-config-use-cache-false-why-wouldn\n\n    # get falcon tockenizer\n    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)  # execs code downloaded from hf hub\n    tokenizer.pad_token = tokenizer.eos_token\n</code></pre>\n<hr />\n<h2>Modifying model gives issues</h2>\n<p>This still not works:</p>\n<pre><code> UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n</code></pre>\n<p>code:</p>\n<pre><code>&quot;&quot;&quot;\nsfttrainer (likely using peft) best practices:\nhttps://huggingface.co/docs/trl/main/en/sft_trainer#best-practices\n\nBest practices\n\nPay attention to the following best practices when training a model with that trainer:\n\n- SFTTrainer always pads by default the sequences to the max_seq_length argument of the SFTTrainer. If none is passed, the trainer will retrieve that value from the tokenizer. Some tokenizers do not provide default value, so there is a check to retrieve the minimum between 2048 and that value. Make sure to check it before training.\n- For training adapters in 8bit, you might need to tweak the arguments of the prepare_model_for_int8_training method from PEFT, hence we advise users to use prepare_in_int8_kwargs field, or create the PeftModel outside the SFTTrainer and pass it.\n- For a more memory-efficient training using adapters, you can load the base model in 8bit, for that simply add load_in_8bit argument when creating the SFTTrainer, or create a base model in 8bit outside the trainer and pass it.\n- If you create a model outside the trainer, make sure to not pass to the trainer any additional keyword arguments that are relative to from_pretrained() method.\n\ntodo: why trust_remote_code? I want more details.\n&quot;&quot;&quot;\nimport sys\n\nimport torch\nfrom peft import LoraConfig\n\nfrom transformers.modeling_utils import PreTrainedModel\n\nfrom pdb import set_trace as st\n\n\ndef test_bfloat16_int4(compute_dtype: torch.dtype,\n                       use_4bit,\n                       ):\n    &quot;&quot;&quot;\npython -c &quot;import torch; print(torch.cuda.get_device_capability());&quot;\n    todo: check other code test_bfloat16() do we need use_4bit?\n    &quot;&quot;&quot;\n    if compute_dtype == torch.float16 and use_4bit:\n        major, _ = torch.cuda.get_device_capability()\n        if major &gt;= 8:\n            print(&quot;=&quot; * 80)\n            print(&quot;Your GPU supports bfloat16, you can accelerate training with the argument --bfloat16&quot;)\n            print(&quot;=&quot; * 80)\n\n\ndef get_model_tokenizer_qlora_falcon7b(\n        # -- mode args\n        # model_id = &quot;tiiuae/falcon-7b&quot;\n        pretrained_model_name_or_path: str = &quot;ybelkada/falcon-7b-sharded-bf16&quot;,\n        use_cache: bool = True,\n        # -- lora args\n        lora_alpha=16,  # todo\n        lora_dropout=0.1,  # todo, evidence drop out really help? google, crfm, gpt4\n        lora_r=64,  # todo\n        bnb_4bit_compute_dtype=torch.float16,  # changed it from Guanaco hf\n\n        # -- training args\n        output_dir=&quot;./results&quot;,\n        per_device_train_batch_size=4,\n        gradient_accumulation_steps=4,\n        # paging so that the sudden mem gpu spikes don't cause the run to shut down\n        # (I think usually caused by too long seqs)\n        # todo: why 32 bit opt?\n        # todo: paged nadamw opt?\n        optim=&quot;paged_adamw_32bit&quot;,\n        save_steps=10,\n        logging_steps=10,\n        learning_rate=2e-4,\n        max_grad_norm=0.3,\n        max_steps=500,\n        warmup_ratio=0.03,\n        lr_scheduler_type=&quot;constant&quot;,\n        # -- quant. args (not recommended to be changed unless you know what your doing?)\n        load_in_4bit=True,  # load (usually huge) base model in 4 bits\n        bnb_4bit_quant_type=&quot;nf4&quot;,  # normal float 4 for the (large) base models qlora\n) -&gt; tuple:\n    &quot;&quot;&quot;\n    Load the Falcon 7B model, quantize it in 4bit and attach LoRA adapters on it.\n\n    bf16 = 1S, 7Exp, 8Mantissa\n    hypothesis: 7b trained due to 6.7 emergence rumour, I still don't think emergence is real.\n    Notes:\n        - ft a model is very specific to the model, tokenizer and training scheme. Thus we return\n            - model, tokenizer, ft config (peft config), training args\n\n    ref:\n        - https://colab.research.google.com/drive/1DOi8MFv4SWN9NImVornZ7t6BgmLoPQO-#scrollTo=AjB0WAqFSzlD\n    &quot;&quot;&quot;\n    from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoTokenizer\n\n    # - Get bnb config for bit-4 base model (bnb lib for using 4bit qlora quantization techniques by tim dettmers)\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=load_in_4bit,  # load (usually huge) base model in 4 bits\n        bnb_4bit_quant_type=bnb_4bit_quant_type,  # normal float 4 for the (usually huge) base model\n        bnb_4bit_compute_dtype=bnb_4bit_compute_dtype,  # if you can, during computation use bf16\n    )\n\n    # - Get falcon 4bit model\n    # todo, where is this being saved &amp; how to download quicker\n    model = AutoModelForCausalLM.from_pretrained(\n        pretrained_model_name_or_path=pretrained_model_name_or_path,\n        quantization_config=bnb_config,\n        trust_remote_code=True  # allows to execute custom code you download from the uploaded model code you are using\n    )\n    print(f'{type(model)=}')\n    print(f'{model=}')\n    # this is here to save gpu vram. Likely only needed when using 40b or when oom issues happen ref: https://stackoverflow.com/questions/76633335/why-does-hugging-face-falcon-model-use-mode-config-use-cache-false-why-wouldn\n    model.config.use_cache = use_cache\n    print(f'{type(model)=}')\n\n    # - Get falcon tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path,\n                                              trust_remote_code=True)  # execs code downloaded from hf hub\n    # tokenizer.pad_token = tokenizer.eos_token  # ref: https://stackoverflow.com/questions/76633368/why-does-the-falcon-qlora-tutorial-code-use-eos-token-as-pad-token\n    # tokenizer.add_special_tokens({'pad_token': '[PAD]'})  # I think this is fine if during the training pad is ignored\n    tokenizer.add_special_tokens({'pad_token': '&lt;|pad|&gt;'})  # I think this is fine if during the training pad is ignored\n\n    # - Modify model\n    # add pad token embed\n    model.resize_token_embeddings(len(tokenizer))  # todo: I think this is fine if during the training pad is ignored\n    model.transformer.word_embeddings.padding_idx = len(tokenizer) - 1\n    model.config.max_new_tokens = len(tokenizer)\n    # model.config.min_length = 1\n    print(f'{model=}')\n    print(f'{type(tokenizer)=}')\n    print(f'{tokenizer.pad_token=}')\n    # data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False) todo\n\n    # - Get falcon lora config\n    peft_config = LoraConfig(\n        lora_alpha=lora_alpha,\n        lora_dropout=lora_dropout,\n        r=lora_r,\n        bias=&quot;none&quot;,\n        task_type=&quot;CAUSAL_LM&quot;,\n        # model card for falcon tiiuae/falcon-7b: https://huggingface.co/tiiuae/falcon-7b/blob/main/modelling_RW.py\n        # does seem to include all trainable params as done by qlora on their own paper\n        target_modules=[\n            # word_embeddings,\n            &quot;query_key_value&quot;,\n            &quot;dense&quot;,\n            &quot;dense_h_to_4h&quot;,\n            &quot;dense_4h_to_h&quot;,\n            # &quot;lm_head&quot;\n        ]\n    )\n    print(f'{type(peft_config)=}')\n\n    # todo: print the num params of the lora = D1*r + D2*r and num of bytes by prec. (bytes) * num params\n    return model, tokenizer, peft_config\n\n\n# -- tests\n\ndef example_test_model_already_has_pad_token():\n    &quot;&quot;&quot;\n    if it already has pad token, it likely has a small prob, so we are done.\n\n    compare it's norm with other tokens to verify this is true.\n\npython ~/ultimate-utils/ultimate-utils-proj-src/uutils/hf_uu/model_tokenizer/falcon_uu_mdl_tok.py\n    &quot;&quot;&quot;\n    # - the get datasets todo: preprocessing, padding, streaming\n    from uutils.hf_uu.data_hf.common import get_guanaco_datsets_add_splits_train_test_only\n    trainset, _, testset = get_guanaco_datsets_add_splits_train_test_only()\n\n    # qlora flacon7b\n    from uutils.hf_uu.model_tokenizer.falcon_uu_mdl_tok import get_model_tokenizer_qlora_falcon7b\n    model, tokenizer, peft_config = get_model_tokenizer_qlora_falcon7b()\n    model: PreTrainedModel = model\n    print(f'{model=}')\n    sent = 'Dogs are great because they are '\n    print()\n\n    # print to see if pad tokens are present and if it ignores the tokens at the end\n    encoded_input = tokenizer(sent, padding='max_length', max_length=10, return_tensors='pt')\n    print(f'{encoded_input=}')\n\n    # Print all special tokens\n    print('\\n---- start Print all special tokens')\n    for token_name, token in tokenizer.special_tokens_map.items():\n        print(f&quot;{token_name}: {token}&quot;)\n    print('\\n---- end Print all special tokens')\n\n    # Get the ID for the '[PAD]' token\n    try:\n        pad_token_id = tokenizer.convert_tokens_to_ids('[PAD]')\n    except KeyError:\n        raise ValueError(&quot;Token [PAD] is not present in the tokenizer vocabulary.&quot;)\n\n    # Index into the model's embedding table\n    try:\n        print(f'{model.get_input_embeddings().weight.size()=}')\n        pad_embedding = model.get_input_embeddings().weight[pad_token_id]\n    except IndexError:\n        raise ValueError(f&quot;Token ID {pad_token_id} is not present in the model's embedding matrix.&quot;)\n\n    print(f'{pad_embedding=}')\n    print('Success!\\n')\n\n    # check it generates something sensible\n    # tokenizer.decode(model.generate(**tokenizer(sent, return_tensors='pt'), do_sample=True)[0])\n    input_ids, attention_mask = encoded_input['input_ids'], encoded_input['attention_mask']\n    predicted_tokens_ids_options = model.generate(input_ids=input_ids, attention_mask=attention_mask, do_sample=True)\n    predicted_tokens_ids = predicted_tokens_ids_options[0]\n    predicted_sent = tokenizer.decode(predicted_tokens_ids)\n    print(f'original sentence: {sent=}')\n    print(f'predicted sentence: {predicted_sent=}')\n    print('Success2!')\n\n\nif __name__ == '__main__':\n    import time\n\n    start_time = time.time()\n    example_test_model_already_has_pad_token()\n    print(f&quot;The main function executed in {time.time() - start_time} seconds.\\a&quot;)\n</code></pre>\n<p>it doesn't like the modifications to the model:</p>\n<pre><code>    model.transformer.word_embeddings.padding_idx = len(tokenizer) - 1\n    model.config.max_new_tokens = len(tokenizer)\n</code></pre>\n<p>How to fix?</p>\n<p>Errors:</p>\n<pre><code>/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/transformers/generation/utils.py:1259: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n  warnings.warn(\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/transformers/generation/utils.py:1452: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n  warnings.warn(\nTraceback (most recent call last):\n  File &quot;/lfs/hyperturing1/0/brando9/ultimate-utils/ultimate-utils-proj-src/uutils/hf_uu/model_tokenizer/falcon_uu_mdl_tok.py&quot;, line 211, in &lt;module&gt;\n    example_test_model_already_has_pad_token()\n  File &quot;/lfs/hyperturing1/0/brando9/ultimate-utils/ultimate-utils-proj-src/uutils/hf_uu/model_tokenizer/falcon_uu_mdl_tok.py&quot;, line 199, in example_test_model_already_has_pad_token\n    predicted_tokens_ids_options = model.generate(input_ids=input_ids, attention_mask=attention_mask, do_sample=True)\n  File &quot;/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/torch/utils/_contextlib.py&quot;, line 115, in decorate_context\n    return func(*args, **kwargs)\n  File &quot;/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/transformers/generation/utils.py&quot;, line 1572, in generate\n    return self.sample(\n  File &quot;/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/transformers/generation/utils.py&quot;, line 2633, in sample\n    next_token_scores = logits_warper(input_ids, next_token_scores)\n  File &quot;/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/transformers/generation/logits_process.py&quot;, line 92, in __call__\n    scores = processor(input_ids, scores)\n  File &quot;/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/transformers/generation/logits_process.py&quot;, line 302, in __call__\n    indices_to_remove = scores &lt; torch.topk(scores, top_k)[0][..., -1, None]\nRuntimeError: &quot;topk_cpu&quot; not implemented for 'Half'\n</code></pre>\n<hr />\n<h2>Bounty Section: Small GPT2 code example</h2>\n<p>Yes I agree that pad is assigned to eos. Eos is still eos. But during fine-tuning now the weights wrt to eos are unchanged. This might be an issue since the probability of eos has not shifted to the fine-tuning regime. One possibility is that eos is outputed with less chance. Yes we can still halt production when we see eos but we've not shifted the probability to output eos according to our fine-tuning distribution -- but all other tokens have changed distribution. I think this could be an issue because it's not like the old probability of eos is conserved since all tokens probs have changed except eos + even if the old eos prob was conserved, it's wrt wrong distribution (not the fine tuning one).</p>\n<p>e.g.,</p>\n<pre><code>if tokenizer.pad_token_id is None:\n    tokenizer.pad_token = tokenizer.eos_token\n ...\nraw_text_batch='a'\ntokenize_batch={'input_ids': tensor([[   64, 50256, 50256, 50256, 50256]]), 'attention_mask': tensor([[1, 0, 0, 0, 0]])}\n</code></pre>\n<p>but it would have been better to have</p>\n<pre><code>tokenize_batch={'input_ids': tensor([[   64, 50256, 50256, 50256, 50256]]), 'attention_mask': tensor([[1, 1, 0, 0, 0]])}\n</code></pre>\n<p>code</p>\n<pre><code>def test_eos_pad():\n    from datasets import load_dataset\n    import torch\n    from transformers import GPT2Tokenizer, GPT2LMHeadModel\n\n    raw_text_batch = 'a'\n\n    tokenizer = GPT2Tokenizer.from_pretrained(&quot;gpt2&quot;)\n    # print(f'{tokenizer.eos_token=}')\n    # print(f'{tokenizer.eos_token_id=}')\n    # print(f'{tokenizer.pad_token=}')\n    # print(f'{tokenizer.pad_token_id=}')\n\n    # print(f'{raw_text_batch=}')\n    # tokenize_batch = tokenizer(raw_text_batch, padding=&quot;max_length&quot;, max_length=5, truncation=True, return_tensors=&quot;pt&quot;)\n    # print(f'{tokenize_batch=}')\n\n    if tokenizer.pad_token_id is None:\n        tokenizer.pad_token = tokenizer.eos_token\n    probe_network = GPT2LMHeadModel.from_pretrained(&quot;gpt2&quot;)\n    device = torch.device(f&quot;cuda:{0}&quot; if torch.cuda.is_available() else &quot;cpu&quot;)\n    probe_network = probe_network.to(device)\n\n    print(f'{tokenizer.eos_token=}')\n    print(f'{tokenizer.eos_token_id=}')\n    print(f'{tokenizer.pad_token=}')\n    print(f'{tokenizer.pad_token_id=}')\n\n    print(f'{raw_text_batch=}')\n    tokenize_batch = tokenizer(raw_text_batch, padding=&quot;max_length&quot;, max_length=5, truncation=True, return_tensors=&quot;pt&quot;)\n    print(f'{tokenize_batch=}')\n    print('Done')\n</code></pre>\n<hr />\n<p>cross:</p>\n<ul>\n<li>hf discuss forum: <a href=\"https://discuss.huggingface.co/t/why-does-the-falcon-qlora-tutorial-code-use-eos-token-as-pad-token/45954\" rel=\"nofollow noreferrer\">https://discuss.huggingface.co/t/why-does-the-falcon-qlora-tutorial-code-use-eos-token-as-pad-token/45954</a></li>\n<li>pytorch forum discuss: <a href=\"https://discuss.pytorch.org/t/how-does-one-set-the-pad-token-correctly-not-to-eos-during-fine-tuning-to-avoid-model-not-predicting-eos/213619\" rel=\"nofollow noreferrer\">https://discuss.pytorch.org/t/how-does-one-set-the-pad-token-correctly-not-to-eos-during-fine-tuning-to-avoid-model-not-predicting-eos/213619</a></li>\n<li><a href=\"https://discord.com/channels/879548962464493619/1126681170957045770/1126681170957045770\" rel=\"nofollow noreferrer\">https://discord.com/channels/879548962464493619/1126681170957045770/1126681170957045770</a></li>\n<li>context peft pacman100 code: <a href=\"https://gist.github.com/pacman100/1731b41f7a90a87b457e8c5415ff1c14\" rel=\"nofollow noreferrer\">https://gist.github.com/pacman100/1731b41f7a90a87b457e8c5415ff1c14</a></li>\n<li>twitter tweet of this: <a href=\"https://twitter.com/BrandoHablando/status/1693676898013061337?s=20\" rel=\"nofollow noreferrer\">https://twitter.com/BrandoHablando/status/1693676898013061337?s=20</a></li>\n</ul>\n",
        "answer": "<p>I have not done the falcon model finetuning using QLoRA but I did it using PEFT and bitsandbytes for the 7b variant by loading the model in 8bit and using LoRA rank of 16 with a micro batch size of 8 on a 24Gb GPU. So I would like to mention this if it helps you:</p>\n<p>I did not find any <code>&lt;pad&gt;</code> token or <code>&lt;unk&gt;</code> token in falcon. (So e.g. the workaround in the alpaca-lora repository of using token id 0 for padding would not work as that is assigned to another token.)\nHowever, using <code>tokenizer.add_special_tokens({'pad_token': '&lt;PAD&gt;'})</code>\nafter loading the tokenizer and <code>model.resize_token_embeddings(len(tokenizer))</code> after loading the model in 8-bit worked (at least I did not get any errors during finetuning and the text generation with the finetuned model also worked).</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 76633368,
            "link": "https://stackoverflow.com/questions/76633368/how-does-one-set-the-pad-token-correctly-not-to-eos-during-fine-tuning-to-avoi"
        }
    },
    {
        "question": "Huggingface library not being able to replace separators in create_documents: &quot;AttributeError: &#39;dict&#39; object has no attribute &#39;replace&#39;&quot;\n<p>I'm a beginner in the chatbot developer world and currently building a rag code to create a context based chatbot, but I keep getting this error, I believe it happens when the text is being split, because even after the function is called, the text remains with the &quot;\\n&quot; separators.\nThe last line of the traceback occurs in the huggingface library.</p>\n<p>The traceback:</p>\n<pre><code>Traceback (most recent call last):\n  File &quot;c:\\Users\\sophi\\miniconda3\\envs\\ambiente3.9\\lib\\runpy.py&quot;, line 197, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File &quot;c:\\Users\\sophi\\miniconda3\\envs\\ambiente3.9\\lib\\runpy.py&quot;, line 87, in _run_code\n    exec(code, run_globals)\n  File &quot;c:\\Users\\sophi\\.vscode\\extensions\\ms-python.debugpy-2024.6.0-win32-x64\\bundled\\libs\\debugpy\\__main__.py&quot;, line 39, in &lt;module&gt;\n    cli.main()\n  File &quot;c:\\Users\\sophi\\.vscode\\extensions\\ms-python.debugpy-2024.6.0-win32-x64\\bundled\\libs\\debugpy/..\\debugpy\\server\\cli.py&quot;, line 430, in main\n    run()\n  File &quot;c:\\Users\\sophi\\.vscode\\extensions\\ms-python.debugpy-2024.6.0-win32-x64\\bundled\\libs\\debugpy/..\\debugpy\\server\\cli.py&quot;, line 284, in run_file\n    runpy.run_path(target, run_name=&quot;__main__&quot;)\n  File &quot;c:\\Users\\sophi\\.vscode\\extensions\\ms-python.debugpy-2024.6.0-win32-x64\\bundled\\libs\\debugpy\\_vendored\\pydevd\\_pydevd_bundle\\pydevd_runpy.py&quot;, line 321, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File &quot;c:\\Users\\sophi\\.vscode\\extensions\\ms-python.debugpy-2024.6.0-win32-x64\\bundled\\libs\\debugpy\\_vendored\\pydevd\\_pydevd_bundle\\pydevd_runpy.py&quot;, line 135, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File &quot;c:\\Users\\sophi\\.vscode\\extensions\\ms-python.debugpy-2024.6.0-win32-x64\\bundled\\libs\\debugpy\\_vendored\\pydevd\\_pydevd_bundle\\pydevd_runpy.py&quot;, line 124, in _run_code\n    exec(code, run_globals)\n  File &quot;c:\\Users\\sophi\\Documents\\ProjetosdePesquisa\\Projeto-de-Pesquisa-SOLIRIS\\llm_rag _ver4\\utils\\rag.py&quot;, line 130, in &lt;module&gt;\n    main()\n  File &quot;c:\\Users\\sophi\\Documents\\ProjetosdePesquisa\\Projeto-de-Pesquisa-SOLIRIS\\llm_rag _ver4\\utils\\rag.py&quot;, line 126, in main\n    response = qa.invoke({&quot;input&quot;: {&quot;context&quot;: context, &quot;question&quot;: question}})\n  File &quot;c:\\Users\\sophi\\miniconda3\\envs\\ambiente3.9\\lib\\site-packages\\langchain_core\\runnables\\base.py&quot;, line 4588, in invoke\n    return self.bound.invoke(\n  File &quot;c:\\Users\\sophi\\miniconda3\\envs\\ambiente3.9\\lib\\site-packages\\langchain_core\\runnables\\base.py&quot;, line 2505, in invoke\n    input = step.invoke(input, config, **kwargs)\n  File &quot;c:\\Users\\sophi\\miniconda3\\envs\\ambiente3.9\\lib\\site-packages\\langchain_core\\runnables\\passthrough.py&quot;, line 469, in invoke\n    return self._call_with_config(self._invoke, input, config, **kwargs)\n  File &quot;c:\\Users\\sophi\\miniconda3\\envs\\ambiente3.9\\lib\\site-packages\\langchain_core\\runnables\\base.py&quot;, line 1599, in _call_with_config       \n    context.run(\n  File &quot;c:\\Users\\sophi\\miniconda3\\envs\\ambiente3.9\\lib\\site-packages\\langchain_core\\runnables\\config.py&quot;, line 380, in call_func_with_variable_args\n    return func(input, **kwargs)  # type: ignore[call-arg]\n  File &quot;c:\\Users\\sophi\\miniconda3\\envs\\ambiente3.9\\lib\\site-packages\\langchain_core\\runnables\\passthrough.py&quot;, line 456, in _invoke\n    **self.mapper.invoke(\n  File &quot;c:\\Users\\sophi\\miniconda3\\envs\\ambiente3.9\\lib\\site-packages\\langchain_core\\runnables\\base.py&quot;, line 3152, in invoke\n    output = {key: future.result() for key, future in zip(steps, futures)}\n  File &quot;c:\\Users\\sophi\\miniconda3\\envs\\ambiente3.9\\lib\\site-packages\\langchain_core\\runnables\\base.py&quot;, line 3152, in &lt;dictcomp&gt;\n    output = {key: future.result() for key, future in zip(steps, futures)}\n  File &quot;c:\\Users\\sophi\\miniconda3\\envs\\ambiente3.9\\lib\\concurrent\\futures\\_base.py&quot;, line 446, in result\n    return self.__get_result()\n  File &quot;c:\\Users\\sophi\\miniconda3\\envs\\ambiente3.9\\lib\\concurrent\\futures\\_base.py&quot;, line 391, in __get_result\n    raise self._exception\n  File &quot;c:\\Users\\sophi\\miniconda3\\envs\\ambiente3.9\\lib\\concurrent\\futures\\thread.py&quot;, line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File &quot;c:\\Users\\sophi\\miniconda3\\envs\\ambiente3.9\\lib\\site-packages\\langchain_core\\runnables\\base.py&quot;, line 4588, in invoke\n    return self.bound.invoke(\n  File &quot;c:\\Users\\sophi\\miniconda3\\envs\\ambiente3.9\\lib\\site-packages\\langchain_core\\runnables\\base.py&quot;, line 2507, in invoke\n    input = step.invoke(input, config)\n  File &quot;c:\\Users\\sophi\\miniconda3\\envs\\ambiente3.9\\lib\\site-packages\\langchain_core\\retrievers.py&quot;, line 221, in invoke\n    raise e\n  File &quot;c:\\Users\\sophi\\miniconda3\\envs\\ambiente3.9\\lib\\site-packages\\langchain_core\\retrievers.py&quot;, line 214, in invoke\n    result = self._get_relevant_documents(\n  File &quot;c:\\Users\\sophi\\miniconda3\\envs\\ambiente3.9\\lib\\site-packages\\langchain_core\\vectorstores.py&quot;, line 797, in _get_relevant_documents    \n    docs = self.vectorstore.similarity_search(query, **self.search_kwargs)\n  File &quot;c:\\Users\\sophi\\miniconda3\\envs\\ambiente3.9\\lib\\site-packages\\langchain_community\\vectorstores\\chroma.py&quot;, line 349, in similarity_search\n    docs_and_scores = self.similarity_search_with_score(\n  File &quot;c:\\Users\\sophi\\miniconda3\\envs\\ambiente3.9\\lib\\site-packages\\langchain_community\\vectorstores\\chroma.py&quot;, line 438, in similarity_search_with_score\n    query_embedding = self._embedding_function.embed_query(query)\n  File &quot;c:\\Users\\sophi\\miniconda3\\envs\\ambiente3.9\\lib\\site-packages\\langchain_huggingface\\embeddings\\huggingface.py&quot;, line 102, in embed_query\n    return self.embed_documents([text])[0]\n  File &quot;c:\\Users\\sophi\\miniconda3\\envs\\ambiente3.9\\lib\\site-packages\\langchain_huggingface\\embeddings\\huggingface.py&quot;, line 81, in embed_documents\n    texts = list(map(lambda x: x.replace(&quot;\\n&quot;, &quot; &quot;), texts))\n  File &quot;c:\\Users\\sophi\\miniconda3\\envs\\ambiente3.9\\lib\\site-**packages\\langchain_huggingface\\embeddings\\huggingface.py&quot;, line 81, in &lt;lambda&gt;   \n    texts = list(map(lambda x: x.replace(&quot;\\n&quot;, &quot; &quot;), texts))\nAttributeError: 'dict' object has no attribute 'replace'**\n</code></pre>\n<h1>This is my entire code: (except for the groq api)</h1>\n<pre><code>\nimport sys\nimport os\nfrom langchain_core.prompts import PromptTemplate \nfrom langchain_groq import ChatGroq\nfrom langchain.chains import create_retrieval_chain\nfrom langchain_community.document_loaders import TextLoader\nfrom sentence_transformers import SentenceTransformer\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\nfrom langchain_community.vectorstores import Chroma\nfrom langchain.docstore.document import Document\nimport PyPDF2\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.llms import CTransformers\n\n# Caminho para o arquivo PDF\nPDF_PATH = 'pdf_handling/entrevistas.pdf'\n\n# Caminho para salvar os dados do ChromaDB\nCHROMA_DATA_PATH = &quot;chroma_data&quot;\n\n# Modelo de embeddings\nEMBED_MODEL = &quot;all-MiniLM-L6-v2&quot;\n\n# Nome da coleção\nCOLLECTION_NAME = &quot;ruth_docs&quot;\n\ndef dict_to_string(input_dict):\n    # Convert the dictionary into a string representation\n    # This uses a list comprehension to create a list of &quot;key: value&quot; strings\n    # and then joins them with a comma and a space.\n    return ', '.join([f&quot;{key}: {value}&quot; for key, value in input_dict.items()])\n\n# Função para extrair texto de um PDF e retornar uma lista de objetos Document\ndef extract_text_from_pdf(file_path):\n    try:\n        with open(file_path, 'rb') as pdf_file:\n            pdf = PyPDF2.PdfReader(pdf_file)\n            paginas = len(pdf.pages)\n            text = &quot;&quot;\n            for i in range(paginas):\n                page = pdf.pages[i]\n                text += page.extract_text()\n            # print(type(text))\n            text_splitter = RecursiveCharacterTextSplitter(\n                chunk_size=500,\n                chunk_overlap=50,\n                length_function=len,\n                separators=['\\n\\n\\n','\\n\\n','\\n', ' ', '']\n            )\n            documents = text_splitter.create_documents([text])\n            splitted_documents = text_splitter.split_documents(documents)\n            # print(documents)\n            # print(&quot;----------------------  vs  ---------------------&quot;)\n            # print(splitted_documents)\n            return splitted_documents\n        \n    except FileNotFoundError:\n        print(&quot;Arquivo não encontrado&quot;)\n        return []\n\nclass criar_vectordb:\n\n    def save_db(self, documents, embeddings, db_path):\n        self.db_path = db_path\n        self.embeddings = embeddings\n        self.documents = documents\n        input=self.documents\n        vectordb = Chroma.from_documents(input, self.embeddings, persist_directory=self.db_path)\n        vectordb = None\n        vectordb = Chroma(db_path, embeddings)\n\n        return vectordb\n    \nembeddings = HuggingFaceEmbeddings(model_name=&quot;sentence-transformers/all-MiniLM-L6-v2&quot;, model_kwargs={'device':'cpu'})\n\n# Extraindo texto do PDF e criando a base de dados vetorial\ndocuments = extract_text_from_pdf(PDF_PATH)\n\nvectordb = criar_vectordb().save_db(documents, embeddings, CHROMA_DATA_PATH)\n\nos.environ[&quot;GROQ_API_KEY&quot;] = &quot;-&quot;\n\nruth_prompt_template = &quot;&quot;&quot;\n                            Você é um assistente virtual de RH utilizando documentos para embasar sua resposta sempre em fatos,\n                            Use as informações presentes no documento para responder a resposta do candidato,\n                            sua resposta deve ser o mais semelhante possível com a descrição presente nos documentos\n                            \n                            contexto: {context}\n                            pergunta: {question}\n                            \n                            Apenas retorne as respostas úteis em ajudar na avaliação e seleção de candidatos e nada mais, usando uma linguagem gentil e empática.\n                            Sempre responda em português, uma descrição em texto contínua, além disso adicione\n                            um ou mais emojis às vezes para demonstrar empatia e emoção.\n                            \n                            \n                            &quot;&quot;&quot;\n\nprompt = PromptTemplate(template=ruth_prompt_template, input_variables=['context', 'question'])\n\n'''\nllm = CTransformers(\n        model = &quot;model/llama-2-7b-chat.ggmlv3.q8_0.bin&quot;,\n        model_type = &quot;llama&quot;,\n        config={'max_new_tokens': 512, \n                'temperature': 0.03,\n                'context_length': 1000,\n                'repetition_penalty': 1.15}\n        )\n'''\n\nllm = ChatGroq(model_name=&quot;llama3-70b-8192&quot;, api_key=os.environ[&quot;GROQ_API_KEY&quot;])\n\nretriever = vectordb.as_retriever(search_kwargs={&quot;k&quot;: 2})\ncombine_docs_chain = create_stuff_documents_chain(\n    llm, prompt\n)\n\nqa = create_retrieval_chain(retriever, combine_docs_chain)\n\n# Main\ndef main():\n    # Exemplo de uso\n    context = &quot;Feedback negativo&quot;\n    question = &quot;Como você lida com feedback negativo?&quot;\n    response = qa.invoke({&quot;input&quot;: {&quot;context&quot;: context, &quot;question&quot;: question}})\n    print(response)\n\nif __name__ == &quot;__main__&quot;:\n    main()\n</code></pre>\n<h1>This is the huggingface file:</h1>\n<pre><code>from typing import Any, Dict, List, Optional\n\nfrom langchain_core.embeddings import Embeddings\nfrom langchain_core.pydantic_v1 import BaseModel, Extra, Field\n\nDEFAULT_MODEL_NAME = &quot;sentence-transformers/all-mpnet-base-v2&quot;\n\n\nclass HuggingFaceEmbeddings(BaseModel, Embeddings):\n    &quot;&quot;&quot;HuggingFace sentence_transformers embedding models.\n\n    To use, you should have the ``sentence_transformers`` python package installed.\n\n    Example:\n        .. code-block:: python\n\n            from langchain_huggingface import HuggingFaceEmbeddings\n\n            model_name = &quot;sentence-transformers/all-mpnet-base-v2&quot;\n            model_kwargs = {'device': 'cpu'}\n            encode_kwargs = {'normalize_embeddings': False}\n            hf = HuggingFaceEmbeddings(\n                model_name=model_name,\n                model_kwargs=model_kwargs,\n                encode_kwargs=encode_kwargs\n            )\n    &quot;&quot;&quot;\n\n    client: Any  #: :meta private:\n    model_name: str = DEFAULT_MODEL_NAME\n    &quot;&quot;&quot;Model name to use.&quot;&quot;&quot;\n    cache_folder: Optional[str] = None\n    &quot;&quot;&quot;Path to store models. \n    Can be also set by SENTENCE_TRANSFORMERS_HOME environment variable.&quot;&quot;&quot;\n    model_kwargs: Dict[str, Any] = Field(default_factory=dict)\n    &quot;&quot;&quot;Keyword arguments to pass to the Sentence Transformer model, such as `device`,\n    `prompts`, `default_prompt_name`, `revision`, `trust_remote_code`, or `token`.\n    See also the Sentence Transformer documentation: https://sbert.net/docs/package_reference/SentenceTransformer.html#sentence_transformers.SentenceTransformer&quot;&quot;&quot;\n    encode_kwargs: Dict[str, Any] = Field(default_factory=dict)\n    &quot;&quot;&quot;Keyword arguments to pass when calling the `encode` method of the Sentence\n    Transformer model, such as `prompt_name`, `prompt`, `batch_size`, `precision`,\n    `normalize_embeddings`, and more.\n    See also the Sentence Transformer documentation: https://sbert.net/docs/package_reference/SentenceTransformer.html#sentence_transformers.SentenceTransformer.encode&quot;&quot;&quot;\n    multi_process: bool = False\n    &quot;&quot;&quot;Run encode() on multiple GPUs.&quot;&quot;&quot;\n    show_progress: bool = False\n    &quot;&quot;&quot;Whether to show a progress bar.&quot;&quot;&quot;\n\n    def __init__(self, **kwargs: Any):\n        &quot;&quot;&quot;Initialize the sentence_transformer.&quot;&quot;&quot;\n        super().__init__(**kwargs)\n        try:\n            import sentence_transformers  # type: ignore[import]\n\n        except ImportError as exc:\n            raise ImportError(\n                &quot;Could not import sentence_transformers python package. &quot;\n                &quot;Please install it with `pip install sentence-transformers`.&quot;\n            ) from exc\n\n        self.client = sentence_transformers.SentenceTransformer(\n            self.model_name, cache_folder=self.cache_folder, **self.model_kwargs\n        )\n\n    class Config:\n        &quot;&quot;&quot;Configuration for this pydantic object.&quot;&quot;&quot;\n\n        extra = Extra.forbid\n\n    def embed_documents(self, texts: List[str]) -&gt; List[List[float]]:\n        &quot;&quot;&quot;Compute doc embeddings using a HuggingFace transformer model.\n\n        Args:\n            texts: The list of texts to embed.\n\n        Returns:\n            List of embeddings, one for each text.\n        &quot;&quot;&quot;\n        import sentence_transformers  # type: ignore[import]\n\n        texts = list(map(lambda x: x.replace(&quot;\\n&quot;, &quot; &quot;), texts))\n        if self.multi_process:\n            pool = self.client.start_multi_process_pool()\n            embeddings = self.client.encode_multi_process(texts, pool)\n            sentence_transformers.SentenceTransformer.stop_multi_process_pool(pool)\n        else:\n            embeddings = self.client.encode(\n                texts, show_progress_bar=self.show_progress, **self.encode_kwargs\n            )\n\n        return embeddings.tolist()\n\n    def embed_query(self, text: str) -&gt; List[float]:\n        &quot;&quot;&quot;Compute query embeddings using a HuggingFace transformer model.\n\n        Args:\n            text: The text to embed.\n\n        Returns:\n            Embeddings for the text.\n        &quot;&quot;&quot;\n        return self.embed_documents([text])[0]\n</code></pre>\n<p>While debugging, I tried to use split_text() and split_documents() instead of create_documents() and it also didn't work, all of them give me the same output: this error, and my text still containing all of the &quot;\\n&quot;. I don't know if it could be something else in the code, as this is the only part that deals with separators.\nPlease help!\nThank you!</p>\n",
        "answer": "<p>I had similar error, turns out I was passing a <code>list</code> instead of a prompt <code>str</code>.<br />\nIn your case too I think the problem is that you are passing a <code>dict</code> here:</p>\n<pre><code>response = qa.invoke({&quot;input&quot;: {&quot;context&quot;: context, &quot;question&quot;: question}})\n</code></pre>\n<p>instead of string. Thus, you get the error:</p>\n<pre><code>texts = list(map(lambda x: x.replace(&quot;\\n&quot;, &quot; &quot;), texts))\nAttributeError: 'dict' object has no attribute 'replace'**\n</code></pre>\n<p>because it is expecting a <code>str</code> while you are providing <code>dict</code>.</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 78704628,
            "link": "https://stackoverflow.com/questions/78704628/huggingface-library-not-being-able-to-replace-separators-in-create-documents-a"
        }
    },
    {
        "question": "Why does HuggingFace-provided Deepseek code result in an &#39;Unknown quantization type&#39; error?\n<p>I am using this code from huggingface:</p>\n<p>This code is directly pasted from the <a href=\"https://huggingface.co/deepseek-ai/DeepSeek-R1\" rel=\"nofollow noreferrer\">HuggingFace website's page on deepseek</a> and is supposed to be plug-and-play code:</p>\n<blockquote>\n<pre class=\"lang-py prettyprint-override\"><code>from transformers import pipeline\n\nmessages = [\n{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Who are you?&quot;},\n]\npipe = pipeline(&quot;text-generation&quot;, model=&quot;deepseek-ai/DeepSeek-R1&quot;, &gt;trust_remote_code=True)\npipe(messages)\n</code></pre>\n</blockquote>\n<p>But I'm unable to load the model. When I do, I get this issue:</p>\n<pre><code>File &quot;&lt;...&gt;/site-packages/transformers/quantizers/auto.py&quot;, line 97, in from_dict\n\nraise ValueError(\n\nValueError: Unknown quantization type, got fp8 - supported types are: \n['awq', 'bitsandbytes_4bit', 'bitsandbytes_8bit', 'gptq', 'aqlm', 'quanto', 'eetq', \n'hqq', 'compressed-tensors', 'fbgemm_fp8', 'torchao', 'bitnet']\n</code></pre>\n<p>I tried different code:</p>\n<pre><code>import torch\ngenerate_text = pipeline(model=&quot;deepseek-ai/DeepSeek-R1&quot;,torch_dtype=torch.bfloat16, trust_remote_code=True, device_map=&quot;auto&quot;)\ngenerate_text(messages)\n</code></pre>\n<p>This gives the following error:</p>\n<blockquote>\n<p>raise ValueError( ValueError: Unknown quantization type, got fp8 - supported types are: ['awq', 'bitsandbytes_4bit', 'bitsandbytes_8bit', 'gptq', 'aqlm', 'quanto', 'eetq', 'higgs', 'hqq', 'compressed-tensors', 'fbgemm_fp8', 'torchao', 'bitnet', 'vptq']</p>\n</blockquote>\n<p>What can I do?</p>\n",
        "answer": "<p>Starting with <strong>transformers v4.51.0</strong> <a href=\"https://huggingface.co/deepseek-ai/DeepSeek-R1\" rel=\"nofollow noreferrer\">deepseek-ai/DeepSeek-R1</a> can be loaded directly with the library. To install the latest version of transformers run:</p>\n<pre class=\"lang-bash prettyprint-override\"><code>pip install --upgrade transformers\n</code></pre>\n<p>To check the loaded transformers version run:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import transformers\nprint(transformers.__version__)\n</code></pre>\n<p>After that the code in the original post will run as long as the required hardware is available:</p>\n<pre class=\"lang-py prettyprint-override\"><code>from transformers import pipeline\n\nmessages = [\n{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Who are you?&quot;},\n]\npipe = pipeline(&quot;text-generation&quot;, model=&quot;deepseek-ai/DeepSeek-R1&quot;, trust_remote_code=True)\npipe(messages)\n</code></pre>\n<p><strong>transformers &lt;4.51.0</strong>:</p>\n<p>The code you posted is auto generated and is not correct. The <a href=\"https://huggingface.co/deepseek-ai/DeepSeek-R1\" rel=\"nofollow noreferrer\">model card</a> states:</p>\n<blockquote>\n<p>NOTE: Hugging Face's Transformers has not been directly supported yet.</p>\n</blockquote>\n<p>The transformer library doesn't support the quantization method DeepSeek used for their model. Huggingface is working on a <a href=\"https://github.com/huggingface/transformers/pull/35926\" rel=\"nofollow noreferrer\">PR</a> to officially support it, but it will take some more time.</p>\n<p>You can still deploy the model on a GPUs that support fp8 via, for example, <a href=\"https://github.com/vllm-project/vllm\" rel=\"nofollow noreferrer\">vllm</a>:</p>\n<pre class=\"lang-bash prettyprint-override\"><code># Install vLLM from pip:\npip install vllm\n# Load and run the model:\nvllm serve &quot;deepseek-ai/DeepSeek-R1&quot;\n# Call the server using curl:\ncurl -X POST &quot;http://localhost:8000/v1/chat/completions&quot; \\\n    -H &quot;Content-Type: application/json&quot; \\\n    --data '{\n        &quot;model&quot;: &quot;deepseek-ai/DeepSeek-R1&quot;,\n        &quot;messages&quot;: [\n            {\n                &quot;role&quot;: &quot;user&quot;,\n                &quot;content&quot;: &quot;What is the capital of France?&quot;\n            }\n        ]\n    }'\n</code></pre>\n<p><a href=\"https://github.com/huggingface/transformers/issues/35471#issuecomment-2624801096\" rel=\"nofollow noreferrer\">Some people</a> have simply removed the quantization part from the config to load it with transformers. I haven't tested the performance impact this might have, so use it with caution:</p>\n<pre class=\"lang-py prettyprint-override\"><code>from transformers import AutoModelForCausalLM, AutoConfig\n\nconfig = AutoConfig.from_pretrained(&quot;deepseek-ai/DeepSeek-R1&quot;, trust_remote_code=True)\ndel config.quantization_config\n\nmodel = AutoModelForCausalLM.from_pretrained(&quot;deepseek-ai/DeepSeek-R1&quot;, config=config, trust_remote_code=True)\n</code></pre>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 79424312,
            "link": "https://stackoverflow.com/questions/79424312/why-does-huggingface-provided-deepseek-code-result-in-an-unknown-quantization-t"
        }
    },
    {
        "question": "Setting padding token as eos token when using DataCollatorForLanguageModeling from HuggingFace\n<p>In <a href=\"https://huggingface.co/learn/nlp-course/chapter7/6#preparing-the-dataset\" rel=\"nofollow noreferrer\">https://huggingface.co/learn/nlp-course/chapter7/6#preparing-the-dataset</a>, there is</p>\n<pre class=\"lang-py prettyprint-override\"><code>from transformers import DataCollatorForLanguageModeling\n\ntokenizer.pad_token = tokenizer.eos_token\ndata_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n</code></pre>\n<p>What the tutorial is doing is using a pretrained GPT2 model and its tokenizer and trying to create a dataset for causal language modeling pretraining task.</p>\n<p>My question with the above line is that padding token is set to be the eos token. As a result even the original eos tokens will be ignored by the model during training since they will be perceived as padding tokens too.</p>\n<p>This would prevent my model from learning to output eos tokens when its generation is over.</p>\n<p><strong>How come this is in the tutorials and it is a correct way ?</strong></p>\n",
        "answer": "<h1>TL;DR</h1>\n<p>Ignoring the EOS symbol when training a normal language model is okay. So padding the sequence with EOS instead of a dedicated PAD symbol is okay too.</p>\n<hr />\n<h1>In Long</h1>\n<p>When using <code>DataCollatorForLanguageModeling(tokenizer, mlm=False)</code>, the &quot;masked-language modeling&quot; model is off and we are doing casual language modeling ,i.e. predicting the next word given the previous. Consider this:</p>\n<pre><code>['this', 'is', 'a', 'foobar', '.', 'EOS']\n</code></pre>\n<p>Now we pad the sequence until it's of length 10 tokens</p>\n<pre><code>['this', 'is', 'a', 'foobar', '.', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS']\n</code></pre>\n<p>When the model learns with causal language model, it's predicting the next word given the previous, i.e.</p>\n<pre><code>&gt;&gt;&gt; predict(next_token, given=[&quot;BOS&quot;])\n'this'\n\n&gt;&gt;&gt; predict(next_token, given=[&quot;BOS&quot;, &quot;this&quot;])\n'is'\n\n...\n\n&gt;&gt;&gt; predict(next_token, given=[&quot;BOS&quot;, &quot;this&quot;, &quot;is&quot;, &quot;a&quot;, &quot;foobar&quot;, &quot;.&quot;])\n'EOS'\n</code></pre>\n<p>In most common inference routine, the model will stop once the first <code>EOS</code> is predicted, or all beams in the search during inference produced their first <code>EOS</code>.</p>\n<p>During training, the model will learn:</p>\n<pre><code>ground_truth = [\n 'this', 'is', 'a', 'foobar', '.', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', \n]\n\nground_prediction = [\n 'this', 'is', 'foobar', '.', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', \n]\n</code></pre>\n<p>And when you compute the perplexity, all the PAD symbols are ignored, and in this case, when you treat the EOS as PAD, you are essentially tell the model even the first <code>EOS</code> is not necessary when computing perplexity.</p>\n<h3>Q: Is that the right thing to do to ignore even the first EOS token, when we use EOS as a padding token?</h3>\n<p>A: It depends on your task and what you want the 'EOS' to mean. For most natural language, we have punctuations before 'EOS', so EOS/PAD doesn't really matter. For programming language, we have '\\n' and ';' or some end of sequence operator, so <code>EOS</code> isn't that necessary too.</p>\n<h3>Q: Then why do we bother to pad?</h3>\n<p>A: Actually that's a good question, we're padding so that the dot-products in transformer attentions can be &quot;easily&quot; computed.</p>\n<p>But there are many cases where pad tokens can be efficiently packed, like in RNN <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_packed_sequence.html\" rel=\"noreferrer\">https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_packed_sequence.html</a> (IIRC, not in transformers architecture though)</p>\n<p>But I don't know how much of that is already in Pytorch/JAX underlying library for &quot;efficient&quot; transformers, which will allow us to avoid pre-padding inputs. From my experience in using Huggingface Pytorch models, if you don't pad the inputs, most probably the model will complain when you do a forward pass =(</p>\n<p>If only, someone fix that mathematically. Maybe someone did try but it's not that common to be largely used by most transformers pre-trained model (yet).</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 76446228,
            "link": "https://stackoverflow.com/questions/76446228/setting-padding-token-as-eos-token-when-using-datacollatorforlanguagemodeling-fr"
        }
    },
    {
        "question": "Unable to import transformers.models.bert.modeling_tf_bert on macOS?\n<p>As the title is self-descriptive, I'm not able to import the <code>BertTokenizer</code> and <code>TFBertModel</code> classes from the <code>transformers</code> package through the following code:</p>\n<pre><code>from transformers import BertTokenizer, TFBertModel\n\ntokenizer = BertTokenizer.from_pretrained(BERT_PATH)\nmodel = TFBertModel.from_pretrained(BERT_PATH)\ntext = &quot;Replace me by any text you'd like.&quot;\nencoded_input = tokenizer(text, return_tensors='tf')\nresp = model(encoded_input)\nprint(resp)\n</code></pre>\n<p>As a result, I'm getting the following error:</p>\n<pre><code>RuntimeError: Failed to import transformers.models.bert.modeling_tf_bert because of the following error (look up to see its traceback):\ndlopen(/Users/tk/miniforge3/envs/QA-benchmark/lib/python3.10/site-packages/tensorflow-plugins/libmetal_plugin.dylib, 0x0006): symbol not found in flat namespace '_TF_GetInputPropertiesList'\n</code></pre>\n<p>Here is my software stack:</p>\n<pre><code>OS: macOS Ventura 13.3.1\nPython: 3.10\nTensorFlow: macOS-tensorflow 2.9.0\nTransformers: 4.28.0\nBERT model: uncased_L-12_H-768_A-12\n</code></pre>\n<p>p.s. I've already posted this issue on the GitHub repository of transformers.</p>\n",
        "answer": "<p>This issue has happened to me several times. This could be a potential dependency issues. Some library may want numpy(or any other library) &gt;=x and some library want numpy &gt;=y so you need to find a common library version which supported by both.</p>\n<p><strong>(look up to see its traceback):</strong></p>\n<p>Please check full trace for exact error. Post full stack trace for more accurate dependency causing error</p>\n<p>OS : Mac 15\nVersion: Python 3.10</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 76025069,
            "link": "https://stackoverflow.com/questions/76025069/unable-to-import-transformers-models-bert-modeling-tf-bert-on-macos"
        }
    },
    {
        "question": "Target modules for applying PEFT / LoRA on different models\n<p>I am looking at a few <a href=\"https://colab.research.google.com/drive/1BiQiw31DT7-cDp1-0ySXvvhzqomTdI-o#scrollTo=NuAx3zBeUL1q\" rel=\"noreferrer\">different</a> <a href=\"https://www.philschmid.de/fine-tune-flan-t5-peft\" rel=\"noreferrer\">examples</a> of using PEFT on different models. The <code>LoraConfig</code> object contains a <code>target_modules</code> array. In some examples, the target modules are <code>[&quot;query_key_value&quot;]</code>, sometimes it is <code>[&quot;q&quot;, &quot;v&quot;]</code>, sometimes something else.</p>\n<p>I don't quite understand where the values of the target modules come from. Where in the model page should I look to know what the LoRA adaptable modules are?</p>\n<p>One example (for the model Falcon 7B):</p>\n<pre><code>peft_config = LoraConfig(\n    lora_alpha=lora_alpha,\n    lora_dropout=lora_dropout,\n    r=lora_r,\n    bias=&quot;none&quot;,\n    task_type=&quot;CAUSAL_LM&quot;,\n    target_modules=[\n        &quot;query_key_value&quot;,\n        &quot;dense&quot;,\n        &quot;dense_h_to_4h&quot;,\n        &quot;dense_4h_to_h&quot;,\n    ]\n</code></pre>\n<p>Another example (for the model Opt-6.7B):</p>\n<pre><code>config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[&quot;q_proj&quot;, &quot;v_proj&quot;],\n    lora_dropout=0.05,\n    bias=&quot;none&quot;,\n    task_type=&quot;CAUSAL_LM&quot;\n)\n</code></pre>\n<p>Yet another (for the model Flan-T5-xxl):</p>\n<pre><code>lora_config = LoraConfig(\n r=16,\n lora_alpha=32,\n target_modules=[&quot;q&quot;, &quot;v&quot;],\n lora_dropout=0.05,\n bias=&quot;none&quot;,\n task_type=TaskType.SEQ_2_SEQ_LM\n)\n</code></pre>\n",
        "answer": "<p>I will add another answer as none of present ones feel complete/general for me.</p>\n<p>To solve the original question, getting a list of Lora compatible modules programmatically, I have tried using</p>\n<pre><code>target_modules = 'all-linear',\n</code></pre>\n<p>which seems available in latest PEFT versions.\nHowever, that would raise an error when applying to <code>google/gemma-2b</code> model.\n(dropout layers were for some reason added to the <code>target_modules</code>, see later for the layers supported by LORA).</p>\n<p>From documentation of the PEFT library:</p>\n<pre><code>only the following modules: `torch.nn.Linear`, `torch.nn.Embedding`, `torch.nn.Conv2d`, `transformers.pytorch_utils.Conv1D`.\n</code></pre>\n<p>I ended up creating this function for getting all Lora compatible modules from arbitrary models:</p>\n<pre><code>import torch\nfrom transformers import Conv1D\n\ndef get_specific_layer_names(model):\n    # Create a list to store the layer names\n    layer_names = []\n    \n    # Recursively visit all modules and submodules\n    for name, module in model.named_modules():\n        # Check if the module is an instance of the specified layers\n        if isinstance(module, (torch.nn.Linear, torch.nn.Embedding, torch.nn.Conv2d, Conv1D)):\n            # model name parsing \n\n            layer_names.append('.'.join(name.split('.')[4:]).split('.')[0])\n    \n    return layer_names\n\nlist(set(get_specific_layer_names(model)))\n</code></pre>\n<p>Which yields on gemma-2B</p>\n<pre><code>[\n 'down_proj',\n 'o_proj',\n 'k_proj',\n 'q_proj',\n 'gate_proj',\n 'up_proj',\n 'v_proj']\n</code></pre>\n<p>This list was valid for a target_modules selection</p>\n<pre><code>peft.__version__\n'0.10.1.dev0'\n\ntransformers.__version__\n'4.39.1'\n</code></pre>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 76768226,
            "link": "https://stackoverflow.com/questions/76768226/target-modules-for-applying-peft-lora-on-different-models"
        }
    },
    {
        "question": "ImportError: cannot import name &#39;HuggingFaceInferenceAPI&#39; from &#39;llama_index.llms&#39; (unknown location)\n<p>want to import HuggingFaceInferenceAPI.</p>\n<pre><code>from llama_index.llms import HugggingFaceInferenceAPI\n</code></pre>\n<p>llama_index.llms documentation doesn't have HugggingFaceInferenceAPI module. Anyone has update on this?</p>\n",
        "answer": "<pre><code>from llama_index.legacy.embeddings.langchain import LangchainEmbedding\n</code></pre>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 78251629,
            "link": "https://stackoverflow.com/questions/78251629/importerror-cannot-import-name-huggingfaceinferenceapi-from-llama-index-llms"
        }
    },
    {
        "question": "How to finetune an LLM model on your own codebase?\n<p>I have 10 code repositories in Javascript (VueJS) (Each repository corresponds to 1 Theme)</p>\n<p>I want to train an LLM model on these 10 code repositories to generate new themes using prompts.</p>\n<p>The LLM model takes the context of 10 code repositories as a reference (since the file structure is similar for all repositories)</p>\n<p>I'm a complete beginner with LLMs and ML.</p>\n<p>How to finetune an LLM model on my codebase?</p>\n",
        "answer": "<p>You can check this link, it was useful for me:</p>\n<p><a href=\"https://huggingface.co/blog/personal-copilot\" rel=\"nofollow noreferrer\">https://huggingface.co/blog/personal-copilot</a></p>\n<p>I guess you basically can create a <code>JSONL</code> file from your codes. Each line is a content of a file. And then fine-tune this file using libraries like <code>hf-autotrain</code>. And chose the textbook format.</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 76471292,
            "link": "https://stackoverflow.com/questions/76471292/how-to-finetune-an-llm-model-on-your-own-codebase"
        }
    },
    {
        "question": "How to load a huggingface dataset from local path?\n<p>Take a simple example in this website, <a href=\"https://huggingface.co/datasets/Dahoas/rm-static\" rel=\"nofollow noreferrer\">https://huggingface.co/datasets/Dahoas/rm-static</a>:</p>\n<p>if I want to load this dataset online, I just directly use,</p>\n<pre><code>from datasets import load_dataset\ndataset = load_dataset(&quot;Dahoas/rm-static&quot;) \n</code></pre>\n<p>What if I want to load dataset from local path, so I download the files and keep the same folder structure from web <code>Files and versions</code> fristly,</p>\n<pre><code>-data\n|-test-00000-of-00001-bf4c733542e35fcb.parquet\n|-train-00000-of-00001-2a1df75c6bce91ab.parquet\n-.gitattributes\n-README.md\n-dataset_infos.json\n</code></pre>\n<p>Then, put them into my folder, but shows error when loading:</p>\n<pre><code>dataset_path =&quot;/data/coco/dataset/Dahoas/rm-static&quot;\ntmp_dataset = load_dataset(dataset_path)\n</code></pre>\n<p>It shows <code>FileNotFoundError: No (supported) data files or dataset script found in /data/coco/dataset/Dahoas/rm-static.</code></p>\n",
        "answer": "<p>One can simply do this:</p>\n<pre><code>from datasets import load_dataset\n\nds = load_dataset(\n    &quot;parquet&quot;, data_dir=&quot;checkpoints/vqav2-small/data&quot;, trust_remote_code=True\n)\n</code></pre>\n<p>the data I using <code>huggingface-cli</code> downloaded to local, it's from:</p>\n<pre><code>huggingface-cli download --repo-type dataset merve/vqav2-small --local-dir vqav2-small\n</code></pre>\n<p>So, you can obviously observe the pattern how it is loaded from local.</p>\n<p>The data under <code>data</code> is all parquet files.</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 77020278,
            "link": "https://stackoverflow.com/questions/77020278/how-to-load-a-huggingface-dataset-from-local-path"
        }
    },
    {
        "question": "Error while loading Deepseek using HuggingFace\n<p>I am using HuggingFace as a pipeline to use text DeepSeek classifying model as follows:</p>\n<pre><code>model_kwargs = {&quot;trust_remote_code&quot;: True}\nembedding_model = HuggingFaceEmbeddings(\n    model_kwargs=model_kwargs,\n    model_name=&quot;deepseek-ai/DeepSeek-V3&quot;) \n</code></pre>\n<p>But when I run the code, I get the following error:</p>\n<blockquote>\n<p>ValueError: Unknown quantization type, got fp8 - supported types are: ['awq', 'bitsandbytes_4bit', 'bitsandbytes_8bit', 'gptq', 'aqlm', 'quanto', 'eetq', 'hqq', 'compressed-tensors', 'fbgemm_fp8', 'torchao']</p>\n</blockquote>\n<p>Any idea how to solve it?</p>\n",
        "answer": "<p><strong>Option 1: Use AutoModel with FP32 (no quantization)</strong></p>\n<p>Manually load the model using transformers:</p>\n<pre><code>from transformers import AutoTokenizer, AutoModel\nimport torch\n\nmodel_name = &quot;deepseek-ai/DeepSeek-V3&quot;\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\nmodel = AutoModel.from_pretrained(model_name, trust_remote_code=True)\n\ntext = &quot;Your input text here&quot;\ninputs = tokenizer(text, return_tensors=&quot;pt&quot;)\noutputs = model(**inputs)\n\n# Get the hidden states (embeddings)\nembeddings = outputs.last_hidden_state.mean(dim=1)\nprint(embeddings)\n</code></pre>\n<p><strong>Option 2: Use a proper embedding model instead</strong></p>\n<p>If your use-case is vector search, classification with embeddings, or retrieval-based QA, it’s better to use an embedding-specific model like:</p>\n<pre><code>•   sentence-transformers/all-MiniLM-L6-v2\n•   BAAI/bge-base-en\n•   intfloat/e5-large-v2\n</code></pre>\n<p>With LangChain:</p>\n<pre><code>from langchain.embeddings import HuggingFaceEmbeddings\n\nembedding_model = HuggingFaceEmbeddings(\n    model_name=&quot;sentence-transformers/all-MiniLM-L6-v2&quot;\n)\n</code></pre>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 79591642,
            "link": "https://stackoverflow.com/questions/79591642/error-while-loading-deepseek-using-huggingface"
        }
    },
    {
        "question": "How to use Hugging Face model with 512 max tokens on longer text (for Named Entity Recognition)\n<p>I have been using the Named Entity Recognition (NER) model <a href=\"https://huggingface.co/cahya/bert-base-indonesian-NER\" rel=\"nofollow noreferrer\">https://huggingface.co/cahya/bert-base-indonesian-NER</a> on Indonesian text as follows:</p>\n<pre class=\"lang-py prettyprint-override\"><code>text = &quot;...&quot;\nmodel_name = &quot;cahya/bert-base-indonesian-NER&quot;\ntokenizer = BertTokenizer.from_pretrained(model_name)\nmodel = BertForTokenClassification.from_pretrained(model_name)\nnlp = pipeline(&quot;ner&quot;, model=model, tokenizer=tokenizer, aggregation_strategy=&quot;simple&quot;)\nentities = nlp(text)\n</code></pre>\n<p>This works great, but when <code>text</code> contains more than 512 tokens I get the error:</p>\n<blockquote>\n<p>The size of tensor a (1098) must match the size of tensor b (512) at non-singleton dimension 1</p>\n</blockquote>\n<p>What is the best way to check if <code>text</code> contains more than 512 tokens, and then split it into manageable chunks that I can use for NER?</p>\n<hr />\n<p>Counting the number of tokens seems straightforward:</p>\n<pre><code>n_tokens = len(tokenizer.encode(text, add_special_tokens=True, truncation=False))\n</code></pre>\n<p>However, it is unclear what is the best way to split this. Surely there should be a suite of functions for this?</p>\n",
        "answer": "<p>If your text is too long for cahya/bert-base-indonesian-NER, just split it into overlapping chunks before running NER.</p>\n<pre><code>from transformers import BertTokenizer, BertForTokenClassification, pipeline\n\n# Initialize tokenizer and model\nmodel_name = &quot;cahya/bert-base-indonesian-NER&quot;\ntokenizer = BertTokenizer.from_pretrained(model_name)\nmodel = BertForTokenClassification.from_pretrained(model_name)\nnlp = pipeline(&quot;ner&quot;, model=model, tokenizer=tokenizer, aggregation_strategy=&quot;simple&quot;)\n\n# Sample text\ntext = &quot;Pemerintah Indonesia sedang berupaya meningkatkan infrastruktur digital di seluruh negeri. Dalam beberapa tahun terakhir, investasi dalam jaringan internet dan teknologi telah meningkat secara signifikan. Banyak perusahaan rintisan (startup) bermunculan, terutama di sektor e-commerce dan fintech. Selain itu, pendidikan digital juga menjadi fokus utama untuk memastikan masyarakat memiliki keterampilan yang dibutuhkan di era teknologi ini.&quot;\n\n# Tokenize text\ntokens = tokenizer.tokenize(text)\n\n# Split tokens into overlapping chunks\nmax_length = 512 - 2  # ignore special tokens\noverlap = 50\nchunks = []\nfor i in range(0, len(tokens), max_length - overlap):\n    chunks.append(tokens[i:i + max_length])\n\n# Detokenize chunks back to text\nchunk_texts = [tokenizer.convert_tokens_to_string(chunk) for chunk in chunks]\n\n# Perform NER on each chunk\nall_entities = []\nfor chunk in chunk_texts:\n    entities = nlp(chunk) \n    all_entities.extend(entities)\n</code></pre>\n<p>I found that this model's performance is not very good. You can use this process as a reference and make adjustments.</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 79207778,
            "link": "https://stackoverflow.com/questions/79207778/how-to-use-hugging-face-model-with-512-max-tokens-on-longer-text-for-named-enti"
        }
    },
    {
        "question": "ModuleNotFoundError: No module named &#39;huggingface_hub.utils&#39; using Anaconda\n<p>I'm trying to execute the example code of the huggingface website:</p>\n<pre class=\"lang-py prettyprint-override\"><code>from transformers import GPTJTokenizer, TFGPTJModel\nimport tensorflow as tf\n\ntokenizer = GPTJTokenizer.from_pretrained(&quot;EleutherAI/gpt-j-6B&quot;)\nmodel = TFGPTJModel.from_pretrained(&quot;EleutherAI/gpt-j-6B&quot;)\n\ninputs = tokenizer(&quot;Hello, my dog is cute&quot;, return_tensors=&quot;tf&quot;)\noutputs = model(inputs)\n\nlast_hidden_states = outputs.last_hidden_state\n</code></pre>\n<p>I'm using anaconda and I installed the transformers package beforehand with <code>conda install -c huggingface transformers</code> as explained in the <a href=\"https://huggingface.co/docs/transformers/installation#install-with-conda\" rel=\"noreferrer\">documentation</a>. But I still get this error, when I'm trying to execute the code. Following error message pops up: <code>ModuleNotFoundError: No module named 'huggingface_hub.utils'</code></p>\n<p>How to resolve this error?</p>\n",
        "answer": "<p>This still happened to me today, so here is what I did.</p>\n<p>I looked up the path to my conda env with</p>\n<pre><code>conda info\n\n# output\n    active env location : /home/jovyan/my-conda-envs/myenv\n# more output\n</code></pre>\n<p>I located the <code>huggingface_hub</code> package at <code>/home/jovyan/my-conda-envs/myenv/lib/python3.10/site-packages/huggingface_hub</code> and saw that it contained a file named <code>LICENSE</code> only.</p>\n<p>So I force-reinstalled <code>huggingface_hub</code> with:</p>\n<pre><code>conda install --force-reinstall --update-deps huggingface_hub\n</code></pre>\n<p>The module now contains many more files, besides <code>LICENSE</code>, the error is gone.</p>\n<p>This solved the problem for me.</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 73960201,
            "link": "https://stackoverflow.com/questions/73960201/modulenotfounderror-no-module-named-huggingface-hub-utils-using-anaconda"
        }
    },
    {
        "question": "NameError: name &#39;init_empty_weights&#39; is not defined while using hugging face models\n<p>I am trying to set up hugging face locally and im running into this issue.</p>\n<pre><code>NameError: name 'init_empty_weights' is not defined\n</code></pre>\n<p>Here is the code I have tested my installation with</p>\n<pre><code>from transformers import pipeline\nclassifier = pipeline(&quot;sentiment-analysis&quot;)\ntext = &quot;I love using Hugging Face Transformers!&quot;\nresult = classifier(text)\nprint(result)\n\n\n</code></pre>\n<p>transformers: 4.51.0 <br>\ntokenizers: 0.21.1 <br>\naccelerate: 1.6.0 <br>\nsentence-transformers: 4.0.2 <br>\nhuggingface_hub: 0.30.1 <br>\nI am currently using  pytorch-metal mac M3 pro.<br></p>\n<p>What causes this, and how can I fix it?</p>\n",
        "answer": "<p>I have the same issue. The issue is related to <a href=\"https://github.com/huggingface/transformers/pull/37337\" rel=\"noreferrer\">https://github.com/huggingface/transformers/pull/37337</a></p>\n<p>In my case, installing <code>accelerate</code> fix the issue, as the workaround.</p>\n<pre><code>pip install accelerate\n</code></pre>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 79559702,
            "link": "https://stackoverflow.com/questions/79559702/nameerror-name-init-empty-weights-is-not-defined-while-using-hugging-face-mod"
        }
    },
    {
        "question": "SSLError: HTTPSConnectionPool(host=&#39;huggingface.co&#39;, port=443): Max retries exceeded with url: /dslim/bert-base-NER/resolve/main/tokenizer_config.json\n<p>I am facing below issue while loading the pretrained BERT model from HuggingFace due to SSL certificate error.</p>\n<h2>Error:</h2>\n<blockquote>\n<p>SSLError: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /dslim/bert-base-NER/resolve/main/tokenizer_config.json (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self signed certificate in certificate chain (_ssl.c:1108)')))</p>\n</blockquote>\n<h2>The line that is causing the issue is:</h2>\n<pre><code>tokenizer = AutoTokenizer.from_pretrained(&quot;dslim/bert-base-NER&quot;)\n</code></pre>\n<h2>Source code:</h2>\n<pre><code>from transformers import AutoTokenizer, AutoModelForTokenClassification\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(&quot;dslim/bert-base-NER&quot;)\nmodel = AutoModelForTokenClassification.from_pretrained(&quot;dslim/bert-base-NER&quot;)\n</code></pre>\n<p>I am expecting to download pre-trained models while running the code in jupyter lab on Windows.</p>\n",
        "answer": "<p>I was working with my linux machine ,Looking at server of hugging face found that they were working fine ,it was just the issue of proxy directly export proxy in .zshrc work for me.</p>\n<pre><code>export https_proxy=http://127.0.0.1:7890\nexport http_proxy=http://127.0.0.1:7890\nexport all_proxy=socks5://127.0.0.1:7890\n</code></pre>\n<p>using source ~/.zshrc\nI tried downgrading request library but didn't worked ,by managing the proxy it did work for me .</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 75110981,
            "link": "https://stackoverflow.com/questions/75110981/sslerror-httpsconnectionpoolhost-huggingface-co-port-443-max-retries-exce"
        }
    },
    {
        "question": "How does one set the pad token correctly (not to eos) during fine-tuning to avoid model not predicting EOS?\n<h2>tldr; what I really want to know is what is the official way to set pad token for <strong>fine tuning</strong> it wasn't set during original training, so that it doesn't not learn to predict EOS.</h2>\n<p>colab: <a href=\"https://colab.research.google.com/drive/1poFdFYmkR_rDM5U5Z2WWjTepMQ8hvzNc?usp=sharing\" rel=\"nofollow noreferrer\">https://colab.research.google.com/drive/1poFdFYmkR_rDM5U5Z2WWjTepMQ8hvzNc?usp=sharing</a></p>\n<hr />\n<p>The HF falcon tutorial has the following line:</p>\n<pre><code>tokenizer.pad_token = tokenizer.eos_token\n</code></pre>\n<p>it looks strange to me. It make sense pad and eos are the same but then why even make a difference between them in the first place in general?</p>\n<p>Note its wrong to do pad = eos. This means during fine-tuning the model will never be trained to output eos (most likely) since eos is treated as pad token and no back propagated:</p>\n<pre><code>I just observed that when I set tokenizer.pad_token = tokenizer.eos_token during training, the model won't stop generating during inference, since it was trained to not output the eos token (per discussions above).\n</code></pre>\n<p>I saw this (here <a href=\"https://github.com/huggingface/transformers/issues/22794\" rel=\"nofollow noreferrer\">https://github.com/huggingface/transformers/issues/22794</a>):</p>\n<pre><code>tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n</code></pre>\n<p>But this assumes the model has a pad_token. I think an additional check has to be done that it does have an embedding for pad_token so that there are no run time errors (~type errors in the matrix extraction from the embedding &quot;table&quot;/matrix).</p>\n<p>But if one does that some care might be needed to initialize the new token so that it dominates the generation: <a href=\"https://nlp.stanford.edu/%7Ejohnhew/vocab-expansion.html\" rel=\"nofollow noreferrer\">https://nlp.stanford.edu/~johnhew/vocab-expansion.html</a></p>\n<hr />\n<p>code:</p>\n<pre class=\"lang-py prettyprint-override\"><code>def get_model_tokenizer_qlora_falcon7b(model_name: str = &quot;ybelkada/falcon-7b-sharded-bf16&quot;,\n                                       config: wand.Config,  # todo\n                                       lora_alpha=16,  # todo\n                                       lora_dropout=0.1,  # todo\n                                       lora_r=64,  # todo\n                                       bnb_4bit_compute_dtype=torch.float16,  # changed it from Guanaco hf\n                                       ) -&gt; tuple:\n    &quot;&quot;&quot;\n    Load the Falcon 7B model, quantize it in 4bit and attach LoRA adapters on it.\n\n    bf16 = 1S, 7Exp, 8Mantissa\n\n    Do:\n        pip install bitsandbytes\n    ref:\n        - https://colab.research.google.com/drive/1DOi8MFv4SWN9NImVornZ7t6BgmLoPQO-#scrollTo=AjB0WAqFSzlD\n    &quot;&quot;&quot;\n    from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoTokenizer\n\n    # model_id = &quot;tiiuae/falcon-7b&quot;\n    # model_name: str = &quot;ybelkada/falcon-7b-sharded-bf16&quot;\n\n    # - get bnb config for bit-4 base model (bnb lib for using 4bit qlora quantization techniques by tim dettmers)\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,  # load (usually huge) base model in 4 bits\n        bnb_4bit_quant_type=&quot;nf4&quot;,  # normal float 4 for the (usually huge) base model. introduces error but fixed by ft\n        # ref: https://gist.github.com/pacman100/1731b41f7a90a87b457e8c5415ff1c14\n        bnb_4bit_compute_dtype=bnb_4bit_compute_dtype,\n    )\n\n    # - get falcon 4bit model\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        quantization_config=bnb_config,\n        trust_remote_code=True  # allows to execute custom code you download from the uploaded model code you are using\n    )\n    model.config.use_cache = False  # todo: why? https://stackoverflow.com/questions/76633335/why-does-hugging-face-falcon-model-use-mode-config-use-cache-false-why-wouldn\n\n    # get falcon tockenizer\n    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)  # execs code downloaded from hf hub\n    tokenizer.pad_token = tokenizer.eos_token\n</code></pre>\n<hr />\n<h2>Modifying model gives issues</h2>\n<p>This still not works:</p>\n<pre><code> UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n</code></pre>\n<p>code:</p>\n<pre><code>&quot;&quot;&quot;\nsfttrainer (likely using peft) best practices:\nhttps://huggingface.co/docs/trl/main/en/sft_trainer#best-practices\n\nBest practices\n\nPay attention to the following best practices when training a model with that trainer:\n\n- SFTTrainer always pads by default the sequences to the max_seq_length argument of the SFTTrainer. If none is passed, the trainer will retrieve that value from the tokenizer. Some tokenizers do not provide default value, so there is a check to retrieve the minimum between 2048 and that value. Make sure to check it before training.\n- For training adapters in 8bit, you might need to tweak the arguments of the prepare_model_for_int8_training method from PEFT, hence we advise users to use prepare_in_int8_kwargs field, or create the PeftModel outside the SFTTrainer and pass it.\n- For a more memory-efficient training using adapters, you can load the base model in 8bit, for that simply add load_in_8bit argument when creating the SFTTrainer, or create a base model in 8bit outside the trainer and pass it.\n- If you create a model outside the trainer, make sure to not pass to the trainer any additional keyword arguments that are relative to from_pretrained() method.\n\ntodo: why trust_remote_code? I want more details.\n&quot;&quot;&quot;\nimport sys\n\nimport torch\nfrom peft import LoraConfig\n\nfrom transformers.modeling_utils import PreTrainedModel\n\nfrom pdb import set_trace as st\n\n\ndef test_bfloat16_int4(compute_dtype: torch.dtype,\n                       use_4bit,\n                       ):\n    &quot;&quot;&quot;\npython -c &quot;import torch; print(torch.cuda.get_device_capability());&quot;\n    todo: check other code test_bfloat16() do we need use_4bit?\n    &quot;&quot;&quot;\n    if compute_dtype == torch.float16 and use_4bit:\n        major, _ = torch.cuda.get_device_capability()\n        if major &gt;= 8:\n            print(&quot;=&quot; * 80)\n            print(&quot;Your GPU supports bfloat16, you can accelerate training with the argument --bfloat16&quot;)\n            print(&quot;=&quot; * 80)\n\n\ndef get_model_tokenizer_qlora_falcon7b(\n        # -- mode args\n        # model_id = &quot;tiiuae/falcon-7b&quot;\n        pretrained_model_name_or_path: str = &quot;ybelkada/falcon-7b-sharded-bf16&quot;,\n        use_cache: bool = True,\n        # -- lora args\n        lora_alpha=16,  # todo\n        lora_dropout=0.1,  # todo, evidence drop out really help? google, crfm, gpt4\n        lora_r=64,  # todo\n        bnb_4bit_compute_dtype=torch.float16,  # changed it from Guanaco hf\n\n        # -- training args\n        output_dir=&quot;./results&quot;,\n        per_device_train_batch_size=4,\n        gradient_accumulation_steps=4,\n        # paging so that the sudden mem gpu spikes don't cause the run to shut down\n        # (I think usually caused by too long seqs)\n        # todo: why 32 bit opt?\n        # todo: paged nadamw opt?\n        optim=&quot;paged_adamw_32bit&quot;,\n        save_steps=10,\n        logging_steps=10,\n        learning_rate=2e-4,\n        max_grad_norm=0.3,\n        max_steps=500,\n        warmup_ratio=0.03,\n        lr_scheduler_type=&quot;constant&quot;,\n        # -- quant. args (not recommended to be changed unless you know what your doing?)\n        load_in_4bit=True,  # load (usually huge) base model in 4 bits\n        bnb_4bit_quant_type=&quot;nf4&quot;,  # normal float 4 for the (large) base models qlora\n) -&gt; tuple:\n    &quot;&quot;&quot;\n    Load the Falcon 7B model, quantize it in 4bit and attach LoRA adapters on it.\n\n    bf16 = 1S, 7Exp, 8Mantissa\n    hypothesis: 7b trained due to 6.7 emergence rumour, I still don't think emergence is real.\n    Notes:\n        - ft a model is very specific to the model, tokenizer and training scheme. Thus we return\n            - model, tokenizer, ft config (peft config), training args\n\n    ref:\n        - https://colab.research.google.com/drive/1DOi8MFv4SWN9NImVornZ7t6BgmLoPQO-#scrollTo=AjB0WAqFSzlD\n    &quot;&quot;&quot;\n    from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoTokenizer\n\n    # - Get bnb config for bit-4 base model (bnb lib for using 4bit qlora quantization techniques by tim dettmers)\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=load_in_4bit,  # load (usually huge) base model in 4 bits\n        bnb_4bit_quant_type=bnb_4bit_quant_type,  # normal float 4 for the (usually huge) base model\n        bnb_4bit_compute_dtype=bnb_4bit_compute_dtype,  # if you can, during computation use bf16\n    )\n\n    # - Get falcon 4bit model\n    # todo, where is this being saved &amp; how to download quicker\n    model = AutoModelForCausalLM.from_pretrained(\n        pretrained_model_name_or_path=pretrained_model_name_or_path,\n        quantization_config=bnb_config,\n        trust_remote_code=True  # allows to execute custom code you download from the uploaded model code you are using\n    )\n    print(f'{type(model)=}')\n    print(f'{model=}')\n    # this is here to save gpu vram. Likely only needed when using 40b or when oom issues happen ref: https://stackoverflow.com/questions/76633335/why-does-hugging-face-falcon-model-use-mode-config-use-cache-false-why-wouldn\n    model.config.use_cache = use_cache\n    print(f'{type(model)=}')\n\n    # - Get falcon tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path,\n                                              trust_remote_code=True)  # execs code downloaded from hf hub\n    # tokenizer.pad_token = tokenizer.eos_token  # ref: https://stackoverflow.com/questions/76633368/why-does-the-falcon-qlora-tutorial-code-use-eos-token-as-pad-token\n    # tokenizer.add_special_tokens({'pad_token': '[PAD]'})  # I think this is fine if during the training pad is ignored\n    tokenizer.add_special_tokens({'pad_token': '&lt;|pad|&gt;'})  # I think this is fine if during the training pad is ignored\n\n    # - Modify model\n    # add pad token embed\n    model.resize_token_embeddings(len(tokenizer))  # todo: I think this is fine if during the training pad is ignored\n    model.transformer.word_embeddings.padding_idx = len(tokenizer) - 1\n    model.config.max_new_tokens = len(tokenizer)\n    # model.config.min_length = 1\n    print(f'{model=}')\n    print(f'{type(tokenizer)=}')\n    print(f'{tokenizer.pad_token=}')\n    # data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False) todo\n\n    # - Get falcon lora config\n    peft_config = LoraConfig(\n        lora_alpha=lora_alpha,\n        lora_dropout=lora_dropout,\n        r=lora_r,\n        bias=&quot;none&quot;,\n        task_type=&quot;CAUSAL_LM&quot;,\n        # model card for falcon tiiuae/falcon-7b: https://huggingface.co/tiiuae/falcon-7b/blob/main/modelling_RW.py\n        # does seem to include all trainable params as done by qlora on their own paper\n        target_modules=[\n            # word_embeddings,\n            &quot;query_key_value&quot;,\n            &quot;dense&quot;,\n            &quot;dense_h_to_4h&quot;,\n            &quot;dense_4h_to_h&quot;,\n            # &quot;lm_head&quot;\n        ]\n    )\n    print(f'{type(peft_config)=}')\n\n    # todo: print the num params of the lora = D1*r + D2*r and num of bytes by prec. (bytes) * num params\n    return model, tokenizer, peft_config\n\n\n# -- tests\n\ndef example_test_model_already_has_pad_token():\n    &quot;&quot;&quot;\n    if it already has pad token, it likely has a small prob, so we are done.\n\n    compare it's norm with other tokens to verify this is true.\n\npython ~/ultimate-utils/ultimate-utils-proj-src/uutils/hf_uu/model_tokenizer/falcon_uu_mdl_tok.py\n    &quot;&quot;&quot;\n    # - the get datasets todo: preprocessing, padding, streaming\n    from uutils.hf_uu.data_hf.common import get_guanaco_datsets_add_splits_train_test_only\n    trainset, _, testset = get_guanaco_datsets_add_splits_train_test_only()\n\n    # qlora flacon7b\n    from uutils.hf_uu.model_tokenizer.falcon_uu_mdl_tok import get_model_tokenizer_qlora_falcon7b\n    model, tokenizer, peft_config = get_model_tokenizer_qlora_falcon7b()\n    model: PreTrainedModel = model\n    print(f'{model=}')\n    sent = 'Dogs are great because they are '\n    print()\n\n    # print to see if pad tokens are present and if it ignores the tokens at the end\n    encoded_input = tokenizer(sent, padding='max_length', max_length=10, return_tensors='pt')\n    print(f'{encoded_input=}')\n\n    # Print all special tokens\n    print('\\n---- start Print all special tokens')\n    for token_name, token in tokenizer.special_tokens_map.items():\n        print(f&quot;{token_name}: {token}&quot;)\n    print('\\n---- end Print all special tokens')\n\n    # Get the ID for the '[PAD]' token\n    try:\n        pad_token_id = tokenizer.convert_tokens_to_ids('[PAD]')\n    except KeyError:\n        raise ValueError(&quot;Token [PAD] is not present in the tokenizer vocabulary.&quot;)\n\n    # Index into the model's embedding table\n    try:\n        print(f'{model.get_input_embeddings().weight.size()=}')\n        pad_embedding = model.get_input_embeddings().weight[pad_token_id]\n    except IndexError:\n        raise ValueError(f&quot;Token ID {pad_token_id} is not present in the model's embedding matrix.&quot;)\n\n    print(f'{pad_embedding=}')\n    print('Success!\\n')\n\n    # check it generates something sensible\n    # tokenizer.decode(model.generate(**tokenizer(sent, return_tensors='pt'), do_sample=True)[0])\n    input_ids, attention_mask = encoded_input['input_ids'], encoded_input['attention_mask']\n    predicted_tokens_ids_options = model.generate(input_ids=input_ids, attention_mask=attention_mask, do_sample=True)\n    predicted_tokens_ids = predicted_tokens_ids_options[0]\n    predicted_sent = tokenizer.decode(predicted_tokens_ids)\n    print(f'original sentence: {sent=}')\n    print(f'predicted sentence: {predicted_sent=}')\n    print('Success2!')\n\n\nif __name__ == '__main__':\n    import time\n\n    start_time = time.time()\n    example_test_model_already_has_pad_token()\n    print(f&quot;The main function executed in {time.time() - start_time} seconds.\\a&quot;)\n</code></pre>\n<p>it doesn't like the modifications to the model:</p>\n<pre><code>    model.transformer.word_embeddings.padding_idx = len(tokenizer) - 1\n    model.config.max_new_tokens = len(tokenizer)\n</code></pre>\n<p>How to fix?</p>\n<p>Errors:</p>\n<pre><code>/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/transformers/generation/utils.py:1259: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n  warnings.warn(\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/transformers/generation/utils.py:1452: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n  warnings.warn(\nTraceback (most recent call last):\n  File &quot;/lfs/hyperturing1/0/brando9/ultimate-utils/ultimate-utils-proj-src/uutils/hf_uu/model_tokenizer/falcon_uu_mdl_tok.py&quot;, line 211, in &lt;module&gt;\n    example_test_model_already_has_pad_token()\n  File &quot;/lfs/hyperturing1/0/brando9/ultimate-utils/ultimate-utils-proj-src/uutils/hf_uu/model_tokenizer/falcon_uu_mdl_tok.py&quot;, line 199, in example_test_model_already_has_pad_token\n    predicted_tokens_ids_options = model.generate(input_ids=input_ids, attention_mask=attention_mask, do_sample=True)\n  File &quot;/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/torch/utils/_contextlib.py&quot;, line 115, in decorate_context\n    return func(*args, **kwargs)\n  File &quot;/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/transformers/generation/utils.py&quot;, line 1572, in generate\n    return self.sample(\n  File &quot;/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/transformers/generation/utils.py&quot;, line 2633, in sample\n    next_token_scores = logits_warper(input_ids, next_token_scores)\n  File &quot;/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/transformers/generation/logits_process.py&quot;, line 92, in __call__\n    scores = processor(input_ids, scores)\n  File &quot;/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/transformers/generation/logits_process.py&quot;, line 302, in __call__\n    indices_to_remove = scores &lt; torch.topk(scores, top_k)[0][..., -1, None]\nRuntimeError: &quot;topk_cpu&quot; not implemented for 'Half'\n</code></pre>\n<hr />\n<h2>Bounty Section: Small GPT2 code example</h2>\n<p>Yes I agree that pad is assigned to eos. Eos is still eos. But during fine-tuning now the weights wrt to eos are unchanged. This might be an issue since the probability of eos has not shifted to the fine-tuning regime. One possibility is that eos is outputed with less chance. Yes we can still halt production when we see eos but we've not shifted the probability to output eos according to our fine-tuning distribution -- but all other tokens have changed distribution. I think this could be an issue because it's not like the old probability of eos is conserved since all tokens probs have changed except eos + even if the old eos prob was conserved, it's wrt wrong distribution (not the fine tuning one).</p>\n<p>e.g.,</p>\n<pre><code>if tokenizer.pad_token_id is None:\n    tokenizer.pad_token = tokenizer.eos_token\n ...\nraw_text_batch='a'\ntokenize_batch={'input_ids': tensor([[   64, 50256, 50256, 50256, 50256]]), 'attention_mask': tensor([[1, 0, 0, 0, 0]])}\n</code></pre>\n<p>but it would have been better to have</p>\n<pre><code>tokenize_batch={'input_ids': tensor([[   64, 50256, 50256, 50256, 50256]]), 'attention_mask': tensor([[1, 1, 0, 0, 0]])}\n</code></pre>\n<p>code</p>\n<pre><code>def test_eos_pad():\n    from datasets import load_dataset\n    import torch\n    from transformers import GPT2Tokenizer, GPT2LMHeadModel\n\n    raw_text_batch = 'a'\n\n    tokenizer = GPT2Tokenizer.from_pretrained(&quot;gpt2&quot;)\n    # print(f'{tokenizer.eos_token=}')\n    # print(f'{tokenizer.eos_token_id=}')\n    # print(f'{tokenizer.pad_token=}')\n    # print(f'{tokenizer.pad_token_id=}')\n\n    # print(f'{raw_text_batch=}')\n    # tokenize_batch = tokenizer(raw_text_batch, padding=&quot;max_length&quot;, max_length=5, truncation=True, return_tensors=&quot;pt&quot;)\n    # print(f'{tokenize_batch=}')\n\n    if tokenizer.pad_token_id is None:\n        tokenizer.pad_token = tokenizer.eos_token\n    probe_network = GPT2LMHeadModel.from_pretrained(&quot;gpt2&quot;)\n    device = torch.device(f&quot;cuda:{0}&quot; if torch.cuda.is_available() else &quot;cpu&quot;)\n    probe_network = probe_network.to(device)\n\n    print(f'{tokenizer.eos_token=}')\n    print(f'{tokenizer.eos_token_id=}')\n    print(f'{tokenizer.pad_token=}')\n    print(f'{tokenizer.pad_token_id=}')\n\n    print(f'{raw_text_batch=}')\n    tokenize_batch = tokenizer(raw_text_batch, padding=&quot;max_length&quot;, max_length=5, truncation=True, return_tensors=&quot;pt&quot;)\n    print(f'{tokenize_batch=}')\n    print('Done')\n</code></pre>\n<hr />\n<p>cross:</p>\n<ul>\n<li>hf discuss forum: <a href=\"https://discuss.huggingface.co/t/why-does-the-falcon-qlora-tutorial-code-use-eos-token-as-pad-token/45954\" rel=\"nofollow noreferrer\">https://discuss.huggingface.co/t/why-does-the-falcon-qlora-tutorial-code-use-eos-token-as-pad-token/45954</a></li>\n<li>pytorch forum discuss: <a href=\"https://discuss.pytorch.org/t/how-does-one-set-the-pad-token-correctly-not-to-eos-during-fine-tuning-to-avoid-model-not-predicting-eos/213619\" rel=\"nofollow noreferrer\">https://discuss.pytorch.org/t/how-does-one-set-the-pad-token-correctly-not-to-eos-during-fine-tuning-to-avoid-model-not-predicting-eos/213619</a></li>\n<li><a href=\"https://discord.com/channels/879548962464493619/1126681170957045770/1126681170957045770\" rel=\"nofollow noreferrer\">https://discord.com/channels/879548962464493619/1126681170957045770/1126681170957045770</a></li>\n<li>context peft pacman100 code: <a href=\"https://gist.github.com/pacman100/1731b41f7a90a87b457e8c5415ff1c14\" rel=\"nofollow noreferrer\">https://gist.github.com/pacman100/1731b41f7a90a87b457e8c5415ff1c14</a></li>\n<li>twitter tweet of this: <a href=\"https://twitter.com/BrandoHablando/status/1693676898013061337?s=20\" rel=\"nofollow noreferrer\">https://twitter.com/BrandoHablando/status/1693676898013061337?s=20</a></li>\n</ul>\n",
        "answer": "<p>the code for finding the first occurance of the EOS/PAD and not setting it to -100 can be simplified to this:</p>\n<pre><code>if self.tokenizer.pad_token_id == self.tokenizer.eos_token_id:\n    mask = ((batch['input_ids'] == self.tokenizer.pad_token_id).cumsum(axis=1) == 1)\n    batch['labels'][mask] = self.tokenizer.eos_token_id\n</code></pre>\n<p>You can add the code snippet right after your collator function completes, so you can still set the PAD token to EOS while training the model to generate the EOS.</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 76633368,
            "link": "https://stackoverflow.com/questions/76633368/how-does-one-set-the-pad-token-correctly-not-to-eos-during-fine-tuning-to-avoi"
        }
    },
    {
        "question": "SSLError: HTTPSConnectionPool(host=&#39;huggingface.co&#39;, port=443): Max retries exceeded with url: /dslim/bert-base-NER/resolve/main/tokenizer_config.json\n<p>I am facing below issue while loading the pretrained BERT model from HuggingFace due to SSL certificate error.</p>\n<h2>Error:</h2>\n<blockquote>\n<p>SSLError: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /dslim/bert-base-NER/resolve/main/tokenizer_config.json (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self signed certificate in certificate chain (_ssl.c:1108)')))</p>\n</blockquote>\n<h2>The line that is causing the issue is:</h2>\n<pre><code>tokenizer = AutoTokenizer.from_pretrained(&quot;dslim/bert-base-NER&quot;)\n</code></pre>\n<h2>Source code:</h2>\n<pre><code>from transformers import AutoTokenizer, AutoModelForTokenClassification\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(&quot;dslim/bert-base-NER&quot;)\nmodel = AutoModelForTokenClassification.from_pretrained(&quot;dslim/bert-base-NER&quot;)\n</code></pre>\n<p>I am expecting to download pre-trained models while running the code in jupyter lab on Windows.</p>\n",
        "answer": "<pre><code>import os\n\nos.environ['CURL_CA_BUNDLE'] = r&quot;C:/examples/ssls/all.crt&quot;\nos.environ['REQUESTS_CA_BUNDLE'] = r&quot;C:/examples/ssls/all.crt&quot;\n</code></pre>\n<p>Note this error may come for other connections also.</p>\n<ol>\n<li><p>Download all those certificates from browser address browser\npadlock.</p>\n</li>\n<li><p>Download the &quot;chain&quot; not just individual (it appeared 2nd option in combo while save).  The question of this post has error related\nto chain.  Because verification will happen till root CA.  Probably\nit was downloaded as single cert. (refer screenshot)</p>\n</li>\n<li><p>Multiple .crt files if you have concat all using concat command (comes with git bash)</p>\n<p>concat a.crt b.crt c.crt &gt; all.crt.</p>\n</li>\n</ol>\n<p><a href=\"https://i.sstatic.net/266KaceM.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/266KaceM.png\" alt=\"enter image description here\" /></a></p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 75110981,
            "link": "https://stackoverflow.com/questions/75110981/sslerror-httpsconnectionpoolhost-huggingface-co-port-443-max-retries-exce"
        }
    },
    {
        "question": "How can I see the size of a HuggingFace dataset before downloading it?\n<p>I want to download a HuggingFace dataset, e.g. <a href=\"https://huggingface.co/datasets/uonlp/CulturaX\" rel=\"nofollow noreferrer\"><code>uonlp/CulturaX</code></a>:</p>\n<pre><code>from datasets import load_dataset\nds = load_dataset(&quot;uonlp/CulturaX&quot;, &quot;en&quot;)\n</code></pre>\n<p>How can I see the size of a HuggingFace dataset before downloading it?</p>\n",
        "answer": "<p>One can use <code>HfApi.api.dataset_info</code> from <code>huggingface_hub</code>:</p>\n<pre><code>from huggingface_hub import HfApi\n\ndef print_dataset_file_sizes(repo_id):\n    api = HfApi()\n    dataset_info = api.dataset_info(repo_id=repo_id, files_metadata=True)\n\n    total_size_bytes = 0  \n    print(f&quot;File sizes for dataset '{repo_id}':\\n&quot;)  \n    for sibling in dataset_info.siblings:  \n        filename = sibling.rfilename  \n        size_in_bytes = sibling.size or 0  \n        total_size_bytes += size_in_bytes  \n        size_mb = size_in_bytes / (1024 * 1024)  \n        print(f&quot;  {filename}: {size_mb:.2f} MiB&quot;)  \n\n    total_size_mb = total_size_bytes / (1024 * 1024)  \n    print(f&quot;\\nTotal size: {total_size_mb:.2f} MiB&quot;)  \n\nprint_dataset_file_sizes('uonlp/CulturaX')\n</code></pre>\n<p>Outputs: <code>Total size: 16658008.77 MiB</code>, i.e. ~15.9 TiB.</p>\n<p>Note that the same code can be used to see the size of a HuggingFace model before downloading it, just replace <code>HfApi.api.dataset_info</code> with <code>HfApi.api.model_info</code>, e.g.:</p>\n<pre><code>from huggingface_hub import HfApi\n\ndef print_dataset_file_sizes(repo_id):\n    api = HfApi()\n    dataset_info = api.model_info(repo_id=repo_id, files_metadata=True)\n\n    total_size_bytes = 0  \n    print(f&quot;File sizes for dataset '{repo_id}':\\n&quot;)  \n    for sibling in dataset_info.siblings:  \n        filename = sibling.rfilename  \n        size_in_bytes = sibling.size or 0  \n        total_size_bytes += size_in_bytes  \n        size_mb = size_in_bytes / (1024 * 1024)  \n        print(f&quot;  {filename}: {size_mb:.2f} MiB&quot;)  \n\n    total_size_mb = total_size_bytes / (1024 * 1024)  \n    print(f&quot;\\nTotal size: {total_size_mb:.2f} MiB&quot;)  \n\nprint_dataset_file_sizes('deepseek-ai/DeepSeek-V3')\n</code></pre>\n<p>outputs: <code>Total size: 656703.88 MiB</code> (i.e., ~641.3 GiB).</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 77139182,
            "link": "https://stackoverflow.com/questions/77139182/how-can-i-see-the-size-of-a-huggingface-dataset-before-downloading-it"
        }
    },
    {
        "question": "Why does llama-index still require an OpenAI key when using Hugging Face local embedding model?\n<p>I am creating a very simple question and answer app based on documents using llama-index. Previously, I had it working with OpenAI. Now I want to try using no external APIs so I'm trying the Hugging Face example <a href=\"https://gpt-index.readthedocs.io/en/latest/core_modules/model_modules/llms/usage_custom.html#example-using-a-huggingface-llm\" rel=\"noreferrer\">in this link</a>.</p>\n<p>It says in the example in the link: &quot;Note that for a completely private experience, also setup a local embedding model (example here).&quot; I'm assuming the example given below is the example being referred to. So, naturally, I'm trying to copy the example (<a href=\"https://gpt-index.readthedocs.io/en/latest/examples/customization/llms/SimpleIndexDemo-Huggingface_stablelm.html\" rel=\"noreferrer\">fuller example here</a>).</p>\n<p>Here is my code:</p>\n<pre class=\"lang-py prettyprint-override\"><code>from pathlib import Path\nimport gradio as gr\nimport sys\nimport logging\nimport os\n\nfrom llama_index.llms import HuggingFaceLLM\nfrom llama_index.prompts.prompts import SimpleInputPrompt\n\nlogging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n\n\nfrom llama_index import SimpleDirectoryReader, VectorStoreIndex, ServiceContext, load_index_from_storage, StorageContext\n\nstorage_path = &quot;storage/&quot;\n\ndocs_path=&quot;docs&quot;\n\ndef construct_index(directory_path):\n    max_input_size = 4096\n    num_outputs = 512\n    #max_chunk_overlap = 20\n    chunk_overlap_ratio = 0.1\n    chunk_size_limit = 600\n\n    #prompt_helper = PromptHelper(max_input_size, num_outputs, chunk_overlap_ratio, chunk_size_limit=chunk_size_limit)\n\n    system_prompt = &quot;&quot;&quot;&lt;|SYSTEM|&gt;# StableLM Tuned (Alpha version)\n    - StableLM is a helpful and harmless open-source AI language model developed by StabilityAI.\n    - StableLM is excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n    - StableLM is more than just an information source, StableLM is also able to write poetry, short stories, and make jokes.\n    - StableLM will refuse to participate in anything that could harm a human.\n    &quot;&quot;&quot;\n\n    # This will wrap the default prompts that are internal to llama-index\n    query_wrapper_prompt = SimpleInputPrompt(&quot;&lt;|USER|&gt;{query_str}&lt;|ASSISTANT|&gt;&quot;)\n\n\n    llm = HuggingFaceLLM(\n        context_window=4096,\n        max_new_tokens=256,\n        generate_kwargs={&quot;temperature&quot;: 0.7, &quot;do_sample&quot;: False},\n        system_prompt=system_prompt,\n        query_wrapper_prompt=query_wrapper_prompt,\n        tokenizer_name=&quot;StabilityAI/stablelm-tuned-alpha-3b&quot;,\n        model_name=&quot;StabilityAI/stablelm-tuned-alpha-3b&quot;,\n        device_map=&quot;auto&quot;,\n        stopping_ids=[50278, 50279, 50277, 1, 0],\n        tokenizer_kwargs={&quot;max_length&quot;: 4096},\n        # uncomment this if using CUDA to reduce memory usage\n        # model_kwargs={&quot;torch_dtype&quot;: torch.float16}\n    )\n    #llm=ChatOpenAI(temperature=0.7, model_name=&quot;gpt-3.5-turbo&quot;, max_tokens=num_outputs)\n    #llm_predictor = LLMPredictor(llm=llm)\n    service_context = ServiceContext.from_defaults(chunk_size=1024, llm=llm)\n\n    documents = SimpleDirectoryReader(directory_path).load_data()\n\n    index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n    #index = VectorStoreIndex(documents, llm_predictor=llm_predictor, prompt_helper=prompt_helper)\n\n    index.storage_context.persist(persist_dir=storage_path)\n\n    return index\n\ndef chatbot(input_text):\n    index = load_index_from_storage(StorageContext.from_defaults(persist_dir=storage_path))\n    #index = GPTVectorStoreIndex.load_from_disk('index.json')\n    #query_engine = index.as_query_engine(response_synthesizer=response_synthesizer);\n    query_engine = index.as_query_engine(streaming=True)\n\n    response = query_engine.query(input_text)\n\n    print(response.source_nodes)\n\n    relevant_files=[]\n\n    for node_with_score in response.source_nodes:\n        print(node_with_score)\n        print(node_with_score.node)\n        print(node_with_score.node.metadata)\n        print(node_with_score.node.metadata['file_name'])\n\n        file = node_with_score.node.metadata['file_name']\n        print( file )\n\n        # Resolve the full file path for the downloading\n        full_file_path = Path( docs_path, file ).resolve()\n\n        # See if it's already in the array\n        if full_file_path not in relevant_files:\n            relevant_files.append( full_file_path ) # Add it\n\n    print( relevant_files )\n\n    return response.get_response(), relevant_files\n\niface = gr.Interface(fn=chatbot,\n                     inputs=gr.components.Textbox(lines=7, label=&quot;Enter your text&quot;),\n                     outputs=[\n                        gr.components.Textbox(label=&quot;Response&quot;), \n                        gr.components.File(label=&quot;Relevant Files&quot;)\n                        ],\n                     title=&quot;Custom-trained AI Chatbot&quot;,\n                     allow_flagging=&quot;never&quot;)\n\nindex = construct_index(docs_path)\niface.launch(share=False)\n\n</code></pre>\n<p>Regardless, the code errors out saying:</p>\n<pre><code>ValueError: No API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n</code></pre>\n<p>Am I not understanding how to set up a local model?</p>\n",
        "answer": "<p>Turns out I had to set the embed_model to &quot;local&quot; on the ServiceContext.</p>\n<pre class=\"lang-py prettyprint-override\"><code>ServiceContext.from_defaults(chunk_size=1024, llm=llm, embed_model=&quot;local&quot;)\n</code></pre>\n<p>Also, when I was loading the vector index from disk I wasn't setting the llm predictor again which cause a secondary issue. So I decided to make the vector index a global variable. Here is my final code that works.</p>\n<pre class=\"lang-py prettyprint-override\"><code>from pathlib import Path\nimport gradio as gr\nimport sys\nimport logging\nimport os\n\nfrom llama_index.llms import HuggingFaceLLM\nfrom llama_index.prompts.prompts import SimpleInputPrompt\n\nlogging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n\n\nfrom llama_index import SimpleDirectoryReader, VectorStoreIndex, ServiceContext, load_index_from_storage, StorageContext\n\nstorage_path = &quot;storage&quot;\n\ndocs_path=&quot;docs&quot;\n\nprint(storage_path)\n\nmax_input_size = 4096\nnum_outputs = 512\n#max_chunk_overlap = 20\nchunk_overlap_ratio = 0.1\nchunk_size_limit = 600\n\n\nsystem_prompt = &quot;&quot;&quot;&lt;|SYSTEM|&gt;# StableLM Tuned (Alpha version)\n- StableLM is a helpful and harmless open-source AI language model developed by StabilityAI.\n- StableLM is excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- StableLM is more than just an information source, StableLM is also able to write poetry, short stories, and make jokes.\n- StableLM will refuse to participate in anything that could harm a human.\n&quot;&quot;&quot;\n\n# This will wrap the default prompts that are internal to llama-index\nquery_wrapper_prompt = SimpleInputPrompt(&quot;&lt;|USER|&gt;{query_str}&lt;|ASSISTANT|&gt;&quot;)\n\n\nllm = HuggingFaceLLM(\n    context_window=4096,\n    max_new_tokens=256,\n    generate_kwargs={&quot;temperature&quot;: 0.7, &quot;do_sample&quot;: False},\n    system_prompt=system_prompt,\n    query_wrapper_prompt=query_wrapper_prompt,\n    tokenizer_name=&quot;StabilityAI/stablelm-tuned-alpha-3b&quot;,\n    model_name=&quot;StabilityAI/stablelm-tuned-alpha-3b&quot;,\n    device_map=&quot;auto&quot;,\n    stopping_ids=[50278, 50279, 50277, 1, 0],\n    tokenizer_kwargs={&quot;max_length&quot;: 4096},\n    # uncomment this if using CUDA to reduce memory usage\n    # model_kwargs={&quot;torch_dtype&quot;: torch.float16}\n)\n\nservice_context = ServiceContext.from_defaults(chunk_size=1024, llm=llm, embed_model=&quot;local&quot;)\n\ndocuments = SimpleDirectoryReader(docs_path).load_data()\n\nindex = VectorStoreIndex.from_documents(documents, service_context=service_context)\n\n\ndef chatbot(input_text):\n    query_engine = index.as_query_engine()\n\n    response = query_engine.query(input_text)\n\n    print(response.source_nodes)\n\n    relevant_files=[]\n\n    for node_with_score in response.source_nodes:\n        print(node_with_score)\n        print(node_with_score.node)\n        print(node_with_score.node.metadata)\n        print(node_with_score.node.metadata['file_name'])\n\n        file = node_with_score.node.metadata['file_name']\n        print( file )\n\n        # Resolve the full file path for the downloading\n        full_file_path = Path( docs_path, file ).resolve()\n\n        # See if it's already in the array\n        if full_file_path not in relevant_files:\n            relevant_files.append( full_file_path ) # Add it\n\n    print( relevant_files )\n\n    return response.response, relevant_files\n\niface = gr.Interface(fn=chatbot,\n                     inputs=gr.components.Textbox(lines=7, label=&quot;Enter your text&quot;),\n                     outputs=[\n                        gr.components.Textbox(label=&quot;Response&quot;), \n                        gr.components.File(label=&quot;Relevant Files&quot;)\n                        ],\n                     title=&quot;Custom-trained AI Chatbot&quot;,\n                     allow_flagging=&quot;never&quot;)\n\niface.launch(share=False)\n</code></pre>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 76771761,
            "link": "https://stackoverflow.com/questions/76771761/why-does-llama-index-still-require-an-openai-key-when-using-hugging-face-local-e"
        }
    },
    {
        "question": "How to Load a 4-bit Quantized VLM Model from Hugging Face with Transformers?\n<p>I’m new to quantization and working with visual language models (VLM).I’m trying to load a 4-bit quantized version of the Ovis1.6-Gemma model from Hugging Face using the transformers library. I downloaded the model from this link: <a href=\"https://huggingface.co/ThetaCursed/Ovis1.6-Gemma2-9B-bnb-4bit\" rel=\"nofollow noreferrer\">https://huggingface.co/ThetaCursed/Ovis1.6-Gemma2-9B-bnb-4bit</a>.</p>\n<p>Here’s the code I’m using to load the model:</p>\n<pre class=\"lang-py prettyprint-override\"><code>from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n\n# Define the quantization configuration\nkwargs = {\n    &quot;quantization_config&quot;: BitsAndBytesConfig(\n        load_in_4bit=True,\n        load_in_8bit=False,\n        bnb_4bit_compute_dtype=&quot;float32&quot;,\n        bnb_4bit_quant_storage=&quot;uint8&quot;,\n        bnb_4bit_quant_type=&quot;fp4&quot;,\n        bnb_4bit_use_double_quant=False,\n        llm_int8_enable_fp32_cpu_offload=False,\n        llm_int8_has_fp16_weight=False,\n        llm_int8_skip_modules=None,\n        llm_int8_threshold=6.0\n    )\n}\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    &quot;ThetaCursed/Ovis1.6-Gemma2-9B-bnb-4bit&quot;,\n    trust_remote_code=True,\n    **kwargs\n).cuda()\n</code></pre>\n<p>However, I am encountering the following warnings:</p>\n<pre class=\"lang-bash prettyprint-override\"><code>warnings.warn(_BETA_TRANSFORMS_WARNING)\nUnused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method'].\nLoading checkpoint shards: 100%|██████████| 2/2 [00:06&lt;00:00,  3.06s/it]\nYou shouldn't move a model that is dispatched using accelerate hooks.\n</code></pre>\n<p>Additionally, when I try to access the tokenizers:</p>\n<pre class=\"lang-py prettyprint-override\"><code>text_tokenizer = model.get_text_tokenizer()\nvisual_tokenizer = model.get_visual_tokenizer()\n</code></pre>\n<p>I get the following error:</p>\n<pre class=\"lang-bash prettyprint-override\"><code>AttributeError: 'NoneType' object has no attribute 'get_text_tokenizer'\n</code></pre>\n<p>How can I properly load the 4-bit quantized model without encountering these warnings?\nWhy am I receiving an AttributeError when trying to access the tokenizers? Does this model not support them?</p>\n",
        "answer": "<p>The problem is using <code>.cuda()</code> to move the model to the GPU when loading the model using the <code>BitsAndBytesConfig</code>. I was able to get the error to go away by using the <code>device_map</code> argument when loading the model, e.g.,</p>\n<pre><code>model = AutoModelForCausalLM.from_pretrained(\n    &quot;ThetaCursed/Ovis1.6-Gemma2-9B-bnb-4bit&quot;,\n    trust_remote_code=True,\n    device_map='auto',\n    **kwargs\n)\n</code></pre>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 79130264,
            "link": "https://stackoverflow.com/questions/79130264/how-to-load-a-4-bit-quantized-vlm-model-from-hugging-face-with-transformers"
        }
    },
    {
        "question": "ModuleNotFoundError: No module named &#39;llama_index.postprocessor&#39;\n<p>I tried to import CohereRerank module from llama_index. But got a <code>ModuleNotFoundError: No module named 'llama_index.postprocessor'</code>.</p>\n",
        "answer": "<p>Try importing it this way :</p>\n<p><code>from llama_index.postprocessor.cohere_rerank import CohereRerank</code></p>\n<p>Ensure you have the following installed in your env</p>\n<p><code>%pip install -q cohere llama-index-postprocessor-cohere-rerank</code></p>\n<p>if the error still persists try making a fresh install for llamaindex and retry, it will work.</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 78333326,
            "link": "https://stackoverflow.com/questions/78333326/modulenotfounderror-no-module-named-llama-index-postprocessor"
        }
    },
    {
        "question": "Vertex AI - The token is not valid or not have permission\n<p>I’ve been trying to deploy Llama 3 from Hugging Face on Google’s Vertex AI (from the Model Garden) but I can’t do it as I keep getting an error with my access token. Google keeps telling me that ‘The token is not valid or not have permission’.</p>\n<p>I’ve tried read only tokens, write tokens, fine-grained tokens with all permissions enabled and just can’t get it to work.</p>\n",
        "answer": "<p>1.Ensure your access token is fine-grained and grants &quot;Read access to contents of all public gated repos you can access.&quot;</p>\n<p>2.Accept the license for the chosen model in your Hugging Face account and acknowledge the license agreement.</p>\n<p><a href=\"https://i.sstatic.net/ZEcUHpmS.png\" rel=\"nofollow noreferrer\">Accept the license</a></p>\n<p>Once these two steps are completed, your access token should be accepted for model deployment.</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 78598935,
            "link": "https://stackoverflow.com/questions/78598935/vertex-ai-the-token-is-not-valid-or-not-have-permission"
        }
    },
    {
        "question": "How to ensure last token in sequence is end-of-sequence token?\n<p>I am using the <code>gpt2</code> model from huggingface's <code>transformers</code> library. When tokenizing, I would like all sequences to end in the end-of-sequence (EOS) token.  How can I do this?</p>\n<p>An easy solution is to manually append the EOS token to each sequence in a batch prior to tokenization:</p>\n<pre><code>from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained('gpt2')\n\ntext = ['Hello world.', 'I am program.']\n\ntext = [el + tokenizer.eos_token for el in text]\n\ntokenized_text = tokenizer(text)\n\n</code></pre>\n<p>It seems like my solution is an inelegant solution for this task that is so commonplace that I expect there is some built-in way of doing this.  I haven't found anything about this in the documentation—is there a way?</p>\n<p>Edit: I want to do this in order to train a GPT-2 to generate specific kinds of responses to input sequences.  Without the end-of-sequence token during training, performance was poor, and the model generated much too much text.</p>\n",
        "answer": "<p>I would do this using f-strings.</p>\n<p><code>text = [f&quot;{el}{tokenizer.eos_token}&quot; for el in text]</code></p>\n<p>I also like using pandas:</p>\n<p><code>df[&quot;text_w_eos&quot;] = df.text.apply(lambda x: f&quot;{x}{tokenizer.eos_token}&quot;)</code></p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 75653539,
            "link": "https://stackoverflow.com/questions/75653539/how-to-ensure-last-token-in-sequence-is-end-of-sequence-token"
        }
    },
    {
        "question": "AttributeError: &#39;AcceleratorState&#39; object has no attribute &#39;distributed_type&#39;\n<pre><code>import transformers\nfrom datasets import load_dataset\nimport tensorflow as tf\n\ntokenizer = transformers.AutoTokenizer.from_pretrained('roberta-base')\n\ndf = load_dataset('csv', data_files={'train':'FinalDatasetTrain.csv', 'test':'FinalDatasetTest.csv'})\n\ndef tokenize_function(examples):\n    return tokenizer(examples[&quot;text&quot;], truncation=True)\n\ntokenized_datasets = df.map(tokenize_function, batched=True)\ndata_collator = transformers.DataCollatorWithPadding(tokenizer=tokenizer)\n\nmodel = transformers.AutoModelForSequenceClassification.from_pretrained('roberta-base', num_labels=7)\n\ntraining_args = transformers.TFTrainingArguments(\n    output_dir=&quot;./results&quot;,\n    num_train_epochs=2,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=16,\n    save_strategy='epoch',\n    evaluation_strategy=&quot;epoch&quot;,\n    logging_dir=&quot;./logs&quot;,\n)\n\ntrainer = transformers.Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets['train'],\n    eval_dataset=tokenized_datasets['test'],\n    data_collator=data_collator,\n    tokenizer=tokenizer\n)\n\ntrainer.train()\n</code></pre>\n<p>When I run this code I get an error saying:</p>\n<blockquote>\n<p>AttributeError: 'AcceleratorState' object has no attribute 'distributed_type'.</p>\n</blockquote>\n<p>How do I fix this (I tried both Jupyter notebook and Google Colab)?</p>\n",
        "answer": "<p>I had the same issue in Colab. Make sure you have the latest versions of transformers and accelerate installed. Once you install them, restarting the runtime would solve the issue.</p>\n<pre><code>!pip install git+https://github.com/huggingface/accelerate\n!pip install --upgrade transformers\n</code></pre>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 76516579,
            "link": "https://stackoverflow.com/questions/76516579/attributeerror-acceleratorstate-object-has-no-attribute-distributed-type"
        }
    },
    {
        "question": "How to parse a resume with few shot method using the specified models from HuggingFace and Langchain?\n<h1>Model selection confusion and some errors while trying to parse a resume with the following codes</h1>\n<ul>\n<li>Trying to do a few shot prompting with a google flan t5 base model</li>\n<li>While Doing so I am getting an error\n<code>ERROR:Service.service:Error parsing resume: &quot;'ContactInformation'&quot;</code></li>\n<li>exception as <code>&quot;detail&quot;: &quot;Failed to parse the file: An error occurred in parsed_resume: \\&quot;'ContactInformation'\\&quot;&quot;</code></li>\n</ul>\n<h2>The code is given below</h2>\n<pre class=\"lang-none prettyprint-override\"><code>from typing import List, Optional, Union, Any, re\nimport json\n\n\n\nclass ContactInformation(BaseModel):\n    Name: Optional[str] = None\n    Email: Optional[str] = None\n    Contact: Optional[str] = None\n    Links: Optional[List[str]] = None\n\n\n\nclass Experience(BaseModel):\n    title: Optional[str] = None\n    company: Optional[str] = None\n    duration: Optional[str] = None\n\n\nclass Education(BaseModel):\n    course: Optional[str] = None\n    branch: Optional[str] = None\n    institute: Optional[str] = None\n\n\nclass Projects(BaseModel):\n    name: Optional[str] = None\n    description: Optional[str] = None\n    link: Optional[str] = None\n\nclass OutputFormat(BaseModel):\n    ContactInformation: Optional[Any] = None\n    AboutMe: Optional[Any] = None\n    Experiences: Optional[List[Any]] = None\n    Educations: Optional[List[Any]] = None\n    Skills: Optional[List[Any]] = None\n    Certificates: Optional[List[Any]] = None\n    Projects: Optional[List[Any]] = None\n    Achievements: Optional[List[Any]] = None\n    Volunteer: Optional[List[Any]] = None\n\n</code></pre>\n<pre class=\"lang-none prettyprint-override\"><code>    def __init__(self, model_name=model_1, fine_tune_model_path: str = None):\n        # Initialize LLM service with specified model\n\n\n        if fine_tune_model_path:\n            # Load fine-tuned model from local directory\n            self.tokenizer = AutoTokenizer.from_pretrained(fine_tune_model_path)\n            self.model = AutoModel.from_pretrained(fine_tune_model_path)\n        else:\n            # Load base model\n            self.llm_service = HuggingFaceHub(\n                repo_id=&quot;google/flan-t5-base&quot;,\n                huggingfacehub_api_token=huggingface_api_key,\n                model_kwargs={\n                    &quot;temperature&quot;: 0.5,\n                    &quot;max_new_tokens&quot;: 200\n                }  # Model parameters for consistent output\n            )\n    def parsed_resume(self, resume_txt: str):\n        df = pd.read_csv(r&quot;C:\\Users\\Sarthak\\PycharmProjects\\JobAxle\\Service\\data\\for_model_resume_dataset.csv&quot;)\n        print(df['prompt'][0])\n        examples = [\n            {'prompt':df['prompt'][0], &quot;completion&quot;:df['completion'][0]},\n            {'prompt': df['prompt'][1], &quot;completion&quot;: df['completion'][1]}\n        ]\n        print('Examples:',examples[0])\n        example_formatter_template = &quot;&quot;&quot;\n        {prompt}\n        {completion}\\n\n        &quot;&quot;&quot;\n        example_prompt = PromptTemplate(\n            input_variables=[&quot;prompt&quot;, &quot;completion&quot;],\n            template=example_formatter_template,\n        )\n        parser = PydanticOutputParser(pydantic_object=OutputFormat)\n        few_shot_prompt_template = FewShotPromptTemplate(\n            examples=examples,\n            example_prompt=example_prompt,\n            suffix=&quot;&quot;&quot;\n                Parse the given resume text, ensuring the output in JSON format:\n        \n                Resume:\n                {resume}\n        \n                {format_instructions}\n        \n                Output as JSON below:\n                completion:&quot;&quot;&quot;,\n            input_variables=[&quot;resume&quot;],\n            example_separator=&quot;\\n&quot;,\n            partial_variables={&quot;format_instructions&quot;: parser.get_format_instructions()}\n        )\n        print(&quot;Few-Shot Prompt Template with Examples and JSON Instructions:\\n&quot;, few_shot_prompt_template)\n\n        prompt_template = PromptTemplate(\n            input_variables=['resume'],\n            template=Prompt_2\n        )\n        # print(few_shot_prompt_template)\n        # Initialize the LLM chain\n        chain = LLMChain(\n            llm=self.llm_service,\n            prompt=few_shot_prompt_template,\n            verbose=True\n        )\n        print(&quot;Chain:&quot;, chain)\n        print(resume_txt)\n        try:\n            response = chain.invoke({'resume': resume_txt}, verbose=True)\n            print(response)\n            logger.info('Model Response: %s', response)\n            print(&quot;Type of Response:&quot;,type(response))\n            # Invoke the chain and get a response\n            response_json = self.process_response(response)\n            parsed_json = self.structure_response(response_json)\n            return OutputFormat(**parsed_json)  # Return as OutputFormat object\n\n        except Exception as e:\n            logger.error(&quot;Error parsing resume: %s&quot;, e)\n            raise Exception(f&quot;An error occurred in parsed_resume: {e}&quot;)\n\n    def process_response(self, response_text: str) -&gt; Dict:\n        &quot;&quot;&quot;Process LLM response into JSON format.&quot;&quot;&quot;\n        response_json = json.dumps(response_text).strip()\n\n        # Remove extraneous characters\n        if response_json.startswith(&quot;```json&quot;):\n            response_json = response_json[len(&quot;```json&quot;):].strip()\n        if response_json.endswith(&quot;```&quot;):\n            response_json = response_json[:-len(&quot;```&quot;)].strip()\n\n        response_json = remove_trailing_commas(response_json)\n\n        try:\n\n            return json.loads(response_json)\n        except json.JSONDecodeError as e:\n            logger.error(&quot;JSON decoding error: %s. Response text: %s&quot;, e, response_json)\n            raise ValueError(&quot;Failed to parse response as valid JSON&quot;)\n\n    def structure_response(self, parsed_json: Dict) -&gt; Dict:\n        &quot;&quot;&quot;\n        Structure response JSON to match the OutputFormat schema, ensuring lists for 'Experiences' and 'Educations'.\n        &quot;&quot;&quot;\n        # Ensure ContactInformation, Experiences, and Educations are present in the expected structure\n        parsed_json[&quot;ContactInformation&quot;] = parsed_json.get(&quot;ContactInformation&quot;, {})\n        if isinstance(parsed_json.get(&quot;Experiences&quot;), dict):\n            parsed_json[&quot;Experiences&quot;] = [parsed_json[&quot;Experiences&quot;]]\n        else:\n            parsed_json[&quot;Experiences&quot;] = parsed_json.get(&quot;Experiences&quot;, [])\n\n        if isinstance(parsed_json.get(&quot;Educations&quot;), dict):\n            parsed_json[&quot;Educations&quot;] = [parsed_json[&quot;Educations&quot;]]\n        else:\n            parsed_json[&quot;Educations&quot;] = parsed_json.get(&quot;Educations&quot;, [])\n\n        parsed_json[&quot;Projects&quot;] = parsed_json.get(&quot;Projects&quot;, [])\n\n        return parsed_json\n</code></pre>\n<p>The df contains prompt, completion feature with resume_text as a prompt and json output as completion.</p>\n<p>Or Is there any Alternative to doing the task on a low-end specs laptop? I am in need of help because I can't access big parameter models. anybody??</p>\n",
        "answer": "<p><strong>Possible reason 1:</strong></p>\n<p>The LLM sometime does not produce consistent output. It might be possible that the key &quot;ContactInformation&quot; might not be present in the output generated by the model. So first do the dubugging and check the output of LLM directly.</p>\n<p>Try this parsed_json.get(&quot;ContactInformation&quot;, {})</p>\n<p>In case if the key is not present then it will add empty dict.</p>\n<p><strong>Possible reason 2:</strong></p>\n<p>It might be possible that the output generated by LLM is not even JSON. In that case it wont be able to use the get method on the JOSN.</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 79180131,
            "link": "https://stackoverflow.com/questions/79180131/how-to-parse-a-resume-with-few-shot-method-using-the-specified-models-from-huggi"
        }
    },
    {
        "question": "Does unsloth support cache directory for models?\n<p>I want to download a model from hugging face to be used with unsloth for trainig:</p>\n<pre class=\"lang-py prettyprint-override\"><code>from unsloth import FastLanguageModel,\n\nmax_seq_length = 16384\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=&quot;unsloth/Llama-3.2-1B-Instruct&quot;,\n    max_seq_length=max_seq_length,\n    load_in_4bit=False,\n)\n</code></pre>\n<p>However, this method doesn't seem to allow any sort of local caching, it downloads the whole model from hugging face every time.</p>\n<p>My question:\nHow can I load unsloth model from local hard drive?</p>\n",
        "answer": "<p>Turns out it is actually really simple, you load the model like this:</p>\n<pre><code>from unsloth import FastLanguageModel,\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    &quot;/content/model&quot;\n)\n\n</code></pre>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 79201360,
            "link": "https://stackoverflow.com/questions/79201360/does-unsloth-support-cache-directory-for-models"
        }
    },
    {
        "question": "Cannot install llama-index-embeddings-huggingface==0.1.3 because these package versions have conflicting dependencies\n<p>I am unable to install the huggingfaceEmbedding \\</p>\n<p>Getting the followng error:</p>\n<pre><code>ERROR: Cannot install llama-index-embeddings-huggingface==0.1.3, llama-index-embeddings-huggingface==0.1.4 and llama-index-embeddings-huggingface==0.1.5 because these package versions have conflicting dependencies.\nERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\n</code></pre>\n<p><strong>Python version:</strong> 3.13</p>\n<p><a href=\"https://i.sstatic.net/IJXlvJWk.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/IJXlvJWk.png\" alt=\"enter image description here\" /></a></p>\n<p><a href=\"https://i.sstatic.net/CUeR8bMr.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/CUeR8bMr.png\" alt=\"enter image description here\" /></a></p>\n",
        "answer": "<p>Several things I had to do in order to make this work.</p>\n<ol>\n<li>Downgrade to python 3.9.  I went specifically to 3.9.13.  Without that some of your imports in code will not work as well.</li>\n<li>Use a virtual environment.  I can't tell if you have done this.  It may not be necessary but it worked for me.</li>\n<li>Use uv to install packages.  It seems to sort out the dependencies a bit easier than straight pip.  uv can be installed via pip install uv.  Then uv pip instal </li>\n</ol>\n<p>This got the llama_index_embedded_huggingface package to install.</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 79257046,
            "link": "https://stackoverflow.com/questions/79257046/cannot-install-llama-index-embeddings-huggingface-0-1-3-because-these-package-v"
        }
    },
    {
        "question": "Is there any method to fully load the GGUF models on GPU\n<p>I have been using LlamaCPP to load my llm models, the llama-index library provides methods to offload some layers onto the GPU. Why does it not provide any methods to fully load the model on GPU. If there is some method Please help.</p>\n<p><a href=\"https://i.sstatic.net/8tBUc.png\" rel=\"nofollow noreferrer\">LlamaCPP method</a></p>\n<p>Here we have the option to offload some layers on GPU but I want to fully load the model on GPU.</p>\n",
        "answer": "<p>Try setting <code>n_gpu_layers</code> to -1.</p>\n<p><a href=\"https://llama-cpp-python.readthedocs.io/en/latest/api-reference/#__codelineno-0-147\" rel=\"nofollow noreferrer\">Check Official Documentation Here</a></p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 78332474,
            "link": "https://stackoverflow.com/questions/78332474/is-there-any-method-to-fully-load-the-gguf-models-on-gpu"
        }
    },
    {
        "question": "ModuleNotFoundError: No module named &#39;huggingface_hub.utils&#39; using Anaconda\n<p>I'm trying to execute the example code of the huggingface website:</p>\n<pre class=\"lang-py prettyprint-override\"><code>from transformers import GPTJTokenizer, TFGPTJModel\nimport tensorflow as tf\n\ntokenizer = GPTJTokenizer.from_pretrained(&quot;EleutherAI/gpt-j-6B&quot;)\nmodel = TFGPTJModel.from_pretrained(&quot;EleutherAI/gpt-j-6B&quot;)\n\ninputs = tokenizer(&quot;Hello, my dog is cute&quot;, return_tensors=&quot;tf&quot;)\noutputs = model(inputs)\n\nlast_hidden_states = outputs.last_hidden_state\n</code></pre>\n<p>I'm using anaconda and I installed the transformers package beforehand with <code>conda install -c huggingface transformers</code> as explained in the <a href=\"https://huggingface.co/docs/transformers/installation#install-with-conda\" rel=\"noreferrer\">documentation</a>. But I still get this error, when I'm trying to execute the code. Following error message pops up: <code>ModuleNotFoundError: No module named 'huggingface_hub.utils'</code></p>\n<p>How to resolve this error?</p>\n",
        "answer": "<p>Please try:</p>\n<p><code>conda install -c conda-forge huggingface_hub</code></p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 73960201,
            "link": "https://stackoverflow.com/questions/73960201/modulenotfounderror-no-module-named-huggingface-hub-utils-using-anaconda"
        }
    },
    {
        "question": "How does one set the pad token correctly (not to eos) during fine-tuning to avoid model not predicting EOS?\n<h2>tldr; what I really want to know is what is the official way to set pad token for <strong>fine tuning</strong> it wasn't set during original training, so that it doesn't not learn to predict EOS.</h2>\n<p>colab: <a href=\"https://colab.research.google.com/drive/1poFdFYmkR_rDM5U5Z2WWjTepMQ8hvzNc?usp=sharing\" rel=\"nofollow noreferrer\">https://colab.research.google.com/drive/1poFdFYmkR_rDM5U5Z2WWjTepMQ8hvzNc?usp=sharing</a></p>\n<hr />\n<p>The HF falcon tutorial has the following line:</p>\n<pre><code>tokenizer.pad_token = tokenizer.eos_token\n</code></pre>\n<p>it looks strange to me. It make sense pad and eos are the same but then why even make a difference between them in the first place in general?</p>\n<p>Note its wrong to do pad = eos. This means during fine-tuning the model will never be trained to output eos (most likely) since eos is treated as pad token and no back propagated:</p>\n<pre><code>I just observed that when I set tokenizer.pad_token = tokenizer.eos_token during training, the model won't stop generating during inference, since it was trained to not output the eos token (per discussions above).\n</code></pre>\n<p>I saw this (here <a href=\"https://github.com/huggingface/transformers/issues/22794\" rel=\"nofollow noreferrer\">https://github.com/huggingface/transformers/issues/22794</a>):</p>\n<pre><code>tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n</code></pre>\n<p>But this assumes the model has a pad_token. I think an additional check has to be done that it does have an embedding for pad_token so that there are no run time errors (~type errors in the matrix extraction from the embedding &quot;table&quot;/matrix).</p>\n<p>But if one does that some care might be needed to initialize the new token so that it dominates the generation: <a href=\"https://nlp.stanford.edu/%7Ejohnhew/vocab-expansion.html\" rel=\"nofollow noreferrer\">https://nlp.stanford.edu/~johnhew/vocab-expansion.html</a></p>\n<hr />\n<p>code:</p>\n<pre class=\"lang-py prettyprint-override\"><code>def get_model_tokenizer_qlora_falcon7b(model_name: str = &quot;ybelkada/falcon-7b-sharded-bf16&quot;,\n                                       config: wand.Config,  # todo\n                                       lora_alpha=16,  # todo\n                                       lora_dropout=0.1,  # todo\n                                       lora_r=64,  # todo\n                                       bnb_4bit_compute_dtype=torch.float16,  # changed it from Guanaco hf\n                                       ) -&gt; tuple:\n    &quot;&quot;&quot;\n    Load the Falcon 7B model, quantize it in 4bit and attach LoRA adapters on it.\n\n    bf16 = 1S, 7Exp, 8Mantissa\n\n    Do:\n        pip install bitsandbytes\n    ref:\n        - https://colab.research.google.com/drive/1DOi8MFv4SWN9NImVornZ7t6BgmLoPQO-#scrollTo=AjB0WAqFSzlD\n    &quot;&quot;&quot;\n    from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoTokenizer\n\n    # model_id = &quot;tiiuae/falcon-7b&quot;\n    # model_name: str = &quot;ybelkada/falcon-7b-sharded-bf16&quot;\n\n    # - get bnb config for bit-4 base model (bnb lib for using 4bit qlora quantization techniques by tim dettmers)\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,  # load (usually huge) base model in 4 bits\n        bnb_4bit_quant_type=&quot;nf4&quot;,  # normal float 4 for the (usually huge) base model. introduces error but fixed by ft\n        # ref: https://gist.github.com/pacman100/1731b41f7a90a87b457e8c5415ff1c14\n        bnb_4bit_compute_dtype=bnb_4bit_compute_dtype,\n    )\n\n    # - get falcon 4bit model\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        quantization_config=bnb_config,\n        trust_remote_code=True  # allows to execute custom code you download from the uploaded model code you are using\n    )\n    model.config.use_cache = False  # todo: why? https://stackoverflow.com/questions/76633335/why-does-hugging-face-falcon-model-use-mode-config-use-cache-false-why-wouldn\n\n    # get falcon tockenizer\n    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)  # execs code downloaded from hf hub\n    tokenizer.pad_token = tokenizer.eos_token\n</code></pre>\n<hr />\n<h2>Modifying model gives issues</h2>\n<p>This still not works:</p>\n<pre><code> UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n</code></pre>\n<p>code:</p>\n<pre><code>&quot;&quot;&quot;\nsfttrainer (likely using peft) best practices:\nhttps://huggingface.co/docs/trl/main/en/sft_trainer#best-practices\n\nBest practices\n\nPay attention to the following best practices when training a model with that trainer:\n\n- SFTTrainer always pads by default the sequences to the max_seq_length argument of the SFTTrainer. If none is passed, the trainer will retrieve that value from the tokenizer. Some tokenizers do not provide default value, so there is a check to retrieve the minimum between 2048 and that value. Make sure to check it before training.\n- For training adapters in 8bit, you might need to tweak the arguments of the prepare_model_for_int8_training method from PEFT, hence we advise users to use prepare_in_int8_kwargs field, or create the PeftModel outside the SFTTrainer and pass it.\n- For a more memory-efficient training using adapters, you can load the base model in 8bit, for that simply add load_in_8bit argument when creating the SFTTrainer, or create a base model in 8bit outside the trainer and pass it.\n- If you create a model outside the trainer, make sure to not pass to the trainer any additional keyword arguments that are relative to from_pretrained() method.\n\ntodo: why trust_remote_code? I want more details.\n&quot;&quot;&quot;\nimport sys\n\nimport torch\nfrom peft import LoraConfig\n\nfrom transformers.modeling_utils import PreTrainedModel\n\nfrom pdb import set_trace as st\n\n\ndef test_bfloat16_int4(compute_dtype: torch.dtype,\n                       use_4bit,\n                       ):\n    &quot;&quot;&quot;\npython -c &quot;import torch; print(torch.cuda.get_device_capability());&quot;\n    todo: check other code test_bfloat16() do we need use_4bit?\n    &quot;&quot;&quot;\n    if compute_dtype == torch.float16 and use_4bit:\n        major, _ = torch.cuda.get_device_capability()\n        if major &gt;= 8:\n            print(&quot;=&quot; * 80)\n            print(&quot;Your GPU supports bfloat16, you can accelerate training with the argument --bfloat16&quot;)\n            print(&quot;=&quot; * 80)\n\n\ndef get_model_tokenizer_qlora_falcon7b(\n        # -- mode args\n        # model_id = &quot;tiiuae/falcon-7b&quot;\n        pretrained_model_name_or_path: str = &quot;ybelkada/falcon-7b-sharded-bf16&quot;,\n        use_cache: bool = True,\n        # -- lora args\n        lora_alpha=16,  # todo\n        lora_dropout=0.1,  # todo, evidence drop out really help? google, crfm, gpt4\n        lora_r=64,  # todo\n        bnb_4bit_compute_dtype=torch.float16,  # changed it from Guanaco hf\n\n        # -- training args\n        output_dir=&quot;./results&quot;,\n        per_device_train_batch_size=4,\n        gradient_accumulation_steps=4,\n        # paging so that the sudden mem gpu spikes don't cause the run to shut down\n        # (I think usually caused by too long seqs)\n        # todo: why 32 bit opt?\n        # todo: paged nadamw opt?\n        optim=&quot;paged_adamw_32bit&quot;,\n        save_steps=10,\n        logging_steps=10,\n        learning_rate=2e-4,\n        max_grad_norm=0.3,\n        max_steps=500,\n        warmup_ratio=0.03,\n        lr_scheduler_type=&quot;constant&quot;,\n        # -- quant. args (not recommended to be changed unless you know what your doing?)\n        load_in_4bit=True,  # load (usually huge) base model in 4 bits\n        bnb_4bit_quant_type=&quot;nf4&quot;,  # normal float 4 for the (large) base models qlora\n) -&gt; tuple:\n    &quot;&quot;&quot;\n    Load the Falcon 7B model, quantize it in 4bit and attach LoRA adapters on it.\n\n    bf16 = 1S, 7Exp, 8Mantissa\n    hypothesis: 7b trained due to 6.7 emergence rumour, I still don't think emergence is real.\n    Notes:\n        - ft a model is very specific to the model, tokenizer and training scheme. Thus we return\n            - model, tokenizer, ft config (peft config), training args\n\n    ref:\n        - https://colab.research.google.com/drive/1DOi8MFv4SWN9NImVornZ7t6BgmLoPQO-#scrollTo=AjB0WAqFSzlD\n    &quot;&quot;&quot;\n    from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoTokenizer\n\n    # - Get bnb config for bit-4 base model (bnb lib for using 4bit qlora quantization techniques by tim dettmers)\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=load_in_4bit,  # load (usually huge) base model in 4 bits\n        bnb_4bit_quant_type=bnb_4bit_quant_type,  # normal float 4 for the (usually huge) base model\n        bnb_4bit_compute_dtype=bnb_4bit_compute_dtype,  # if you can, during computation use bf16\n    )\n\n    # - Get falcon 4bit model\n    # todo, where is this being saved &amp; how to download quicker\n    model = AutoModelForCausalLM.from_pretrained(\n        pretrained_model_name_or_path=pretrained_model_name_or_path,\n        quantization_config=bnb_config,\n        trust_remote_code=True  # allows to execute custom code you download from the uploaded model code you are using\n    )\n    print(f'{type(model)=}')\n    print(f'{model=}')\n    # this is here to save gpu vram. Likely only needed when using 40b or when oom issues happen ref: https://stackoverflow.com/questions/76633335/why-does-hugging-face-falcon-model-use-mode-config-use-cache-false-why-wouldn\n    model.config.use_cache = use_cache\n    print(f'{type(model)=}')\n\n    # - Get falcon tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path,\n                                              trust_remote_code=True)  # execs code downloaded from hf hub\n    # tokenizer.pad_token = tokenizer.eos_token  # ref: https://stackoverflow.com/questions/76633368/why-does-the-falcon-qlora-tutorial-code-use-eos-token-as-pad-token\n    # tokenizer.add_special_tokens({'pad_token': '[PAD]'})  # I think this is fine if during the training pad is ignored\n    tokenizer.add_special_tokens({'pad_token': '&lt;|pad|&gt;'})  # I think this is fine if during the training pad is ignored\n\n    # - Modify model\n    # add pad token embed\n    model.resize_token_embeddings(len(tokenizer))  # todo: I think this is fine if during the training pad is ignored\n    model.transformer.word_embeddings.padding_idx = len(tokenizer) - 1\n    model.config.max_new_tokens = len(tokenizer)\n    # model.config.min_length = 1\n    print(f'{model=}')\n    print(f'{type(tokenizer)=}')\n    print(f'{tokenizer.pad_token=}')\n    # data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False) todo\n\n    # - Get falcon lora config\n    peft_config = LoraConfig(\n        lora_alpha=lora_alpha,\n        lora_dropout=lora_dropout,\n        r=lora_r,\n        bias=&quot;none&quot;,\n        task_type=&quot;CAUSAL_LM&quot;,\n        # model card for falcon tiiuae/falcon-7b: https://huggingface.co/tiiuae/falcon-7b/blob/main/modelling_RW.py\n        # does seem to include all trainable params as done by qlora on their own paper\n        target_modules=[\n            # word_embeddings,\n            &quot;query_key_value&quot;,\n            &quot;dense&quot;,\n            &quot;dense_h_to_4h&quot;,\n            &quot;dense_4h_to_h&quot;,\n            # &quot;lm_head&quot;\n        ]\n    )\n    print(f'{type(peft_config)=}')\n\n    # todo: print the num params of the lora = D1*r + D2*r and num of bytes by prec. (bytes) * num params\n    return model, tokenizer, peft_config\n\n\n# -- tests\n\ndef example_test_model_already_has_pad_token():\n    &quot;&quot;&quot;\n    if it already has pad token, it likely has a small prob, so we are done.\n\n    compare it's norm with other tokens to verify this is true.\n\npython ~/ultimate-utils/ultimate-utils-proj-src/uutils/hf_uu/model_tokenizer/falcon_uu_mdl_tok.py\n    &quot;&quot;&quot;\n    # - the get datasets todo: preprocessing, padding, streaming\n    from uutils.hf_uu.data_hf.common import get_guanaco_datsets_add_splits_train_test_only\n    trainset, _, testset = get_guanaco_datsets_add_splits_train_test_only()\n\n    # qlora flacon7b\n    from uutils.hf_uu.model_tokenizer.falcon_uu_mdl_tok import get_model_tokenizer_qlora_falcon7b\n    model, tokenizer, peft_config = get_model_tokenizer_qlora_falcon7b()\n    model: PreTrainedModel = model\n    print(f'{model=}')\n    sent = 'Dogs are great because they are '\n    print()\n\n    # print to see if pad tokens are present and if it ignores the tokens at the end\n    encoded_input = tokenizer(sent, padding='max_length', max_length=10, return_tensors='pt')\n    print(f'{encoded_input=}')\n\n    # Print all special tokens\n    print('\\n---- start Print all special tokens')\n    for token_name, token in tokenizer.special_tokens_map.items():\n        print(f&quot;{token_name}: {token}&quot;)\n    print('\\n---- end Print all special tokens')\n\n    # Get the ID for the '[PAD]' token\n    try:\n        pad_token_id = tokenizer.convert_tokens_to_ids('[PAD]')\n    except KeyError:\n        raise ValueError(&quot;Token [PAD] is not present in the tokenizer vocabulary.&quot;)\n\n    # Index into the model's embedding table\n    try:\n        print(f'{model.get_input_embeddings().weight.size()=}')\n        pad_embedding = model.get_input_embeddings().weight[pad_token_id]\n    except IndexError:\n        raise ValueError(f&quot;Token ID {pad_token_id} is not present in the model's embedding matrix.&quot;)\n\n    print(f'{pad_embedding=}')\n    print('Success!\\n')\n\n    # check it generates something sensible\n    # tokenizer.decode(model.generate(**tokenizer(sent, return_tensors='pt'), do_sample=True)[0])\n    input_ids, attention_mask = encoded_input['input_ids'], encoded_input['attention_mask']\n    predicted_tokens_ids_options = model.generate(input_ids=input_ids, attention_mask=attention_mask, do_sample=True)\n    predicted_tokens_ids = predicted_tokens_ids_options[0]\n    predicted_sent = tokenizer.decode(predicted_tokens_ids)\n    print(f'original sentence: {sent=}')\n    print(f'predicted sentence: {predicted_sent=}')\n    print('Success2!')\n\n\nif __name__ == '__main__':\n    import time\n\n    start_time = time.time()\n    example_test_model_already_has_pad_token()\n    print(f&quot;The main function executed in {time.time() - start_time} seconds.\\a&quot;)\n</code></pre>\n<p>it doesn't like the modifications to the model:</p>\n<pre><code>    model.transformer.word_embeddings.padding_idx = len(tokenizer) - 1\n    model.config.max_new_tokens = len(tokenizer)\n</code></pre>\n<p>How to fix?</p>\n<p>Errors:</p>\n<pre><code>/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/transformers/generation/utils.py:1259: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n  warnings.warn(\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/transformers/generation/utils.py:1452: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n  warnings.warn(\nTraceback (most recent call last):\n  File &quot;/lfs/hyperturing1/0/brando9/ultimate-utils/ultimate-utils-proj-src/uutils/hf_uu/model_tokenizer/falcon_uu_mdl_tok.py&quot;, line 211, in &lt;module&gt;\n    example_test_model_already_has_pad_token()\n  File &quot;/lfs/hyperturing1/0/brando9/ultimate-utils/ultimate-utils-proj-src/uutils/hf_uu/model_tokenizer/falcon_uu_mdl_tok.py&quot;, line 199, in example_test_model_already_has_pad_token\n    predicted_tokens_ids_options = model.generate(input_ids=input_ids, attention_mask=attention_mask, do_sample=True)\n  File &quot;/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/torch/utils/_contextlib.py&quot;, line 115, in decorate_context\n    return func(*args, **kwargs)\n  File &quot;/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/transformers/generation/utils.py&quot;, line 1572, in generate\n    return self.sample(\n  File &quot;/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/transformers/generation/utils.py&quot;, line 2633, in sample\n    next_token_scores = logits_warper(input_ids, next_token_scores)\n  File &quot;/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/transformers/generation/logits_process.py&quot;, line 92, in __call__\n    scores = processor(input_ids, scores)\n  File &quot;/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/transformers/generation/logits_process.py&quot;, line 302, in __call__\n    indices_to_remove = scores &lt; torch.topk(scores, top_k)[0][..., -1, None]\nRuntimeError: &quot;topk_cpu&quot; not implemented for 'Half'\n</code></pre>\n<hr />\n<h2>Bounty Section: Small GPT2 code example</h2>\n<p>Yes I agree that pad is assigned to eos. Eos is still eos. But during fine-tuning now the weights wrt to eos are unchanged. This might be an issue since the probability of eos has not shifted to the fine-tuning regime. One possibility is that eos is outputed with less chance. Yes we can still halt production when we see eos but we've not shifted the probability to output eos according to our fine-tuning distribution -- but all other tokens have changed distribution. I think this could be an issue because it's not like the old probability of eos is conserved since all tokens probs have changed except eos + even if the old eos prob was conserved, it's wrt wrong distribution (not the fine tuning one).</p>\n<p>e.g.,</p>\n<pre><code>if tokenizer.pad_token_id is None:\n    tokenizer.pad_token = tokenizer.eos_token\n ...\nraw_text_batch='a'\ntokenize_batch={'input_ids': tensor([[   64, 50256, 50256, 50256, 50256]]), 'attention_mask': tensor([[1, 0, 0, 0, 0]])}\n</code></pre>\n<p>but it would have been better to have</p>\n<pre><code>tokenize_batch={'input_ids': tensor([[   64, 50256, 50256, 50256, 50256]]), 'attention_mask': tensor([[1, 1, 0, 0, 0]])}\n</code></pre>\n<p>code</p>\n<pre><code>def test_eos_pad():\n    from datasets import load_dataset\n    import torch\n    from transformers import GPT2Tokenizer, GPT2LMHeadModel\n\n    raw_text_batch = 'a'\n\n    tokenizer = GPT2Tokenizer.from_pretrained(&quot;gpt2&quot;)\n    # print(f'{tokenizer.eos_token=}')\n    # print(f'{tokenizer.eos_token_id=}')\n    # print(f'{tokenizer.pad_token=}')\n    # print(f'{tokenizer.pad_token_id=}')\n\n    # print(f'{raw_text_batch=}')\n    # tokenize_batch = tokenizer(raw_text_batch, padding=&quot;max_length&quot;, max_length=5, truncation=True, return_tensors=&quot;pt&quot;)\n    # print(f'{tokenize_batch=}')\n\n    if tokenizer.pad_token_id is None:\n        tokenizer.pad_token = tokenizer.eos_token\n    probe_network = GPT2LMHeadModel.from_pretrained(&quot;gpt2&quot;)\n    device = torch.device(f&quot;cuda:{0}&quot; if torch.cuda.is_available() else &quot;cpu&quot;)\n    probe_network = probe_network.to(device)\n\n    print(f'{tokenizer.eos_token=}')\n    print(f'{tokenizer.eos_token_id=}')\n    print(f'{tokenizer.pad_token=}')\n    print(f'{tokenizer.pad_token_id=}')\n\n    print(f'{raw_text_batch=}')\n    tokenize_batch = tokenizer(raw_text_batch, padding=&quot;max_length&quot;, max_length=5, truncation=True, return_tensors=&quot;pt&quot;)\n    print(f'{tokenize_batch=}')\n    print('Done')\n</code></pre>\n<hr />\n<p>cross:</p>\n<ul>\n<li>hf discuss forum: <a href=\"https://discuss.huggingface.co/t/why-does-the-falcon-qlora-tutorial-code-use-eos-token-as-pad-token/45954\" rel=\"nofollow noreferrer\">https://discuss.huggingface.co/t/why-does-the-falcon-qlora-tutorial-code-use-eos-token-as-pad-token/45954</a></li>\n<li>pytorch forum discuss: <a href=\"https://discuss.pytorch.org/t/how-does-one-set-the-pad-token-correctly-not-to-eos-during-fine-tuning-to-avoid-model-not-predicting-eos/213619\" rel=\"nofollow noreferrer\">https://discuss.pytorch.org/t/how-does-one-set-the-pad-token-correctly-not-to-eos-during-fine-tuning-to-avoid-model-not-predicting-eos/213619</a></li>\n<li><a href=\"https://discord.com/channels/879548962464493619/1126681170957045770/1126681170957045770\" rel=\"nofollow noreferrer\">https://discord.com/channels/879548962464493619/1126681170957045770/1126681170957045770</a></li>\n<li>context peft pacman100 code: <a href=\"https://gist.github.com/pacman100/1731b41f7a90a87b457e8c5415ff1c14\" rel=\"nofollow noreferrer\">https://gist.github.com/pacman100/1731b41f7a90a87b457e8c5415ff1c14</a></li>\n<li>twitter tweet of this: <a href=\"https://twitter.com/BrandoHablando/status/1693676898013061337?s=20\" rel=\"nofollow noreferrer\">https://twitter.com/BrandoHablando/status/1693676898013061337?s=20</a></li>\n</ul>\n",
        "answer": "<p>I feel the simplest answer during training is:</p>\n<blockquote>\n<p>putting the eos string token explicitly then letting the model pad, even if eos == pad, makes sure that we do train on the intended tokens (including eos).</p>\n</blockquote>\n<p>eg see the output:</p>\n<pre class=\"lang-bash prettyprint-override\"><code>--&gt; WARNING: NOTE THIS MODEL mistralai/Mistral-7B-v0.1 DIDNT HAVE A PAD TOKEN, WE ASSINGED IT TO EOS.\nBOS/EOS/PAD --&gt; BOS: 1 EOS: 2 PAD: 2\nBOS/EOS/PAD --&gt; BOS: &lt;s&gt; EOS: &lt;/s&gt; PAD: &lt;/s&gt;\nCurrent raw rext (about to be tokenized): ['Hello world.&lt;/s&gt;', 'The dog is brown.&lt;/s&gt;']\nTFA Encoded text (explict args!): tokenizer(text_examples, add_special_tokens=False)={'input_ids': [[22557, 1526, 28723, 2], [415, 3914, 349, 9060, 28723, 2]], 'attention_mask': [[1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}\nTraining (batch) Encoded text args (explict args!): tokenizer(text_examples, padding='max_length', truncation=True, max_length=13, return_tensors='pt', padding_side='right')={'input_ids': tensor([[    1, 22557,  1526, 28723,     2,     2,     2,     2,     2,     2,\n             2,     2,     2],\n        [    1,   415,  3914,   349,  9060, 28723,     2,     2,     2,     2,\n             2,     2,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]])}\n</code></pre>\n<p>Code</p>\n<pre class=\"lang-py prettyprint-override\"><code># af_tokenizers.py\n\nimport os\nimport time\nimport torch\nfrom typing import Optional\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    PreTrainedModel\n)\nfrom datasets import load_dataset\n\ndef seed_everything(seed: int = 42):\n    &quot;&quot;&quot;\n    Seed Python, NumPy, and PyTorch for reproducibility.\n    &quot;&quot;&quot;\n    import random\n    import numpy as np\n    from transformers import set_seed as hf_set_seed\n\n    print(f&quot;Setting random seed = {seed}&quot;)\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n    if torch.cuda.is_available():\n        hf_set_seed(seed)\n    else:\n        print(&quot;Warning: Transformers is only fully deterministic on GPU&quot;)\n\n\ndef main():\n    os.environ['CUDA_VISIBLE_DEVICES'] = '4'  # choose GPU\n    seed_everything()\n\n    # 1) Our model list (including all desired models, even if some remain commented)\n    model_token_configs = [\n        {\n            &quot;name&quot;: &quot;internlm2-math-plus-1_8b&quot;,\n            &quot;repo&quot;: &quot;internlm/internlm2-math-plus-1_8b&quot;,\n        },\n        {\n            &quot;name&quot;: &quot;google/gemma-2-2b&quot;,\n            &quot;repo&quot;: &quot;google/gemma-2-2b&quot;,\n        },\n        {\n            &quot;name&quot;: &quot;Mistral-7B-v0.1&quot;,\n            &quot;repo&quot;: &quot;mistralai/Mistral-7B-v0.1&quot;,\n        },\n    ]\n\n    device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;\n\n    print('\\nLoop starting to see model tokenizations...')\n    for config in model_token_configs:\n        model_name = config[&quot;name&quot;]\n        repo = config[&quot;repo&quot;]\n        print(f'Model repo: {repo}')\n\n        model = AutoModelForCausalLM.from_pretrained(repo, trust_remote_code=True).to(device)\n\n        # putting the eos string token explicitly then letting the model pad, even if eos == pad, makes sure that we do train on the intended tokens (including eos).\n        text_examples = [\n            &quot;Hello world.&lt;/s&gt;&quot;,\n            &quot;The dog is brown.&lt;/s&gt;&quot;\n        ]\n        # 2) Get Tokenizer &amp; BOS/EOS/PAD info\n        # I often have this line and we should be able to see quickly what the tokenizers outputs for the models in question\n        tokenizer = AutoTokenizer.from_pretrained(repo, trust_remote_code=True)\n        if tokenizer.pad_token is None:\n            tokenizer.pad_token = tokenizer.eos_token if tokenizer.pad_token is None else tokenizer.pad_token\n            print(f'--&gt; WARNING: NOTE THIS MODEL {repo} DIDNT HAVE A PAD TOKEN, WE ASSINGED IT TO EOS.')\n        print(f'BOS/EOS/PAD --&gt; BOS: {tokenizer.bos_token_id} EOS: {tokenizer.eos_token_id} PAD: {tokenizer.pad_token_id}')\n        print(f'BOS/EOS/PAD --&gt; BOS: {tokenizer.bos_token} EOS: {tokenizer.eos_token} PAD: {tokenizer.pad_token}')\n\n        # 3) Tokenize according to common ways I do it for thi project\n        print(f'Current raw rext (about to be tokenized): {text_examples}')\n\n        # TFA encodings\n        print(f'TFA Encoded text (explict args!): {tokenizer(text_examples, add_special_tokens=False)=}')\n        \n        # Train encodings\n        # Note: add_eos_token is not always present, so we won't be using: tok = AutoTokenizer.from_pretrained(pretrained_model_name_or_path, padding_side=&quot;right&quot;, trust_remote_code=True, add_eos_token=True)\n        print(f&quot;Training (batch) Encoded text args (explict args!): {tokenizer(text_examples, padding='max_length', truncation=True, max_length=13, return_tensors='pt', padding_side='right')=}&quot;)\n        print()\n\n\nif __name__ == &quot;__main__&quot;:\n    main()\n</code></pre>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 76633368,
            "link": "https://stackoverflow.com/questions/76633368/how-does-one-set-the-pad-token-correctly-not-to-eos-during-fine-tuning-to-avoi"
        }
    },
    {
        "question": "ModuleNotFoundError: No module named &#39;llama_index.postprocessor&#39;\n<p>I tried to import CohereRerank module from llama_index. But got a <code>ModuleNotFoundError: No module named 'llama_index.postprocessor'</code>.</p>\n",
        "answer": "<p>Please <code>pip install llama-index-postprocessor-colbert-rerank</code> to install Colbert Rerank package.</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 78333326,
            "link": "https://stackoverflow.com/questions/78333326/modulenotfounderror-no-module-named-llama-index-postprocessor"
        }
    },
    {
        "question": "Why does llama-index still require an OpenAI key when using Hugging Face local embedding model?\n<p>I am creating a very simple question and answer app based on documents using llama-index. Previously, I had it working with OpenAI. Now I want to try using no external APIs so I'm trying the Hugging Face example <a href=\"https://gpt-index.readthedocs.io/en/latest/core_modules/model_modules/llms/usage_custom.html#example-using-a-huggingface-llm\" rel=\"noreferrer\">in this link</a>.</p>\n<p>It says in the example in the link: &quot;Note that for a completely private experience, also setup a local embedding model (example here).&quot; I'm assuming the example given below is the example being referred to. So, naturally, I'm trying to copy the example (<a href=\"https://gpt-index.readthedocs.io/en/latest/examples/customization/llms/SimpleIndexDemo-Huggingface_stablelm.html\" rel=\"noreferrer\">fuller example here</a>).</p>\n<p>Here is my code:</p>\n<pre class=\"lang-py prettyprint-override\"><code>from pathlib import Path\nimport gradio as gr\nimport sys\nimport logging\nimport os\n\nfrom llama_index.llms import HuggingFaceLLM\nfrom llama_index.prompts.prompts import SimpleInputPrompt\n\nlogging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n\n\nfrom llama_index import SimpleDirectoryReader, VectorStoreIndex, ServiceContext, load_index_from_storage, StorageContext\n\nstorage_path = &quot;storage/&quot;\n\ndocs_path=&quot;docs&quot;\n\ndef construct_index(directory_path):\n    max_input_size = 4096\n    num_outputs = 512\n    #max_chunk_overlap = 20\n    chunk_overlap_ratio = 0.1\n    chunk_size_limit = 600\n\n    #prompt_helper = PromptHelper(max_input_size, num_outputs, chunk_overlap_ratio, chunk_size_limit=chunk_size_limit)\n\n    system_prompt = &quot;&quot;&quot;&lt;|SYSTEM|&gt;# StableLM Tuned (Alpha version)\n    - StableLM is a helpful and harmless open-source AI language model developed by StabilityAI.\n    - StableLM is excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n    - StableLM is more than just an information source, StableLM is also able to write poetry, short stories, and make jokes.\n    - StableLM will refuse to participate in anything that could harm a human.\n    &quot;&quot;&quot;\n\n    # This will wrap the default prompts that are internal to llama-index\n    query_wrapper_prompt = SimpleInputPrompt(&quot;&lt;|USER|&gt;{query_str}&lt;|ASSISTANT|&gt;&quot;)\n\n\n    llm = HuggingFaceLLM(\n        context_window=4096,\n        max_new_tokens=256,\n        generate_kwargs={&quot;temperature&quot;: 0.7, &quot;do_sample&quot;: False},\n        system_prompt=system_prompt,\n        query_wrapper_prompt=query_wrapper_prompt,\n        tokenizer_name=&quot;StabilityAI/stablelm-tuned-alpha-3b&quot;,\n        model_name=&quot;StabilityAI/stablelm-tuned-alpha-3b&quot;,\n        device_map=&quot;auto&quot;,\n        stopping_ids=[50278, 50279, 50277, 1, 0],\n        tokenizer_kwargs={&quot;max_length&quot;: 4096},\n        # uncomment this if using CUDA to reduce memory usage\n        # model_kwargs={&quot;torch_dtype&quot;: torch.float16}\n    )\n    #llm=ChatOpenAI(temperature=0.7, model_name=&quot;gpt-3.5-turbo&quot;, max_tokens=num_outputs)\n    #llm_predictor = LLMPredictor(llm=llm)\n    service_context = ServiceContext.from_defaults(chunk_size=1024, llm=llm)\n\n    documents = SimpleDirectoryReader(directory_path).load_data()\n\n    index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n    #index = VectorStoreIndex(documents, llm_predictor=llm_predictor, prompt_helper=prompt_helper)\n\n    index.storage_context.persist(persist_dir=storage_path)\n\n    return index\n\ndef chatbot(input_text):\n    index = load_index_from_storage(StorageContext.from_defaults(persist_dir=storage_path))\n    #index = GPTVectorStoreIndex.load_from_disk('index.json')\n    #query_engine = index.as_query_engine(response_synthesizer=response_synthesizer);\n    query_engine = index.as_query_engine(streaming=True)\n\n    response = query_engine.query(input_text)\n\n    print(response.source_nodes)\n\n    relevant_files=[]\n\n    for node_with_score in response.source_nodes:\n        print(node_with_score)\n        print(node_with_score.node)\n        print(node_with_score.node.metadata)\n        print(node_with_score.node.metadata['file_name'])\n\n        file = node_with_score.node.metadata['file_name']\n        print( file )\n\n        # Resolve the full file path for the downloading\n        full_file_path = Path( docs_path, file ).resolve()\n\n        # See if it's already in the array\n        if full_file_path not in relevant_files:\n            relevant_files.append( full_file_path ) # Add it\n\n    print( relevant_files )\n\n    return response.get_response(), relevant_files\n\niface = gr.Interface(fn=chatbot,\n                     inputs=gr.components.Textbox(lines=7, label=&quot;Enter your text&quot;),\n                     outputs=[\n                        gr.components.Textbox(label=&quot;Response&quot;), \n                        gr.components.File(label=&quot;Relevant Files&quot;)\n                        ],\n                     title=&quot;Custom-trained AI Chatbot&quot;,\n                     allow_flagging=&quot;never&quot;)\n\nindex = construct_index(docs_path)\niface.launch(share=False)\n\n</code></pre>\n<p>Regardless, the code errors out saying:</p>\n<pre><code>ValueError: No API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n</code></pre>\n<p>Am I not understanding how to set up a local model?</p>\n",
        "answer": "<p>The error is here:</p>\n<pre><code>        tokenizer_name=&quot;StabilityAI/stablelm-tuned-alpha-3b&quot;,\n        model_name=&quot;StabilityAI/stablelm-tuned-alpha-3b&quot;,\n</code></pre>\n<p>the HF LLM you're trying to use is hosted on the HuggingFace Model Hub, which requires an API key for access-- tokenizer_name + model_name parameters are  &quot;StabilityAI/stablelm-tuned-alpha-3b&quot;, which is a model hosted on the  Hub. you need to download the model files and then provide the path to the local model files as the model_name parameter.</p>\n<p>You can download a model from the HuggingFace Model Hub using the transformers-cli command-line tool:</p>\n<pre><code>transformers-cli download --model StabilityAI/stablelm-tuned-alpha-3b\n</code></pre>\n<p>and add that to the code like this:</p>\n<pre><code>llm = HuggingFaceLLM(\n    # blah blah blah\n    model_name=&quot;FILE_LOCATION/StabilityAI/stablelm-tuned-alpha-3b&quot;,\n    # blah blah blah\n)\n</code></pre>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 76771761,
            "link": "https://stackoverflow.com/questions/76771761/why-does-llama-index-still-require-an-openai-key-when-using-hugging-face-local-e"
        }
    },
    {
        "question": "Error Loading &quot;sentence-transformers/all-MiniLM-L6-v2&quot;\n<p>I have built a document question-answering system using llama-2, but while downloading the embedding model, I am getting an OSError.</p>\n<p>OSError: We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like sentence-transformers/all-MiniLM-L6-v2 is not the path to a directory containing a file named config.json. Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.</p>\n",
        "answer": "<p>In my case it was solved by disabling the authentication token:</p>\n<pre class=\"lang-py prettyprint-override\"><code>SentenceTransformer(&quot;sentence-transformers/all-MiniLM-L6-v2&quot;, token=False)\n</code></pre>\n<p>See <a href=\"https://github.com/UKPLab/sentence-transformers/issues/3212\" rel=\"nofollow noreferrer\">https://github.com/UKPLab/sentence-transformers/issues/3212</a>\nMaybe your issue is different though</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 78446414,
            "link": "https://stackoverflow.com/questions/78446414/error-loading-sentence-transformers-all-minilm-l6-v2"
        }
    },
    {
        "question": "SSLError: HTTPSConnectionPool(host=&#39;huggingface.co&#39;, port=443): Max retries exceeded with url: /dslim/bert-base-NER/resolve/main/tokenizer_config.json\n<p>I am facing below issue while loading the pretrained BERT model from HuggingFace due to SSL certificate error.</p>\n<h2>Error:</h2>\n<blockquote>\n<p>SSLError: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /dslim/bert-base-NER/resolve/main/tokenizer_config.json (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self signed certificate in certificate chain (_ssl.c:1108)')))</p>\n</blockquote>\n<h2>The line that is causing the issue is:</h2>\n<pre><code>tokenizer = AutoTokenizer.from_pretrained(&quot;dslim/bert-base-NER&quot;)\n</code></pre>\n<h2>Source code:</h2>\n<pre><code>from transformers import AutoTokenizer, AutoModelForTokenClassification\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(&quot;dslim/bert-base-NER&quot;)\nmodel = AutoModelForTokenClassification.from_pretrained(&quot;dslim/bert-base-NER&quot;)\n</code></pre>\n<p>I am expecting to download pre-trained models while running the code in jupyter lab on Windows.</p>\n",
        "answer": "<p>By setting the below environment variables it worked:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import os\nos.environ['CURL_CA_BUNDLE'] = ''\nos.environ['REQUESTS_CA_BUNDLE'] = ''\n</code></pre>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 75110981,
            "link": "https://stackoverflow.com/questions/75110981/sslerror-httpsconnectionpoolhost-huggingface-co-port-443-max-retries-exce"
        }
    },
    {
        "question": "Convert PyTorch Model to Hugging Face model\n<p>I have looked at a lot resources but I still have issues trying to convert a PyTorch model to a hugging face model format. I ultimately want to be able to use inference API with my custom model.</p>\n<p>I have a &quot;model.pt&quot; file which I got from fine-tuning the Facebook Musicgen medium model (<a href=\"https://github.com/chavinlo/musicgen_trainer\" rel=\"nofollow noreferrer\">The Git repo I used to train / Fine tune the model is here</a>). I want to upload this to the hugging face hub so i can use this with inference API. How can I convert the <code>.pt</code> model to files/model that can be used on hugging face hub? I tried looking at other posts but there is no clear answer, or it is poorly explained.</p>\n<p>Any help / guidance would be greatly appreciated 🙏</p>\n<p>This is the code I have right now that is not working:</p>\n<pre><code>import torch\nfrom transformers import MusicgenConfig, MusicgenModel\nfrom audiocraft.models import musicgen\nimport os\n\nos.mkdir('models')\n\nstate_dict = musicgen.MusicGen.get_pretrained('facebook/musicgen-medium', device='cuda').lm.load_state_dict(torch.load('NEW_MODEL.pt'))\n\nconfig = MusicgenConfig.from_pretrained('facebook/musicgen-medium')\nmodel = MusicgenModel(config)\nmodel.load_state_dict(state_dict)\n\nmodel.save_pretrained('/models')\n\nloaded_model = MusicgenModel.from_pretrained('/models')\n</code></pre>\n",
        "answer": "<p>In case your model is a (custom) PyTorch model, you can leverage the <code>PyTorchModelHubMixin</code> class available in the <code>huggingface_hub</code> Python library. It is a minimal class which adds <code>from_pretrained</code> and <code>push_to_hub</code> capabilities to any <code>nn.Module</code>, along with download metrics.</p>\n<pre><code>import torch\nimport torch.nn as nn\nfrom huggingface_hub import PyTorchModelHubMixin\n\n\nclass MyModel(nn.Module, PyTorchModelHubMixin):\n    def __init__(self, config: dict):\n        super().__init__()\n        self.param = nn.Parameter(torch.rand(config[&quot;num_channels&quot;], config[&quot;hidden_size&quot;]))\n        self.linear = nn.Linear(config[&quot;hidden_size&quot;], config[&quot;num_classes&quot;])\n\n    def forward(self, x):\n        return self.linear(x + self.param)\n\n# create model\nconfig = {&quot;num_channels&quot;: 3, &quot;hidden_size&quot;: 32, &quot;num_classes&quot;: 10}\nmodel = MyModel(config=config)\n\n# save locally\nmodel.save_pretrained(&quot;my-awesome-model&quot;, config=config)\n\n# push to the hub\nmodel.push_to_hub(&quot;my-awesome-model&quot;, config=config)\n\n# reload\nmodel = MyModel.from_pretrained(&quot;username/my-awesome-model&quot;)\n</code></pre>\n<p>Here is a link to the huggingface docs, explaining how to push pytorch model to the huggingface hub.</p>\n<p><a href=\"https://huggingface.co/docs/hub/en/models-uploading#upload-a-pytorch-model-using-huggingfacehub\" rel=\"nofollow noreferrer\">https://huggingface.co/docs/hub/en/models-uploading#upload-a-pytorch-model-using-huggingfacehub</a></p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 78105436,
            "link": "https://stackoverflow.com/questions/78105436/convert-pytorch-model-to-hugging-face-model"
        }
    },
    {
        "question": "Huggingface Push_to_hub Permission denied for certain names\n<p>So I am retraining a whisper model and have it saved in a certain path (<code>model_name_or_path</code>).\nWhen I want to push it to the hub I <strong>sometimes</strong> get a <code>[Errno 13] Permission denied</code>\nI can push the adapter but not the model.</p>\n<p>I would like to push to <code>trained_model_repo ='4STest/DataSize06dentalAudioModel-medium'</code>\nIf I change the repo to <code>''4STest/lol''</code> there is no problem...\nCan anyone explain this?</p>\n<p>My code looks as follows:</p>\n<pre><code>model = WhisperForConditionalGeneration.from_pretrained(model_name_or_path,load_in_8bit=False, device_map=&quot;auto&quot;)\n\nmodel = PeftModel.from_pretrained(\nmodel,\nadapter_to_push,\n)\n#push adapter\nmodel.push_to_hub(trained_adapter_repo,private=True)\n\n#save model\nmodel = model.merge_and_unload()\nmodel.save_pretrained(trained_model_repo)\nprocessor.save_pretrained(trained_model_repo)\n\n#push model\nmodel.push_to_hub(trained_model_repo,safe_serialization=True,private=True)\nprocessor.push_to_hub(trained_model_repo,private=True)\n</code></pre>\n<p>Variables:</p>\n<pre><code>model_name_or_path=&quot;openai/whisper-medium&quot;\nadapter_to_push=&quot;./4STest/DA_model_partial_DS02medium/checkpoint-9&quot;\ntrained_adapter_repo=&quot;4STest/DataSize02AudioAdapter-medium&quot;\ntrained_model_repo=&quot;4STest/DataSize02AudioModel-medium&quot;\n</code></pre>\n<p>Alternative <code>trained_model_repo=&quot;4STest/lol&quot;</code> works just fine.</p>\n<p>Any help is appreciated</p>\n",
        "answer": "<p>So I think I figured it out.</p>\n<p>There is not allowed to be  directory in your working directory that has the same name as your organization.</p>\n<p>So if your organization is <code>YourOrg</code> then you can not save your model to a directory with the same name <code>model.save_pretrained(&quot;YourOrg/someModel&quot;)</code></p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 79364745,
            "link": "https://stackoverflow.com/questions/79364745/huggingface-push-to-hub-permission-denied-for-certain-names"
        }
    },
    {
        "question": "SSLError: HTTPSConnectionPool(host=&#39;huggingface.co&#39;, port=443): Max retries exceeded with url: /dslim/bert-base-NER/resolve/main/tokenizer_config.json\n<p>I am facing below issue while loading the pretrained BERT model from HuggingFace due to SSL certificate error.</p>\n<h2>Error:</h2>\n<blockquote>\n<p>SSLError: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /dslim/bert-base-NER/resolve/main/tokenizer_config.json (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self signed certificate in certificate chain (_ssl.c:1108)')))</p>\n</blockquote>\n<h2>The line that is causing the issue is:</h2>\n<pre><code>tokenizer = AutoTokenizer.from_pretrained(&quot;dslim/bert-base-NER&quot;)\n</code></pre>\n<h2>Source code:</h2>\n<pre><code>from transformers import AutoTokenizer, AutoModelForTokenClassification\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(&quot;dslim/bert-base-NER&quot;)\nmodel = AutoModelForTokenClassification.from_pretrained(&quot;dslim/bert-base-NER&quot;)\n</code></pre>\n<p>I am expecting to download pre-trained models while running the code in jupyter lab on Windows.</p>\n",
        "answer": "<p>A working solution to this is the following:</p>\n<p>1)</p>\n<pre><code> pip install requests==2.27.1   \n</code></pre>\n<p>Please note that both prerequisites need to take place, just running the following code will not work unless you have that specific version of requests (I had to downgrade in my case from <code>2.29.0</code> to <code>2.27.1</code>.</p>\n<p>2)</p>\n<pre><code>import os\nos.environ['CURL_CA_BUNDLE'] = ''\n</code></pre>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 75110981,
            "link": "https://stackoverflow.com/questions/75110981/sslerror-httpsconnectionpoolhost-huggingface-co-port-443-max-retries-exce"
        }
    },
    {
        "question": "What is the recommended number of threads for PyTorch relative to available CPU cores?\n<p>First I want to say that I don't have much experience with pytorch, ML, NLP and other related topics, so I may confuse some concepts. Sorry.</p>\n<p>I downloaded few models from Hugging Face, organized them in one Python script and started to perform benchmark to get overview of performance. During benchmark I monitored CPU usage and saw that only 50% of CPU was used. I have 8 vCPU, but only 4 of them are loaded at 100% at the same time. The load is jumping, i.e. there may be cores 1, 3, 5, 7 that are loaded at 100%, then cores 2, 4, 6, 8 that are loaded at 100%. But in total CPU load never raises above 50%, it also never goes below 50%. This 50% load is constant.</p>\n<p>After quick googling I found <a href=\"https://pytorch.org/docs/stable/torch.html#parallelism\" rel=\"nofollow noreferrer\">parallelism doc</a>. I called <code>get_num_threads()</code> and <code>get_num_interop_threads()</code> and output was <code>4</code> for both calls. Only 50% of available CPU cores which kind of explains why CPU load was at 50%.</p>\n<p>Then I called <code>set_num_threads(8)</code> and <code>set_num_interop_threads(8)</code>, and then performed benchmark. CPU usage was at constant 100%. In general performance was a bit faster, but some models started to work a bit slowly than at 50% of CPU.</p>\n<p>So I wonder why pytorch by default uses only half of CPU? It is optimal and recommended way? Should I manually call <code>set_num_threads()</code> and <code>set_num_interop_threads()</code> with all available CPU cores if I want to achieve best performance?</p>\n<p>Edit.</p>\n<p>I made an additional benchmarks:</p>\n<ul>\n<li>one pytorch process with 50% of vCPU is a bit faster than one pytorch process with 100% of vCPU. Earlier it was vice versa, so I think it depends on models that are being used.</li>\n<li>two pytorch concurrent processes with 50% of vCPU will handle more inputs than one pytorch process with 50% of vCPU, but it is not 2x increase, it is ~1.2x increase. Process time of one input is much slower than with one pytorch process.</li>\n<li>two pytorch concurrent processes with 100% of vCPU can't complete even one input. I guess CPU is constantly switching between these processes.</li>\n</ul>\n<p>So thank you to Phoenix's answer, I think it is completely reasonable to use pytorch default settings which sets number of threads according to number of physical (not virtual) cores.</p>\n<p>Edit.</p>\n<p>pytorch documentation about this - <a href=\"https://pytorch.org/docs/stable/notes/cpu_threading_torchscript_inference.html\" rel=\"nofollow noreferrer\">https://pytorch.org/docs/stable/notes/cpu_threading_torchscript_inference.html</a></p>\n",
        "answer": "<p>PyTorch typically uses the number of physical CPU cores as the default number of threads. This means:\n<code>torch.get_num_threads()</code> and <code>torch.get_num_interop_threads()</code> typically return the number of physical CPU cores.</p>\n<ul>\n<li>Use the default behavior unless you have a specific reason to change it .</li>\n<li>When changing the number of threads, use <code>torch.set_num_threads()</code> and <code>torch.set_num_interop_threads()</code>.</li>\n<li>Avoid oversubscription by not using more threads than available CPU cores.</li>\n</ul>\n<p>For example:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import torch\n\n# Get current number of threads\nnum_threads = torch.get_num_threads()\nprint(f&quot;Current number of threads: {num_threads}&quot;)\n\n# Set custom number of threads (e.g., equal to physical cores)\ntorch.set_num_threads(num_threads)\ntorch.set_num_interop_threads(num_threads)\n\n# Check new settings\nprint(f&quot;New number of threads: {torch.get_num_threads()}&quot;)\nprint(f&quot;New number of inter-op threads: {torch.get_num_interop_threads()}&quot;)\n</code></pre>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 76084214,
            "link": "https://stackoverflow.com/questions/76084214/what-is-the-recommended-number-of-threads-for-pytorch-relative-to-available-cpu"
        }
    },
    {
        "question": "Getting Cuda out of memory when importing microsoft/Orca-2-13b from hugging faces\n<p>I am using Ubuntu 24.04.1 on an AWS EC2 instance g5.8xlarge.</p>\n<p>I am receiving the following error message:</p>\n<pre><code>OutOfMemoryError: Allocation on device \n</code></pre>\n<p><strong>Code:</strong></p>\n<pre><code>import os\nos.environ[&quot;PYTORCH_CUDA_ALLOC_CONF&quot;] = &quot;backend:cudaMallocAsync&quot;\nimport torch\ntorch.cuda.empty_cache()\nimport transformers\n    \nif torch.cuda.is_available():\n    torch.set_default_device(&quot;cuda&quot;)\n    \ndevice = torch.device(&quot;cuda&quot;)\n    \nmodel = transformers.AutoModelForCausalLM.from_pretrained(&quot;microsoft/Orca-2-13b&quot;, device_map=device)\n</code></pre>\n<p><strong>Full error:</strong></p>\n<pre><code>/home/ubuntu/anaconda3/envs/ai/lib/python3.12/site-packages/torch/cuda/__init__.py:734: UserWarning: Can't initialize NVML\n  warnings.warn(&quot;Can't initialize NVML&quot;)\n\nLoading checkpoint shards:  33%\n 2/6 [00:04&lt;00:06,  1.72s/it]\n\n/home/ubuntu/anaconda3/envs/ai/lib/python3.12/site-packages/torch/cuda/__init__.py:734: UserWarning: Can't initialize NVML\n  warnings.warn(&quot;Can't initialize NVML&quot;)\n\n---------------------------------------------------------------------------\nOutOfMemoryError                          Traceback (most recent call last)\nCell In[5], line 6\n      2     torch.set_default_device(&quot;cuda&quot;)\n      4 device = torch.device(&quot;cuda&quot;)\n----&gt; 6 model = transformers.AutoModelForCausalLM.from_pretrained(&quot;microsoft/Orca-2-13b&quot;, device_map=device)\n      8 # https://github.com/huggingface/transformers/issues/27132\n      9 # please use the slow tokenizer since fast and slow tokenizer produces different tokens\n     10 tokenizer = transformers.AutoTokenizer.from_pretrained(\n     11         &quot;microsoft/Orca-2-13b&quot;,\n     12         use_fast=True,\n     13     )\n\nFile ~/anaconda3/envs/ai/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:564, in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)\n    562 elif type(config) in cls._model_mapping.keys():\n    563     model_class = _get_model_class(config, cls._model_mapping)\n--&gt; 564     return model_class.from_pretrained(\n    565         pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs\n    566     )\n    567 raise ValueError(\n    568     f&quot;Unrecognized configuration class {config.__class__} for this kind of AutoModel: {cls.__name__}.\\n&quot;\n    569     f&quot;Model type should be one of {', '.join(c.__name__ for c in cls._model_mapping.keys())}.&quot;\n    570 )\n\nFile ~/anaconda3/envs/ai/lib/python3.12/site-packages/transformers/modeling_utils.py:262, in restore_default_torch_dtype.&lt;locals&gt;._wrapper(*args, **kwargs)\n    260 old_dtype = torch.get_default_dtype()\n    261 try:\n--&gt; 262     return func(*args, **kwargs)\n    263 finally:\n    264     torch.set_default_dtype(old_dtype)\n\nFile ~/anaconda3/envs/ai/lib/python3.12/site-packages/transformers/modeling_utils.py:4319, in PreTrainedModel.from_pretrained(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\n   4309     if dtype_orig is not None:\n   4310         torch.set_default_dtype(dtype_orig)\n   4312     (\n   4313         model,\n   4314         missing_keys,\n   4315         unexpected_keys,\n   4316         mismatched_keys,\n   4317         offload_index,\n   4318         error_msgs,\n-&gt; 4319     ) = cls._load_pretrained_model(\n   4320         model,\n   4321         state_dict,\n   4322         loaded_state_dict_keys,  # XXX: rename?\n   4323         resolved_archive_file,\n   4324         pretrained_model_name_or_path,\n   4325         ignore_mismatched_sizes=ignore_mismatched_sizes,\n   4326         sharded_metadata=sharded_metadata,\n   4327         _fast_init=_fast_init,\n   4328         low_cpu_mem_usage=low_cpu_mem_usage,\n   4329         device_map=device_map,\n   4330         offload_folder=offload_folder,\n   4331         offload_state_dict=offload_state_dict,\n   4332         dtype=torch_dtype,\n   4333         hf_quantizer=hf_quantizer,\n   4334         keep_in_fp32_modules=keep_in_fp32_modules,\n   4335         gguf_path=gguf_path,\n   4336         weights_only=weights_only,\n   4337     )\n   4339 # make sure token embedding weights are still tied if needed\n   4340 model.tie_weights()\n\nFile ~/anaconda3/envs/ai/lib/python3.12/site-packages/transformers/modeling_utils.py:4897, in PreTrainedModel._load_pretrained_model(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules, gguf_path, weights_only)\n   4895     else:\n   4896         fixed_state_dict = cls._fix_state_dict_keys_on_load(state_dict)\n-&gt; 4897         new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(\n   4898             model_to_load,\n   4899             fixed_state_dict,\n   4900             start_prefix,\n   4901             expected_keys,\n   4902             device_map=device_map,\n   4903             offload_folder=offload_folder,\n   4904             offload_index=offload_index,\n   4905             state_dict_folder=state_dict_folder,\n   4906             state_dict_index=state_dict_index,\n   4907             dtype=dtype,\n   4908             hf_quantizer=hf_quantizer,\n   4909             is_safetensors=is_safetensors,\n   4910             keep_in_fp32_modules=keep_in_fp32_modules,\n   4911             unexpected_keys=unexpected_keys,\n   4912         )\n   4913         error_msgs += new_error_msgs\n   4914 else:\n   4915     # Sharded checkpoint or whole but low_cpu_mem_usage==True\n\nFile ~/anaconda3/envs/ai/lib/python3.12/site-packages/transformers/modeling_utils.py:896, in _load_state_dict_into_meta_model(model, state_dict, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, hf_quantizer, is_safetensors, keep_in_fp32_modules, unexpected_keys, pretrained_model_name_or_path)\n    893         param_device = &quot;cpu&quot; if is_local_dist_rank_0() else &quot;meta&quot;\n    895     # For backward compatibility with older versions of `accelerate` and for non-quantized params\n--&gt; 896     set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)\n    897 else:\n    898     hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)\n\nFile ~/anaconda3/envs/ai/lib/python3.12/site-packages/accelerate/utils/modeling.py:330, in set_module_tensor_to_device(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map)\n    328             module._parameters[tensor_name] = param_cls(new_value, requires_grad=old_value.requires_grad)\n    329 elif isinstance(value, torch.Tensor):\n--&gt; 330     new_value = value.to(device)\n    331 else:\n    332     new_value = torch.tensor(value, device=device)\n\nFile ~/anaconda3/envs/ai/lib/python3.12/site-packages/torch/utils/_device.py:104, in DeviceContext.__torch_function__(self, func, types, args, kwargs)\n    102 if func in _device_constructors() and kwargs.get('device') is None:\n    103     kwargs['device'] = self.device\n--&gt; 104 return func(*args, **kwargs)\n\nOutOfMemoryError: Allocation on device \n</code></pre>\n",
        "answer": "<p>You can check out information on the specific model <a href=\"https://llm.extractum.io/model/microsoft%2FOrca-2-13b,61363fW16x75Cre5MxWM0A\" rel=\"nofollow noreferrer\">here</a>. But you can see it requires <code>52.1 GB</code> of VRAM (GPU memory).</p>\n<p>Based on <a href=\"https://instances.vantage.sh/aws/ec2/g5.8xlarge\" rel=\"nofollow noreferrer\">this table</a> we see that you have <code>24GB</code> of GPU memory. So it won't be able to fit. If you aren't able to get more GPU memory, you can look into quantized models.</p>\n<p>You can check out the models on <a href=\"https://huggingface.co/TheBloke/Orca-2-13B-GGUF\" rel=\"nofollow noreferrer\">huggingface</a> that have quantized versions, the GPU memory required, and the best use case.</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 79502752,
            "link": "https://stackoverflow.com/questions/79502752/getting-cuda-out-of-memory-when-importing-microsoft-orca-2-13b-from-hugging-face"
        }
    },
    {
        "question": "SSLError: HTTPSConnectionPool(host=&#39;huggingface.co&#39;, port=443): Max retries exceeded with url: /dslim/bert-base-NER/resolve/main/tokenizer_config.json\n<p>I am facing below issue while loading the pretrained BERT model from HuggingFace due to SSL certificate error.</p>\n<h2>Error:</h2>\n<blockquote>\n<p>SSLError: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /dslim/bert-base-NER/resolve/main/tokenizer_config.json (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self signed certificate in certificate chain (_ssl.c:1108)')))</p>\n</blockquote>\n<h2>The line that is causing the issue is:</h2>\n<pre><code>tokenizer = AutoTokenizer.from_pretrained(&quot;dslim/bert-base-NER&quot;)\n</code></pre>\n<h2>Source code:</h2>\n<pre><code>from transformers import AutoTokenizer, AutoModelForTokenClassification\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(&quot;dslim/bert-base-NER&quot;)\nmodel = AutoModelForTokenClassification.from_pretrained(&quot;dslim/bert-base-NER&quot;)\n</code></pre>\n<p>I am expecting to download pre-trained models while running the code in jupyter lab on Windows.</p>\n",
        "answer": "<p>Please check if you have the access token to download the hugging face model.</p>\n<p>Please refer the video\n<a href=\"https://www.youtube.com/watch?v=t-0s_2uZZU0\" rel=\"nofollow noreferrer\">https://www.youtube.com/watch?v=t-0s_2uZZU0</a>\nand check the information given between timestamps 1:44:18 - 1:45:34</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 75110981,
            "link": "https://stackoverflow.com/questions/75110981/sslerror-httpsconnectionpoolhost-huggingface-co-port-443-max-retries-exce"
        }
    },
    {
        "question": "How does one set the pad token correctly (not to eos) during fine-tuning to avoid model not predicting EOS?\n<h2>tldr; what I really want to know is what is the official way to set pad token for <strong>fine tuning</strong> it wasn't set during original training, so that it doesn't not learn to predict EOS.</h2>\n<p>colab: <a href=\"https://colab.research.google.com/drive/1poFdFYmkR_rDM5U5Z2WWjTepMQ8hvzNc?usp=sharing\" rel=\"nofollow noreferrer\">https://colab.research.google.com/drive/1poFdFYmkR_rDM5U5Z2WWjTepMQ8hvzNc?usp=sharing</a></p>\n<hr />\n<p>The HF falcon tutorial has the following line:</p>\n<pre><code>tokenizer.pad_token = tokenizer.eos_token\n</code></pre>\n<p>it looks strange to me. It make sense pad and eos are the same but then why even make a difference between them in the first place in general?</p>\n<p>Note its wrong to do pad = eos. This means during fine-tuning the model will never be trained to output eos (most likely) since eos is treated as pad token and no back propagated:</p>\n<pre><code>I just observed that when I set tokenizer.pad_token = tokenizer.eos_token during training, the model won't stop generating during inference, since it was trained to not output the eos token (per discussions above).\n</code></pre>\n<p>I saw this (here <a href=\"https://github.com/huggingface/transformers/issues/22794\" rel=\"nofollow noreferrer\">https://github.com/huggingface/transformers/issues/22794</a>):</p>\n<pre><code>tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n</code></pre>\n<p>But this assumes the model has a pad_token. I think an additional check has to be done that it does have an embedding for pad_token so that there are no run time errors (~type errors in the matrix extraction from the embedding &quot;table&quot;/matrix).</p>\n<p>But if one does that some care might be needed to initialize the new token so that it dominates the generation: <a href=\"https://nlp.stanford.edu/%7Ejohnhew/vocab-expansion.html\" rel=\"nofollow noreferrer\">https://nlp.stanford.edu/~johnhew/vocab-expansion.html</a></p>\n<hr />\n<p>code:</p>\n<pre class=\"lang-py prettyprint-override\"><code>def get_model_tokenizer_qlora_falcon7b(model_name: str = &quot;ybelkada/falcon-7b-sharded-bf16&quot;,\n                                       config: wand.Config,  # todo\n                                       lora_alpha=16,  # todo\n                                       lora_dropout=0.1,  # todo\n                                       lora_r=64,  # todo\n                                       bnb_4bit_compute_dtype=torch.float16,  # changed it from Guanaco hf\n                                       ) -&gt; tuple:\n    &quot;&quot;&quot;\n    Load the Falcon 7B model, quantize it in 4bit and attach LoRA adapters on it.\n\n    bf16 = 1S, 7Exp, 8Mantissa\n\n    Do:\n        pip install bitsandbytes\n    ref:\n        - https://colab.research.google.com/drive/1DOi8MFv4SWN9NImVornZ7t6BgmLoPQO-#scrollTo=AjB0WAqFSzlD\n    &quot;&quot;&quot;\n    from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoTokenizer\n\n    # model_id = &quot;tiiuae/falcon-7b&quot;\n    # model_name: str = &quot;ybelkada/falcon-7b-sharded-bf16&quot;\n\n    # - get bnb config for bit-4 base model (bnb lib for using 4bit qlora quantization techniques by tim dettmers)\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,  # load (usually huge) base model in 4 bits\n        bnb_4bit_quant_type=&quot;nf4&quot;,  # normal float 4 for the (usually huge) base model. introduces error but fixed by ft\n        # ref: https://gist.github.com/pacman100/1731b41f7a90a87b457e8c5415ff1c14\n        bnb_4bit_compute_dtype=bnb_4bit_compute_dtype,\n    )\n\n    # - get falcon 4bit model\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        quantization_config=bnb_config,\n        trust_remote_code=True  # allows to execute custom code you download from the uploaded model code you are using\n    )\n    model.config.use_cache = False  # todo: why? https://stackoverflow.com/questions/76633335/why-does-hugging-face-falcon-model-use-mode-config-use-cache-false-why-wouldn\n\n    # get falcon tockenizer\n    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)  # execs code downloaded from hf hub\n    tokenizer.pad_token = tokenizer.eos_token\n</code></pre>\n<hr />\n<h2>Modifying model gives issues</h2>\n<p>This still not works:</p>\n<pre><code> UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n</code></pre>\n<p>code:</p>\n<pre><code>&quot;&quot;&quot;\nsfttrainer (likely using peft) best practices:\nhttps://huggingface.co/docs/trl/main/en/sft_trainer#best-practices\n\nBest practices\n\nPay attention to the following best practices when training a model with that trainer:\n\n- SFTTrainer always pads by default the sequences to the max_seq_length argument of the SFTTrainer. If none is passed, the trainer will retrieve that value from the tokenizer. Some tokenizers do not provide default value, so there is a check to retrieve the minimum between 2048 and that value. Make sure to check it before training.\n- For training adapters in 8bit, you might need to tweak the arguments of the prepare_model_for_int8_training method from PEFT, hence we advise users to use prepare_in_int8_kwargs field, or create the PeftModel outside the SFTTrainer and pass it.\n- For a more memory-efficient training using adapters, you can load the base model in 8bit, for that simply add load_in_8bit argument when creating the SFTTrainer, or create a base model in 8bit outside the trainer and pass it.\n- If you create a model outside the trainer, make sure to not pass to the trainer any additional keyword arguments that are relative to from_pretrained() method.\n\ntodo: why trust_remote_code? I want more details.\n&quot;&quot;&quot;\nimport sys\n\nimport torch\nfrom peft import LoraConfig\n\nfrom transformers.modeling_utils import PreTrainedModel\n\nfrom pdb import set_trace as st\n\n\ndef test_bfloat16_int4(compute_dtype: torch.dtype,\n                       use_4bit,\n                       ):\n    &quot;&quot;&quot;\npython -c &quot;import torch; print(torch.cuda.get_device_capability());&quot;\n    todo: check other code test_bfloat16() do we need use_4bit?\n    &quot;&quot;&quot;\n    if compute_dtype == torch.float16 and use_4bit:\n        major, _ = torch.cuda.get_device_capability()\n        if major &gt;= 8:\n            print(&quot;=&quot; * 80)\n            print(&quot;Your GPU supports bfloat16, you can accelerate training with the argument --bfloat16&quot;)\n            print(&quot;=&quot; * 80)\n\n\ndef get_model_tokenizer_qlora_falcon7b(\n        # -- mode args\n        # model_id = &quot;tiiuae/falcon-7b&quot;\n        pretrained_model_name_or_path: str = &quot;ybelkada/falcon-7b-sharded-bf16&quot;,\n        use_cache: bool = True,\n        # -- lora args\n        lora_alpha=16,  # todo\n        lora_dropout=0.1,  # todo, evidence drop out really help? google, crfm, gpt4\n        lora_r=64,  # todo\n        bnb_4bit_compute_dtype=torch.float16,  # changed it from Guanaco hf\n\n        # -- training args\n        output_dir=&quot;./results&quot;,\n        per_device_train_batch_size=4,\n        gradient_accumulation_steps=4,\n        # paging so that the sudden mem gpu spikes don't cause the run to shut down\n        # (I think usually caused by too long seqs)\n        # todo: why 32 bit opt?\n        # todo: paged nadamw opt?\n        optim=&quot;paged_adamw_32bit&quot;,\n        save_steps=10,\n        logging_steps=10,\n        learning_rate=2e-4,\n        max_grad_norm=0.3,\n        max_steps=500,\n        warmup_ratio=0.03,\n        lr_scheduler_type=&quot;constant&quot;,\n        # -- quant. args (not recommended to be changed unless you know what your doing?)\n        load_in_4bit=True,  # load (usually huge) base model in 4 bits\n        bnb_4bit_quant_type=&quot;nf4&quot;,  # normal float 4 for the (large) base models qlora\n) -&gt; tuple:\n    &quot;&quot;&quot;\n    Load the Falcon 7B model, quantize it in 4bit and attach LoRA adapters on it.\n\n    bf16 = 1S, 7Exp, 8Mantissa\n    hypothesis: 7b trained due to 6.7 emergence rumour, I still don't think emergence is real.\n    Notes:\n        - ft a model is very specific to the model, tokenizer and training scheme. Thus we return\n            - model, tokenizer, ft config (peft config), training args\n\n    ref:\n        - https://colab.research.google.com/drive/1DOi8MFv4SWN9NImVornZ7t6BgmLoPQO-#scrollTo=AjB0WAqFSzlD\n    &quot;&quot;&quot;\n    from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoTokenizer\n\n    # - Get bnb config for bit-4 base model (bnb lib for using 4bit qlora quantization techniques by tim dettmers)\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=load_in_4bit,  # load (usually huge) base model in 4 bits\n        bnb_4bit_quant_type=bnb_4bit_quant_type,  # normal float 4 for the (usually huge) base model\n        bnb_4bit_compute_dtype=bnb_4bit_compute_dtype,  # if you can, during computation use bf16\n    )\n\n    # - Get falcon 4bit model\n    # todo, where is this being saved &amp; how to download quicker\n    model = AutoModelForCausalLM.from_pretrained(\n        pretrained_model_name_or_path=pretrained_model_name_or_path,\n        quantization_config=bnb_config,\n        trust_remote_code=True  # allows to execute custom code you download from the uploaded model code you are using\n    )\n    print(f'{type(model)=}')\n    print(f'{model=}')\n    # this is here to save gpu vram. Likely only needed when using 40b or when oom issues happen ref: https://stackoverflow.com/questions/76633335/why-does-hugging-face-falcon-model-use-mode-config-use-cache-false-why-wouldn\n    model.config.use_cache = use_cache\n    print(f'{type(model)=}')\n\n    # - Get falcon tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path,\n                                              trust_remote_code=True)  # execs code downloaded from hf hub\n    # tokenizer.pad_token = tokenizer.eos_token  # ref: https://stackoverflow.com/questions/76633368/why-does-the-falcon-qlora-tutorial-code-use-eos-token-as-pad-token\n    # tokenizer.add_special_tokens({'pad_token': '[PAD]'})  # I think this is fine if during the training pad is ignored\n    tokenizer.add_special_tokens({'pad_token': '&lt;|pad|&gt;'})  # I think this is fine if during the training pad is ignored\n\n    # - Modify model\n    # add pad token embed\n    model.resize_token_embeddings(len(tokenizer))  # todo: I think this is fine if during the training pad is ignored\n    model.transformer.word_embeddings.padding_idx = len(tokenizer) - 1\n    model.config.max_new_tokens = len(tokenizer)\n    # model.config.min_length = 1\n    print(f'{model=}')\n    print(f'{type(tokenizer)=}')\n    print(f'{tokenizer.pad_token=}')\n    # data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False) todo\n\n    # - Get falcon lora config\n    peft_config = LoraConfig(\n        lora_alpha=lora_alpha,\n        lora_dropout=lora_dropout,\n        r=lora_r,\n        bias=&quot;none&quot;,\n        task_type=&quot;CAUSAL_LM&quot;,\n        # model card for falcon tiiuae/falcon-7b: https://huggingface.co/tiiuae/falcon-7b/blob/main/modelling_RW.py\n        # does seem to include all trainable params as done by qlora on their own paper\n        target_modules=[\n            # word_embeddings,\n            &quot;query_key_value&quot;,\n            &quot;dense&quot;,\n            &quot;dense_h_to_4h&quot;,\n            &quot;dense_4h_to_h&quot;,\n            # &quot;lm_head&quot;\n        ]\n    )\n    print(f'{type(peft_config)=}')\n\n    # todo: print the num params of the lora = D1*r + D2*r and num of bytes by prec. (bytes) * num params\n    return model, tokenizer, peft_config\n\n\n# -- tests\n\ndef example_test_model_already_has_pad_token():\n    &quot;&quot;&quot;\n    if it already has pad token, it likely has a small prob, so we are done.\n\n    compare it's norm with other tokens to verify this is true.\n\npython ~/ultimate-utils/ultimate-utils-proj-src/uutils/hf_uu/model_tokenizer/falcon_uu_mdl_tok.py\n    &quot;&quot;&quot;\n    # - the get datasets todo: preprocessing, padding, streaming\n    from uutils.hf_uu.data_hf.common import get_guanaco_datsets_add_splits_train_test_only\n    trainset, _, testset = get_guanaco_datsets_add_splits_train_test_only()\n\n    # qlora flacon7b\n    from uutils.hf_uu.model_tokenizer.falcon_uu_mdl_tok import get_model_tokenizer_qlora_falcon7b\n    model, tokenizer, peft_config = get_model_tokenizer_qlora_falcon7b()\n    model: PreTrainedModel = model\n    print(f'{model=}')\n    sent = 'Dogs are great because they are '\n    print()\n\n    # print to see if pad tokens are present and if it ignores the tokens at the end\n    encoded_input = tokenizer(sent, padding='max_length', max_length=10, return_tensors='pt')\n    print(f'{encoded_input=}')\n\n    # Print all special tokens\n    print('\\n---- start Print all special tokens')\n    for token_name, token in tokenizer.special_tokens_map.items():\n        print(f&quot;{token_name}: {token}&quot;)\n    print('\\n---- end Print all special tokens')\n\n    # Get the ID for the '[PAD]' token\n    try:\n        pad_token_id = tokenizer.convert_tokens_to_ids('[PAD]')\n    except KeyError:\n        raise ValueError(&quot;Token [PAD] is not present in the tokenizer vocabulary.&quot;)\n\n    # Index into the model's embedding table\n    try:\n        print(f'{model.get_input_embeddings().weight.size()=}')\n        pad_embedding = model.get_input_embeddings().weight[pad_token_id]\n    except IndexError:\n        raise ValueError(f&quot;Token ID {pad_token_id} is not present in the model's embedding matrix.&quot;)\n\n    print(f'{pad_embedding=}')\n    print('Success!\\n')\n\n    # check it generates something sensible\n    # tokenizer.decode(model.generate(**tokenizer(sent, return_tensors='pt'), do_sample=True)[0])\n    input_ids, attention_mask = encoded_input['input_ids'], encoded_input['attention_mask']\n    predicted_tokens_ids_options = model.generate(input_ids=input_ids, attention_mask=attention_mask, do_sample=True)\n    predicted_tokens_ids = predicted_tokens_ids_options[0]\n    predicted_sent = tokenizer.decode(predicted_tokens_ids)\n    print(f'original sentence: {sent=}')\n    print(f'predicted sentence: {predicted_sent=}')\n    print('Success2!')\n\n\nif __name__ == '__main__':\n    import time\n\n    start_time = time.time()\n    example_test_model_already_has_pad_token()\n    print(f&quot;The main function executed in {time.time() - start_time} seconds.\\a&quot;)\n</code></pre>\n<p>it doesn't like the modifications to the model:</p>\n<pre><code>    model.transformer.word_embeddings.padding_idx = len(tokenizer) - 1\n    model.config.max_new_tokens = len(tokenizer)\n</code></pre>\n<p>How to fix?</p>\n<p>Errors:</p>\n<pre><code>/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/transformers/generation/utils.py:1259: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n  warnings.warn(\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/transformers/generation/utils.py:1452: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n  warnings.warn(\nTraceback (most recent call last):\n  File &quot;/lfs/hyperturing1/0/brando9/ultimate-utils/ultimate-utils-proj-src/uutils/hf_uu/model_tokenizer/falcon_uu_mdl_tok.py&quot;, line 211, in &lt;module&gt;\n    example_test_model_already_has_pad_token()\n  File &quot;/lfs/hyperturing1/0/brando9/ultimate-utils/ultimate-utils-proj-src/uutils/hf_uu/model_tokenizer/falcon_uu_mdl_tok.py&quot;, line 199, in example_test_model_already_has_pad_token\n    predicted_tokens_ids_options = model.generate(input_ids=input_ids, attention_mask=attention_mask, do_sample=True)\n  File &quot;/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/torch/utils/_contextlib.py&quot;, line 115, in decorate_context\n    return func(*args, **kwargs)\n  File &quot;/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/transformers/generation/utils.py&quot;, line 1572, in generate\n    return self.sample(\n  File &quot;/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/transformers/generation/utils.py&quot;, line 2633, in sample\n    next_token_scores = logits_warper(input_ids, next_token_scores)\n  File &quot;/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/transformers/generation/logits_process.py&quot;, line 92, in __call__\n    scores = processor(input_ids, scores)\n  File &quot;/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/transformers/generation/logits_process.py&quot;, line 302, in __call__\n    indices_to_remove = scores &lt; torch.topk(scores, top_k)[0][..., -1, None]\nRuntimeError: &quot;topk_cpu&quot; not implemented for 'Half'\n</code></pre>\n<hr />\n<h2>Bounty Section: Small GPT2 code example</h2>\n<p>Yes I agree that pad is assigned to eos. Eos is still eos. But during fine-tuning now the weights wrt to eos are unchanged. This might be an issue since the probability of eos has not shifted to the fine-tuning regime. One possibility is that eos is outputed with less chance. Yes we can still halt production when we see eos but we've not shifted the probability to output eos according to our fine-tuning distribution -- but all other tokens have changed distribution. I think this could be an issue because it's not like the old probability of eos is conserved since all tokens probs have changed except eos + even if the old eos prob was conserved, it's wrt wrong distribution (not the fine tuning one).</p>\n<p>e.g.,</p>\n<pre><code>if tokenizer.pad_token_id is None:\n    tokenizer.pad_token = tokenizer.eos_token\n ...\nraw_text_batch='a'\ntokenize_batch={'input_ids': tensor([[   64, 50256, 50256, 50256, 50256]]), 'attention_mask': tensor([[1, 0, 0, 0, 0]])}\n</code></pre>\n<p>but it would have been better to have</p>\n<pre><code>tokenize_batch={'input_ids': tensor([[   64, 50256, 50256, 50256, 50256]]), 'attention_mask': tensor([[1, 1, 0, 0, 0]])}\n</code></pre>\n<p>code</p>\n<pre><code>def test_eos_pad():\n    from datasets import load_dataset\n    import torch\n    from transformers import GPT2Tokenizer, GPT2LMHeadModel\n\n    raw_text_batch = 'a'\n\n    tokenizer = GPT2Tokenizer.from_pretrained(&quot;gpt2&quot;)\n    # print(f'{tokenizer.eos_token=}')\n    # print(f'{tokenizer.eos_token_id=}')\n    # print(f'{tokenizer.pad_token=}')\n    # print(f'{tokenizer.pad_token_id=}')\n\n    # print(f'{raw_text_batch=}')\n    # tokenize_batch = tokenizer(raw_text_batch, padding=&quot;max_length&quot;, max_length=5, truncation=True, return_tensors=&quot;pt&quot;)\n    # print(f'{tokenize_batch=}')\n\n    if tokenizer.pad_token_id is None:\n        tokenizer.pad_token = tokenizer.eos_token\n    probe_network = GPT2LMHeadModel.from_pretrained(&quot;gpt2&quot;)\n    device = torch.device(f&quot;cuda:{0}&quot; if torch.cuda.is_available() else &quot;cpu&quot;)\n    probe_network = probe_network.to(device)\n\n    print(f'{tokenizer.eos_token=}')\n    print(f'{tokenizer.eos_token_id=}')\n    print(f'{tokenizer.pad_token=}')\n    print(f'{tokenizer.pad_token_id=}')\n\n    print(f'{raw_text_batch=}')\n    tokenize_batch = tokenizer(raw_text_batch, padding=&quot;max_length&quot;, max_length=5, truncation=True, return_tensors=&quot;pt&quot;)\n    print(f'{tokenize_batch=}')\n    print('Done')\n</code></pre>\n<hr />\n<p>cross:</p>\n<ul>\n<li>hf discuss forum: <a href=\"https://discuss.huggingface.co/t/why-does-the-falcon-qlora-tutorial-code-use-eos-token-as-pad-token/45954\" rel=\"nofollow noreferrer\">https://discuss.huggingface.co/t/why-does-the-falcon-qlora-tutorial-code-use-eos-token-as-pad-token/45954</a></li>\n<li>pytorch forum discuss: <a href=\"https://discuss.pytorch.org/t/how-does-one-set-the-pad-token-correctly-not-to-eos-during-fine-tuning-to-avoid-model-not-predicting-eos/213619\" rel=\"nofollow noreferrer\">https://discuss.pytorch.org/t/how-does-one-set-the-pad-token-correctly-not-to-eos-during-fine-tuning-to-avoid-model-not-predicting-eos/213619</a></li>\n<li><a href=\"https://discord.com/channels/879548962464493619/1126681170957045770/1126681170957045770\" rel=\"nofollow noreferrer\">https://discord.com/channels/879548962464493619/1126681170957045770/1126681170957045770</a></li>\n<li>context peft pacman100 code: <a href=\"https://gist.github.com/pacman100/1731b41f7a90a87b457e8c5415ff1c14\" rel=\"nofollow noreferrer\">https://gist.github.com/pacman100/1731b41f7a90a87b457e8c5415ff1c14</a></li>\n<li>twitter tweet of this: <a href=\"https://twitter.com/BrandoHablando/status/1693676898013061337?s=20\" rel=\"nofollow noreferrer\">https://twitter.com/BrandoHablando/status/1693676898013061337?s=20</a></li>\n</ul>\n",
        "answer": "<p>for falcon you can use already existing <a href=\"https://huggingface.co/tiiuae/falcon-7b/blob/main/special_tokens_map.json\" rel=\"nofollow noreferrer\">special tokens</a> available for the model.</p>\n<pre class=\"lang-py prettyprint-override\"><code> tokenizer.add_special_tokens({&quot;pad_token&quot;: &quot;&gt;&gt;SUFFIX&lt;&lt;&quot;})\n model.config.pad_token_id = tokenizer.pad_token_id\n</code></pre>\n<p>this way you dont have to extend the embedding of the model like its done <a href=\"https://github.com/tatsu-lab/stanford_alpaca/blob/761dc5bfbdeeffa89b8bff5d038781a4055f796a/train.py#L65C42-L65C42\" rel=\"nofollow noreferrer\">here</a></p>\n<hr />\n<p>for other models, like llama2 you can set the</p>\n<pre class=\"lang-py prettyprint-override\"><code>tokenizer.pad_token = tokenizer.unk_token\n</code></pre>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 76633368,
            "link": "https://stackoverflow.com/questions/76633368/how-does-one-set-the-pad-token-correctly-not-to-eos-during-fine-tuning-to-avoi"
        }
    },
    {
        "question": "Poetry Cannot install bitsandbytes 0.43.1 in python 3.11\n<p>When I try poetry adding bitsandbytes, i get:</p>\n<pre><code>  - Installing bitsandbytes (0.43.1): Failed\n\n  RuntimeError\n\n  Unable to find installation candidates for bitsandbytes (0.43.1)\n\n  at ~/Library/Application Support/pypoetry/venv/lib/python3.11/site-packages/poetry/installation/chooser.py:74 in choose_for\n       70│\n       71│             links.append(link)\n       72│\n       73│         if not links:\n    →  74│             raise RuntimeError(f&quot;Unable to find installation candidates for {package}&quot;)\n       75│\n       76│         # Get the best link\n       77│         chosen = max(links, key=lambda link: self._sort_key(package, link))\n       78│\n\nCannot install bitsandbytes.\n</code></pre>\n<p>I am not sure how to get around this when installing this package using bitsandbytes. I tried accessing the poetry shell then doing:</p>\n<pre><code>pip install bitsandbytes\n</code></pre>\n<p>which picks up version <code>0.42.01</code>, but it does not seem to find <code>0.43.1</code> while poetry add picks up this version which is what I need.</p>\n",
        "answer": "<p>I am face the same question.</p>\n<p><a href=\"https://pypi.org/project/bitsandbytes/#files\" rel=\"nofollow noreferrer\">https://pypi.org/project/bitsandbytes/#files</a></p>\n<p>I guess it may be the high version require glibc 2.24+,but my system's glibc is lower than v2.24</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 78379101,
            "link": "https://stackoverflow.com/questions/78379101/poetry-cannot-install-bitsandbytes-0-43-1-in-python-3-11"
        }
    },
    {
        "question": "Incomplete Output with LLM with max_new_tokens\n<p>I am experimenting with Huggingface LLM models.</p>\n<p>And one issue I noticed is that output of the model ends abruptly and I ideally want it to complete the paragraph/sentences/code which it was it between of. (or altogether try to complete the answer within some fixed num of tokens)</p>\n<p>Although I have provided max_new_tokens = 300 and also in prompt I write:\n&quot;Output should be maximum of 300 words.&quot;</p>\n<p>The response is always incomplete and ends abruptly. Any way I can ask for a complete output within desired number of output tokens?</p>\n<p>Code:</p>\n<pre><code>checkpoint = &quot;HuggingFaceH4/starchat-alpha&quot;\ndevice = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot; \nclass StarCoderModel:\n  def __init__(self):\n    self.tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n    # make sure `--gpus all` is provided in docker run command if gpu is required\n    self.model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map='auto')\n\n  def infer(self, input_text, token_count):\n    inputs = self.tokenizer.encode(input_text, return_tensors=&quot;pt&quot;).to(device)\n    outputs = self.model.generate(inputs,  max_new_tokens=token_count, pad_token_id=self.tokenizer.eos_token_id)\n    return self.tokenizer.decode(outputs[0])[len(input_text):]\n</code></pre>\n<p>Sample-Output:</p>\n<pre><code>private DataType FuntionName(String someId) {\n    // TODO: Replace with implementation that utilizes someId to obtain information\n    return DataType.Value;\n}\n\n\nThe comment:\n\n- If someId is present in the code, use the getAPI from Client with someId as a parameter to obtain some information.\n- If the\n\n</code></pre>\n",
        "answer": "<p>As far as I know when working with basic model like the one you mentioned there are chances that the model won't be able to give quite good results. You can switch to working with models like GPT-3 and llama if that's feasible.</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 77061898,
            "link": "https://stackoverflow.com/questions/77061898/incomplete-output-with-llm-with-max-new-tokens"
        }
    },
    {
        "question": "Save a LLM model after adding RAG pipeline and embedded model and deploy as hugging face inference?\n<p>I have created a RAG (Retrieval-augmented generation) pipeline and using it with a 4-bit quantized openllama 13b loaded directly from hugging face and without fine-tuning the model.</p>\n<ol>\n<li>At first I need to save the model into local. But after using <code>torch.save(model.state_dict(), 'path')</code> to save the model, the model saved as adapter model and I can not load it from local again as well as can not able to push into hugging face.</li>\n<li>How can I use this configuration into hugging face to make inference API in the hugging face interface?</li>\n</ol>\n<p>Here is the code of loading quantized model:</p>\n<pre><code>bnb_config = transformers.BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type='nf4',\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_compute_dtype=bfloat16\n)\nhf_auth = '*'\nmodel_config = transformers.AutoConfig.from_pretrained(\n    model_id,\n    use_auth_token=hf_auth\n)\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n    model_id,\n    trust_remote_code=True,\n    config=model_config,\n    quantization_config=bnb_config,\n    device_map='auto',\n    use_auth_token=hf_auth\n)\nmodel.eval()\n</code></pre>\n",
        "answer": "<ul>\n<li>Login to hugging face by using</li>\n</ul>\n<blockquote>\n<p>huggingface-cli login</p>\n</blockquote>\n<ul>\n<li>Once you have logged in your terminal, now you can push your model to hugging face using below line.</li>\n</ul>\n<blockquote>\n<pre><code>model.push_to_hub(&quot;my-awesome-model&quot;)\n</code></pre>\n</blockquote>\n<p>But if you have not defined any arguments then it will give error.Please go through this link <a href=\"https://huggingface.co/docs/transformers/v4.15.0/en/model_sharing\" rel=\"nofollow noreferrer\">https://huggingface.co/docs/transformers/v4.15.0/en/model_sharing</a></p>\n<p>Please like the answer if you got the solution.</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 76955400,
            "link": "https://stackoverflow.com/questions/76955400/save-a-llm-model-after-adding-rag-pipeline-and-embedded-model-and-deploy-as-hugg"
        }
    },
    {
        "question": "How to load a huggingface dataset from local path?\n<p>Take a simple example in this website, <a href=\"https://huggingface.co/datasets/Dahoas/rm-static\" rel=\"nofollow noreferrer\">https://huggingface.co/datasets/Dahoas/rm-static</a>:</p>\n<p>if I want to load this dataset online, I just directly use,</p>\n<pre><code>from datasets import load_dataset\ndataset = load_dataset(&quot;Dahoas/rm-static&quot;) \n</code></pre>\n<p>What if I want to load dataset from local path, so I download the files and keep the same folder structure from web <code>Files and versions</code> fristly,</p>\n<pre><code>-data\n|-test-00000-of-00001-bf4c733542e35fcb.parquet\n|-train-00000-of-00001-2a1df75c6bce91ab.parquet\n-.gitattributes\n-README.md\n-dataset_infos.json\n</code></pre>\n<p>Then, put them into my folder, but shows error when loading:</p>\n<pre><code>dataset_path =&quot;/data/coco/dataset/Dahoas/rm-static&quot;\ntmp_dataset = load_dataset(dataset_path)\n</code></pre>\n<p>It shows <code>FileNotFoundError: No (supported) data files or dataset script found in /data/coco/dataset/Dahoas/rm-static.</code></p>\n",
        "answer": "<p>Save the data with <code>save_to_disk</code> then load it with <code>load_from_disk</code>. For example:</p>\n<pre><code>import datasets\nds = datasets.load_dataset(&quot;Dahoas/rm-static&quot;) \nds.save_to_disk(&quot;Path/to/save&quot;)\n</code></pre>\n<p>and later if you wanna re-utilize it just normal load_dataset will work</p>\n<pre><code>ds = datasets.load_from_disk(&quot;Path/to/save&quot;)\n</code></pre>\n<p>you can verify the same by printing the dataset you will be getting same result for both. This is the easier way out. The file format it is generally saved in is arrow.</p>\n<p>For the second method where you are downloading the parquet file. Would require you to explicitly declaring the dataset and it config, might be included in json and then you can load it.</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 77020278,
            "link": "https://stackoverflow.com/questions/77020278/how-to-load-a-huggingface-dataset-from-local-path"
        }
    },
    {
        "question": "Checkpoints ValueError with downloading HuggingFace models\n<p>I am having trouble downloading deepseek_vl_v2 into my computer.</p>\n<p>Here is the error in my terminal</p>\n<blockquote>\n<p>ValueError: The checkpoint you are trying to load has model type\n<code>deepseek_vl_v2</code> but Transformers does not recognize this\narchitecture. This could be because of an issue with the checkpoint,\nor because your version of Transformers is out of date.</p>\n<p>You can update Transformers with the command <code>pip install --upgrade transformers</code>. If this does not work, and the checkpoint is very new,\nthen there may not be a release version that supports this model yet.\nIn this case, you can get the most up-to-date code by installing\nTransformers from source with the command <code>pip install git+https://github.com/huggingface/transformers.git</code></p>\n</blockquote>\n<p>However, I have downloaded many HuggingFace models before and never seemed to have had this issue, which means I should have all the libraries all correctly downloaded. I have already tried updating the transformers library and installing from the direct source, but neither have resolved the issue.</p>\n<p>Please let me know if there any more information I can provide.</p>\n<p>Thanks so much in advance!</p>\n",
        "answer": "<p>Following up on this question I asked, I found the solution <a href=\"https://huggingface.co/deepseek-ai/deepseek-vl2/discussions/3\" rel=\"nofollow noreferrer\">here</a></p>\n<p>Basically, DeepSeek is not a model supported by HuggingFace's transformer library, so the only option for downloading this model is through importing the model source code directly as of now.</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 79401652,
            "link": "https://stackoverflow.com/questions/79401652/checkpoints-valueerror-with-downloading-huggingface-models"
        }
    },
    {
        "question": "How does one set the pad token correctly (not to eos) during fine-tuning to avoid model not predicting EOS?\n<h2>tldr; what I really want to know is what is the official way to set pad token for <strong>fine tuning</strong> it wasn't set during original training, so that it doesn't not learn to predict EOS.</h2>\n<p>colab: <a href=\"https://colab.research.google.com/drive/1poFdFYmkR_rDM5U5Z2WWjTepMQ8hvzNc?usp=sharing\" rel=\"nofollow noreferrer\">https://colab.research.google.com/drive/1poFdFYmkR_rDM5U5Z2WWjTepMQ8hvzNc?usp=sharing</a></p>\n<hr />\n<p>The HF falcon tutorial has the following line:</p>\n<pre><code>tokenizer.pad_token = tokenizer.eos_token\n</code></pre>\n<p>it looks strange to me. It make sense pad and eos are the same but then why even make a difference between them in the first place in general?</p>\n<p>Note its wrong to do pad = eos. This means during fine-tuning the model will never be trained to output eos (most likely) since eos is treated as pad token and no back propagated:</p>\n<pre><code>I just observed that when I set tokenizer.pad_token = tokenizer.eos_token during training, the model won't stop generating during inference, since it was trained to not output the eos token (per discussions above).\n</code></pre>\n<p>I saw this (here <a href=\"https://github.com/huggingface/transformers/issues/22794\" rel=\"nofollow noreferrer\">https://github.com/huggingface/transformers/issues/22794</a>):</p>\n<pre><code>tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n</code></pre>\n<p>But this assumes the model has a pad_token. I think an additional check has to be done that it does have an embedding for pad_token so that there are no run time errors (~type errors in the matrix extraction from the embedding &quot;table&quot;/matrix).</p>\n<p>But if one does that some care might be needed to initialize the new token so that it dominates the generation: <a href=\"https://nlp.stanford.edu/%7Ejohnhew/vocab-expansion.html\" rel=\"nofollow noreferrer\">https://nlp.stanford.edu/~johnhew/vocab-expansion.html</a></p>\n<hr />\n<p>code:</p>\n<pre class=\"lang-py prettyprint-override\"><code>def get_model_tokenizer_qlora_falcon7b(model_name: str = &quot;ybelkada/falcon-7b-sharded-bf16&quot;,\n                                       config: wand.Config,  # todo\n                                       lora_alpha=16,  # todo\n                                       lora_dropout=0.1,  # todo\n                                       lora_r=64,  # todo\n                                       bnb_4bit_compute_dtype=torch.float16,  # changed it from Guanaco hf\n                                       ) -&gt; tuple:\n    &quot;&quot;&quot;\n    Load the Falcon 7B model, quantize it in 4bit and attach LoRA adapters on it.\n\n    bf16 = 1S, 7Exp, 8Mantissa\n\n    Do:\n        pip install bitsandbytes\n    ref:\n        - https://colab.research.google.com/drive/1DOi8MFv4SWN9NImVornZ7t6BgmLoPQO-#scrollTo=AjB0WAqFSzlD\n    &quot;&quot;&quot;\n    from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoTokenizer\n\n    # model_id = &quot;tiiuae/falcon-7b&quot;\n    # model_name: str = &quot;ybelkada/falcon-7b-sharded-bf16&quot;\n\n    # - get bnb config for bit-4 base model (bnb lib for using 4bit qlora quantization techniques by tim dettmers)\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,  # load (usually huge) base model in 4 bits\n        bnb_4bit_quant_type=&quot;nf4&quot;,  # normal float 4 for the (usually huge) base model. introduces error but fixed by ft\n        # ref: https://gist.github.com/pacman100/1731b41f7a90a87b457e8c5415ff1c14\n        bnb_4bit_compute_dtype=bnb_4bit_compute_dtype,\n    )\n\n    # - get falcon 4bit model\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        quantization_config=bnb_config,\n        trust_remote_code=True  # allows to execute custom code you download from the uploaded model code you are using\n    )\n    model.config.use_cache = False  # todo: why? https://stackoverflow.com/questions/76633335/why-does-hugging-face-falcon-model-use-mode-config-use-cache-false-why-wouldn\n\n    # get falcon tockenizer\n    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)  # execs code downloaded from hf hub\n    tokenizer.pad_token = tokenizer.eos_token\n</code></pre>\n<hr />\n<h2>Modifying model gives issues</h2>\n<p>This still not works:</p>\n<pre><code> UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n</code></pre>\n<p>code:</p>\n<pre><code>&quot;&quot;&quot;\nsfttrainer (likely using peft) best practices:\nhttps://huggingface.co/docs/trl/main/en/sft_trainer#best-practices\n\nBest practices\n\nPay attention to the following best practices when training a model with that trainer:\n\n- SFTTrainer always pads by default the sequences to the max_seq_length argument of the SFTTrainer. If none is passed, the trainer will retrieve that value from the tokenizer. Some tokenizers do not provide default value, so there is a check to retrieve the minimum between 2048 and that value. Make sure to check it before training.\n- For training adapters in 8bit, you might need to tweak the arguments of the prepare_model_for_int8_training method from PEFT, hence we advise users to use prepare_in_int8_kwargs field, or create the PeftModel outside the SFTTrainer and pass it.\n- For a more memory-efficient training using adapters, you can load the base model in 8bit, for that simply add load_in_8bit argument when creating the SFTTrainer, or create a base model in 8bit outside the trainer and pass it.\n- If you create a model outside the trainer, make sure to not pass to the trainer any additional keyword arguments that are relative to from_pretrained() method.\n\ntodo: why trust_remote_code? I want more details.\n&quot;&quot;&quot;\nimport sys\n\nimport torch\nfrom peft import LoraConfig\n\nfrom transformers.modeling_utils import PreTrainedModel\n\nfrom pdb import set_trace as st\n\n\ndef test_bfloat16_int4(compute_dtype: torch.dtype,\n                       use_4bit,\n                       ):\n    &quot;&quot;&quot;\npython -c &quot;import torch; print(torch.cuda.get_device_capability());&quot;\n    todo: check other code test_bfloat16() do we need use_4bit?\n    &quot;&quot;&quot;\n    if compute_dtype == torch.float16 and use_4bit:\n        major, _ = torch.cuda.get_device_capability()\n        if major &gt;= 8:\n            print(&quot;=&quot; * 80)\n            print(&quot;Your GPU supports bfloat16, you can accelerate training with the argument --bfloat16&quot;)\n            print(&quot;=&quot; * 80)\n\n\ndef get_model_tokenizer_qlora_falcon7b(\n        # -- mode args\n        # model_id = &quot;tiiuae/falcon-7b&quot;\n        pretrained_model_name_or_path: str = &quot;ybelkada/falcon-7b-sharded-bf16&quot;,\n        use_cache: bool = True,\n        # -- lora args\n        lora_alpha=16,  # todo\n        lora_dropout=0.1,  # todo, evidence drop out really help? google, crfm, gpt4\n        lora_r=64,  # todo\n        bnb_4bit_compute_dtype=torch.float16,  # changed it from Guanaco hf\n\n        # -- training args\n        output_dir=&quot;./results&quot;,\n        per_device_train_batch_size=4,\n        gradient_accumulation_steps=4,\n        # paging so that the sudden mem gpu spikes don't cause the run to shut down\n        # (I think usually caused by too long seqs)\n        # todo: why 32 bit opt?\n        # todo: paged nadamw opt?\n        optim=&quot;paged_adamw_32bit&quot;,\n        save_steps=10,\n        logging_steps=10,\n        learning_rate=2e-4,\n        max_grad_norm=0.3,\n        max_steps=500,\n        warmup_ratio=0.03,\n        lr_scheduler_type=&quot;constant&quot;,\n        # -- quant. args (not recommended to be changed unless you know what your doing?)\n        load_in_4bit=True,  # load (usually huge) base model in 4 bits\n        bnb_4bit_quant_type=&quot;nf4&quot;,  # normal float 4 for the (large) base models qlora\n) -&gt; tuple:\n    &quot;&quot;&quot;\n    Load the Falcon 7B model, quantize it in 4bit and attach LoRA adapters on it.\n\n    bf16 = 1S, 7Exp, 8Mantissa\n    hypothesis: 7b trained due to 6.7 emergence rumour, I still don't think emergence is real.\n    Notes:\n        - ft a model is very specific to the model, tokenizer and training scheme. Thus we return\n            - model, tokenizer, ft config (peft config), training args\n\n    ref:\n        - https://colab.research.google.com/drive/1DOi8MFv4SWN9NImVornZ7t6BgmLoPQO-#scrollTo=AjB0WAqFSzlD\n    &quot;&quot;&quot;\n    from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoTokenizer\n\n    # - Get bnb config for bit-4 base model (bnb lib for using 4bit qlora quantization techniques by tim dettmers)\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=load_in_4bit,  # load (usually huge) base model in 4 bits\n        bnb_4bit_quant_type=bnb_4bit_quant_type,  # normal float 4 for the (usually huge) base model\n        bnb_4bit_compute_dtype=bnb_4bit_compute_dtype,  # if you can, during computation use bf16\n    )\n\n    # - Get falcon 4bit model\n    # todo, where is this being saved &amp; how to download quicker\n    model = AutoModelForCausalLM.from_pretrained(\n        pretrained_model_name_or_path=pretrained_model_name_or_path,\n        quantization_config=bnb_config,\n        trust_remote_code=True  # allows to execute custom code you download from the uploaded model code you are using\n    )\n    print(f'{type(model)=}')\n    print(f'{model=}')\n    # this is here to save gpu vram. Likely only needed when using 40b or when oom issues happen ref: https://stackoverflow.com/questions/76633335/why-does-hugging-face-falcon-model-use-mode-config-use-cache-false-why-wouldn\n    model.config.use_cache = use_cache\n    print(f'{type(model)=}')\n\n    # - Get falcon tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path,\n                                              trust_remote_code=True)  # execs code downloaded from hf hub\n    # tokenizer.pad_token = tokenizer.eos_token  # ref: https://stackoverflow.com/questions/76633368/why-does-the-falcon-qlora-tutorial-code-use-eos-token-as-pad-token\n    # tokenizer.add_special_tokens({'pad_token': '[PAD]'})  # I think this is fine if during the training pad is ignored\n    tokenizer.add_special_tokens({'pad_token': '&lt;|pad|&gt;'})  # I think this is fine if during the training pad is ignored\n\n    # - Modify model\n    # add pad token embed\n    model.resize_token_embeddings(len(tokenizer))  # todo: I think this is fine if during the training pad is ignored\n    model.transformer.word_embeddings.padding_idx = len(tokenizer) - 1\n    model.config.max_new_tokens = len(tokenizer)\n    # model.config.min_length = 1\n    print(f'{model=}')\n    print(f'{type(tokenizer)=}')\n    print(f'{tokenizer.pad_token=}')\n    # data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False) todo\n\n    # - Get falcon lora config\n    peft_config = LoraConfig(\n        lora_alpha=lora_alpha,\n        lora_dropout=lora_dropout,\n        r=lora_r,\n        bias=&quot;none&quot;,\n        task_type=&quot;CAUSAL_LM&quot;,\n        # model card for falcon tiiuae/falcon-7b: https://huggingface.co/tiiuae/falcon-7b/blob/main/modelling_RW.py\n        # does seem to include all trainable params as done by qlora on their own paper\n        target_modules=[\n            # word_embeddings,\n            &quot;query_key_value&quot;,\n            &quot;dense&quot;,\n            &quot;dense_h_to_4h&quot;,\n            &quot;dense_4h_to_h&quot;,\n            # &quot;lm_head&quot;\n        ]\n    )\n    print(f'{type(peft_config)=}')\n\n    # todo: print the num params of the lora = D1*r + D2*r and num of bytes by prec. (bytes) * num params\n    return model, tokenizer, peft_config\n\n\n# -- tests\n\ndef example_test_model_already_has_pad_token():\n    &quot;&quot;&quot;\n    if it already has pad token, it likely has a small prob, so we are done.\n\n    compare it's norm with other tokens to verify this is true.\n\npython ~/ultimate-utils/ultimate-utils-proj-src/uutils/hf_uu/model_tokenizer/falcon_uu_mdl_tok.py\n    &quot;&quot;&quot;\n    # - the get datasets todo: preprocessing, padding, streaming\n    from uutils.hf_uu.data_hf.common import get_guanaco_datsets_add_splits_train_test_only\n    trainset, _, testset = get_guanaco_datsets_add_splits_train_test_only()\n\n    # qlora flacon7b\n    from uutils.hf_uu.model_tokenizer.falcon_uu_mdl_tok import get_model_tokenizer_qlora_falcon7b\n    model, tokenizer, peft_config = get_model_tokenizer_qlora_falcon7b()\n    model: PreTrainedModel = model\n    print(f'{model=}')\n    sent = 'Dogs are great because they are '\n    print()\n\n    # print to see if pad tokens are present and if it ignores the tokens at the end\n    encoded_input = tokenizer(sent, padding='max_length', max_length=10, return_tensors='pt')\n    print(f'{encoded_input=}')\n\n    # Print all special tokens\n    print('\\n---- start Print all special tokens')\n    for token_name, token in tokenizer.special_tokens_map.items():\n        print(f&quot;{token_name}: {token}&quot;)\n    print('\\n---- end Print all special tokens')\n\n    # Get the ID for the '[PAD]' token\n    try:\n        pad_token_id = tokenizer.convert_tokens_to_ids('[PAD]')\n    except KeyError:\n        raise ValueError(&quot;Token [PAD] is not present in the tokenizer vocabulary.&quot;)\n\n    # Index into the model's embedding table\n    try:\n        print(f'{model.get_input_embeddings().weight.size()=}')\n        pad_embedding = model.get_input_embeddings().weight[pad_token_id]\n    except IndexError:\n        raise ValueError(f&quot;Token ID {pad_token_id} is not present in the model's embedding matrix.&quot;)\n\n    print(f'{pad_embedding=}')\n    print('Success!\\n')\n\n    # check it generates something sensible\n    # tokenizer.decode(model.generate(**tokenizer(sent, return_tensors='pt'), do_sample=True)[0])\n    input_ids, attention_mask = encoded_input['input_ids'], encoded_input['attention_mask']\n    predicted_tokens_ids_options = model.generate(input_ids=input_ids, attention_mask=attention_mask, do_sample=True)\n    predicted_tokens_ids = predicted_tokens_ids_options[0]\n    predicted_sent = tokenizer.decode(predicted_tokens_ids)\n    print(f'original sentence: {sent=}')\n    print(f'predicted sentence: {predicted_sent=}')\n    print('Success2!')\n\n\nif __name__ == '__main__':\n    import time\n\n    start_time = time.time()\n    example_test_model_already_has_pad_token()\n    print(f&quot;The main function executed in {time.time() - start_time} seconds.\\a&quot;)\n</code></pre>\n<p>it doesn't like the modifications to the model:</p>\n<pre><code>    model.transformer.word_embeddings.padding_idx = len(tokenizer) - 1\n    model.config.max_new_tokens = len(tokenizer)\n</code></pre>\n<p>How to fix?</p>\n<p>Errors:</p>\n<pre><code>/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/transformers/generation/utils.py:1259: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n  warnings.warn(\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/transformers/generation/utils.py:1452: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n  warnings.warn(\nTraceback (most recent call last):\n  File &quot;/lfs/hyperturing1/0/brando9/ultimate-utils/ultimate-utils-proj-src/uutils/hf_uu/model_tokenizer/falcon_uu_mdl_tok.py&quot;, line 211, in &lt;module&gt;\n    example_test_model_already_has_pad_token()\n  File &quot;/lfs/hyperturing1/0/brando9/ultimate-utils/ultimate-utils-proj-src/uutils/hf_uu/model_tokenizer/falcon_uu_mdl_tok.py&quot;, line 199, in example_test_model_already_has_pad_token\n    predicted_tokens_ids_options = model.generate(input_ids=input_ids, attention_mask=attention_mask, do_sample=True)\n  File &quot;/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/torch/utils/_contextlib.py&quot;, line 115, in decorate_context\n    return func(*args, **kwargs)\n  File &quot;/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/transformers/generation/utils.py&quot;, line 1572, in generate\n    return self.sample(\n  File &quot;/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/transformers/generation/utils.py&quot;, line 2633, in sample\n    next_token_scores = logits_warper(input_ids, next_token_scores)\n  File &quot;/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/transformers/generation/logits_process.py&quot;, line 92, in __call__\n    scores = processor(input_ids, scores)\n  File &quot;/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/transformers/generation/logits_process.py&quot;, line 302, in __call__\n    indices_to_remove = scores &lt; torch.topk(scores, top_k)[0][..., -1, None]\nRuntimeError: &quot;topk_cpu&quot; not implemented for 'Half'\n</code></pre>\n<hr />\n<h2>Bounty Section: Small GPT2 code example</h2>\n<p>Yes I agree that pad is assigned to eos. Eos is still eos. But during fine-tuning now the weights wrt to eos are unchanged. This might be an issue since the probability of eos has not shifted to the fine-tuning regime. One possibility is that eos is outputed with less chance. Yes we can still halt production when we see eos but we've not shifted the probability to output eos according to our fine-tuning distribution -- but all other tokens have changed distribution. I think this could be an issue because it's not like the old probability of eos is conserved since all tokens probs have changed except eos + even if the old eos prob was conserved, it's wrt wrong distribution (not the fine tuning one).</p>\n<p>e.g.,</p>\n<pre><code>if tokenizer.pad_token_id is None:\n    tokenizer.pad_token = tokenizer.eos_token\n ...\nraw_text_batch='a'\ntokenize_batch={'input_ids': tensor([[   64, 50256, 50256, 50256, 50256]]), 'attention_mask': tensor([[1, 0, 0, 0, 0]])}\n</code></pre>\n<p>but it would have been better to have</p>\n<pre><code>tokenize_batch={'input_ids': tensor([[   64, 50256, 50256, 50256, 50256]]), 'attention_mask': tensor([[1, 1, 0, 0, 0]])}\n</code></pre>\n<p>code</p>\n<pre><code>def test_eos_pad():\n    from datasets import load_dataset\n    import torch\n    from transformers import GPT2Tokenizer, GPT2LMHeadModel\n\n    raw_text_batch = 'a'\n\n    tokenizer = GPT2Tokenizer.from_pretrained(&quot;gpt2&quot;)\n    # print(f'{tokenizer.eos_token=}')\n    # print(f'{tokenizer.eos_token_id=}')\n    # print(f'{tokenizer.pad_token=}')\n    # print(f'{tokenizer.pad_token_id=}')\n\n    # print(f'{raw_text_batch=}')\n    # tokenize_batch = tokenizer(raw_text_batch, padding=&quot;max_length&quot;, max_length=5, truncation=True, return_tensors=&quot;pt&quot;)\n    # print(f'{tokenize_batch=}')\n\n    if tokenizer.pad_token_id is None:\n        tokenizer.pad_token = tokenizer.eos_token\n    probe_network = GPT2LMHeadModel.from_pretrained(&quot;gpt2&quot;)\n    device = torch.device(f&quot;cuda:{0}&quot; if torch.cuda.is_available() else &quot;cpu&quot;)\n    probe_network = probe_network.to(device)\n\n    print(f'{tokenizer.eos_token=}')\n    print(f'{tokenizer.eos_token_id=}')\n    print(f'{tokenizer.pad_token=}')\n    print(f'{tokenizer.pad_token_id=}')\n\n    print(f'{raw_text_batch=}')\n    tokenize_batch = tokenizer(raw_text_batch, padding=&quot;max_length&quot;, max_length=5, truncation=True, return_tensors=&quot;pt&quot;)\n    print(f'{tokenize_batch=}')\n    print('Done')\n</code></pre>\n<hr />\n<p>cross:</p>\n<ul>\n<li>hf discuss forum: <a href=\"https://discuss.huggingface.co/t/why-does-the-falcon-qlora-tutorial-code-use-eos-token-as-pad-token/45954\" rel=\"nofollow noreferrer\">https://discuss.huggingface.co/t/why-does-the-falcon-qlora-tutorial-code-use-eos-token-as-pad-token/45954</a></li>\n<li>pytorch forum discuss: <a href=\"https://discuss.pytorch.org/t/how-does-one-set-the-pad-token-correctly-not-to-eos-during-fine-tuning-to-avoid-model-not-predicting-eos/213619\" rel=\"nofollow noreferrer\">https://discuss.pytorch.org/t/how-does-one-set-the-pad-token-correctly-not-to-eos-during-fine-tuning-to-avoid-model-not-predicting-eos/213619</a></li>\n<li><a href=\"https://discord.com/channels/879548962464493619/1126681170957045770/1126681170957045770\" rel=\"nofollow noreferrer\">https://discord.com/channels/879548962464493619/1126681170957045770/1126681170957045770</a></li>\n<li>context peft pacman100 code: <a href=\"https://gist.github.com/pacman100/1731b41f7a90a87b457e8c5415ff1c14\" rel=\"nofollow noreferrer\">https://gist.github.com/pacman100/1731b41f7a90a87b457e8c5415ff1c14</a></li>\n<li>twitter tweet of this: <a href=\"https://twitter.com/BrandoHablando/status/1693676898013061337?s=20\" rel=\"nofollow noreferrer\">https://twitter.com/BrandoHablando/status/1693676898013061337?s=20</a></li>\n</ul>\n",
        "answer": "<p>For next-token-prediction, setting EOS to PAD during fine-tuning is actually okay. Note that you may need to manually add the EOS token (<code>[PAD]</code> in this case) to the training data though. As an example, given the input sequence <code>Hello world.</code>, we can tokenize the sequence into [<code>Hello</code>, <code>world</code>, <code>.</code>, <code>[PAD]</code>].</p>\n<p>The attention mask will be [<code>1</code>, <code>1</code>, <code>1</code>, <code>0</code>] and no next token will be predicted for the <code>[PAD]</code> token.</p>\n<p>Remember however that in autoregressive language modelling, the token labels  for an input sequence will always be the next token in that sequence. For example:</p>\n<p>Input: [<code>Hello</code>, <code>world</code>, <code>.</code>]</p>\n<p>Labels: [<code>world</code>, <code>.</code>, <code>[PAD]</code>]</p>\n<p>In other words; the model is optimized to generate a <code>[PAD]</code> token after a <code>.</code> token by optimising the token-level cross-entropy loss, even though the <code>[PAD]</code> token (EOS) itself is masked during the attention calculation and thus not back-propagated.</p>\n<p>Note that while you can use any token as the EOS token, using the same embedding for the PAD and EOS token is a slight optimisation to remove one entry from the embedding weight matrix. Due to the size of the vocabulary this memory saving is however often negligible (1 token out of e.g. ~50k tokens in GPT2).</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 76633368,
            "link": "https://stackoverflow.com/questions/76633368/how-does-one-set-the-pad-token-correctly-not-to-eos-during-fine-tuning-to-avoi"
        }
    },
    {
        "question": "How does the data splitting actually work in Multi GPU Inference for Accelerate when used in a batched inference setting?\n<p>I followed the code given in this <a href=\"https://github.com/huggingface/accelerate/issues/2018\" rel=\"nofollow noreferrer\">github issue</a> and this <a href=\"https://medium.com/@geronimo7/llms-multi-gpu-inference-with-accelerate-5a8333e4c5db\" rel=\"nofollow noreferrer\">medium blog</a></p>\n<p>I ran the batched experiment with <code>process = 1</code> and <code>process=4</code> it gave me the result but I'm confused right now because I thought the result would be in order. If they are not in orger, them I won't be able to map those with the ground Truth</p>\n<p>For example let's say my <code>data_length=5</code> and my <code>batch=3</code>. So if I got results <code>[[1,2,3], [4,5]]</code> for <code>process=1</code> then I'm expecting when using <code>process = 4</code>, I should get the same results when I flatten the results.</p>\n<p>they are coming out of order. What am I doing wrong?</p>\n<p><strong><em>NOTE: I used a <code>zip(text,label)</code> while passing data to processes to get the correct mapping BUT that is not the question</em></strong></p>\n<p>Below is the code:</p>\n<pre><code>def seed_everything(seed=13):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    set_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed = 13)\n\n\ndef test():\n    accelerator = Accelerator()\n    accelerator.wait_for_everyone() \n    seed_everything(seed = 13)\n    \n    model = load_model(model = &quot;my_model_path&quot;\n                        lora = &quot;./my_lora_checkpoint/checkpoint-8200&quot;,\n                       device = {&quot;&quot;: accelerator.process_index}, \n                       num_labels = NUM_LABELS,\n                       merge_unload = False)\n    \n    \n    with accelerator.split_between_processes(zipped_text_label) as prompts:\n    \n        res = {&quot;pred_probs&quot;: [], &quot;pred_labels&quot;: []}\n\n        BATCH_SIZE = 10\n    \n        BATCHES = [prompts[i:i + BATCH_SIZE] for i in range(0, len(prompts), BATCH_SIZE)]\n        print(len(BATCHES[0]))\n\n        pred_probs = []\n        pred_labels = []\n\n        for batch in tqdm(BATCHES):\n            text_batch = [i[0] for i in batch]\n            score_batch = [i[1] for i in batch]\n            \n            with torch.no_grad():\n                inputs = tokenizer(text_batch,truncation= True, max_length=MAX_LENGTH, padding=&quot;max_length&quot;, return_tensors = &quot;pt&quot;).to(model.device)\n                logits = model(**inputs).logits.cpu().to(torch.float32)\n                probs = torch.softmax(logits, dim = 1).numpy()\n                res[&quot;pred_probs&quot;].append(probs.tolist())\n                res[&quot;pred_labels&quot;].append(probs.argmax(axis = 1).tolist())\n        \n        res = [res]\n    \n    result = gather_object(res)\n    if accelerator.is_main_process:\n        print(result)\n\n\nnotebook_launcher(test, num_processes=1)                              \n</code></pre>\n<pre><code></code></pre>\n",
        "answer": "<p>In <code>accelerate</code>, the proper way to control batch splitting is using <code>DataLoaderConfiguration</code> class. Here is an example:</p>\n<pre><code>from accelerate.utils import DataLoaderConfiguration\n\ndataloader_config = DataLoaderConfiguration(dispatch_batches=True, split_batches=False)\naccelerator = accelerate.Accelerator(dataloader_config=dataloader_config)\n</code></pre>\n<p>In this example, batches given by your dataloader will be dispatched to different process. Each process has <strong>exactly one batch</strong> and batches from different processes are different. Because normally you won't be able to control which asynchronous process runs faster, you should expect the batches are dispatched <strong>randomly</strong> in practice.</p>\n<p>By the way, <code>split_batches</code> param controls whether to split each batch given by your dataloader into smaller batches and dispatch each smaller batch to each process.</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 78953324,
            "link": "https://stackoverflow.com/questions/78953324/how-does-the-data-splitting-actually-work-in-multi-gpu-inference-for-accelerate"
        }
    },
    {
        "question": "How does one set the pad token correctly (not to eos) during fine-tuning to avoid model not predicting EOS?\n<h2>tldr; what I really want to know is what is the official way to set pad token for <strong>fine tuning</strong> it wasn't set during original training, so that it doesn't not learn to predict EOS.</h2>\n<p>colab: <a href=\"https://colab.research.google.com/drive/1poFdFYmkR_rDM5U5Z2WWjTepMQ8hvzNc?usp=sharing\" rel=\"nofollow noreferrer\">https://colab.research.google.com/drive/1poFdFYmkR_rDM5U5Z2WWjTepMQ8hvzNc?usp=sharing</a></p>\n<hr />\n<p>The HF falcon tutorial has the following line:</p>\n<pre><code>tokenizer.pad_token = tokenizer.eos_token\n</code></pre>\n<p>it looks strange to me. It make sense pad and eos are the same but then why even make a difference between them in the first place in general?</p>\n<p>Note its wrong to do pad = eos. This means during fine-tuning the model will never be trained to output eos (most likely) since eos is treated as pad token and no back propagated:</p>\n<pre><code>I just observed that when I set tokenizer.pad_token = tokenizer.eos_token during training, the model won't stop generating during inference, since it was trained to not output the eos token (per discussions above).\n</code></pre>\n<p>I saw this (here <a href=\"https://github.com/huggingface/transformers/issues/22794\" rel=\"nofollow noreferrer\">https://github.com/huggingface/transformers/issues/22794</a>):</p>\n<pre><code>tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n</code></pre>\n<p>But this assumes the model has a pad_token. I think an additional check has to be done that it does have an embedding for pad_token so that there are no run time errors (~type errors in the matrix extraction from the embedding &quot;table&quot;/matrix).</p>\n<p>But if one does that some care might be needed to initialize the new token so that it dominates the generation: <a href=\"https://nlp.stanford.edu/%7Ejohnhew/vocab-expansion.html\" rel=\"nofollow noreferrer\">https://nlp.stanford.edu/~johnhew/vocab-expansion.html</a></p>\n<hr />\n<p>code:</p>\n<pre class=\"lang-py prettyprint-override\"><code>def get_model_tokenizer_qlora_falcon7b(model_name: str = &quot;ybelkada/falcon-7b-sharded-bf16&quot;,\n                                       config: wand.Config,  # todo\n                                       lora_alpha=16,  # todo\n                                       lora_dropout=0.1,  # todo\n                                       lora_r=64,  # todo\n                                       bnb_4bit_compute_dtype=torch.float16,  # changed it from Guanaco hf\n                                       ) -&gt; tuple:\n    &quot;&quot;&quot;\n    Load the Falcon 7B model, quantize it in 4bit and attach LoRA adapters on it.\n\n    bf16 = 1S, 7Exp, 8Mantissa\n\n    Do:\n        pip install bitsandbytes\n    ref:\n        - https://colab.research.google.com/drive/1DOi8MFv4SWN9NImVornZ7t6BgmLoPQO-#scrollTo=AjB0WAqFSzlD\n    &quot;&quot;&quot;\n    from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoTokenizer\n\n    # model_id = &quot;tiiuae/falcon-7b&quot;\n    # model_name: str = &quot;ybelkada/falcon-7b-sharded-bf16&quot;\n\n    # - get bnb config for bit-4 base model (bnb lib for using 4bit qlora quantization techniques by tim dettmers)\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,  # load (usually huge) base model in 4 bits\n        bnb_4bit_quant_type=&quot;nf4&quot;,  # normal float 4 for the (usually huge) base model. introduces error but fixed by ft\n        # ref: https://gist.github.com/pacman100/1731b41f7a90a87b457e8c5415ff1c14\n        bnb_4bit_compute_dtype=bnb_4bit_compute_dtype,\n    )\n\n    # - get falcon 4bit model\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        quantization_config=bnb_config,\n        trust_remote_code=True  # allows to execute custom code you download from the uploaded model code you are using\n    )\n    model.config.use_cache = False  # todo: why? https://stackoverflow.com/questions/76633335/why-does-hugging-face-falcon-model-use-mode-config-use-cache-false-why-wouldn\n\n    # get falcon tockenizer\n    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)  # execs code downloaded from hf hub\n    tokenizer.pad_token = tokenizer.eos_token\n</code></pre>\n<hr />\n<h2>Modifying model gives issues</h2>\n<p>This still not works:</p>\n<pre><code> UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n</code></pre>\n<p>code:</p>\n<pre><code>&quot;&quot;&quot;\nsfttrainer (likely using peft) best practices:\nhttps://huggingface.co/docs/trl/main/en/sft_trainer#best-practices\n\nBest practices\n\nPay attention to the following best practices when training a model with that trainer:\n\n- SFTTrainer always pads by default the sequences to the max_seq_length argument of the SFTTrainer. If none is passed, the trainer will retrieve that value from the tokenizer. Some tokenizers do not provide default value, so there is a check to retrieve the minimum between 2048 and that value. Make sure to check it before training.\n- For training adapters in 8bit, you might need to tweak the arguments of the prepare_model_for_int8_training method from PEFT, hence we advise users to use prepare_in_int8_kwargs field, or create the PeftModel outside the SFTTrainer and pass it.\n- For a more memory-efficient training using adapters, you can load the base model in 8bit, for that simply add load_in_8bit argument when creating the SFTTrainer, or create a base model in 8bit outside the trainer and pass it.\n- If you create a model outside the trainer, make sure to not pass to the trainer any additional keyword arguments that are relative to from_pretrained() method.\n\ntodo: why trust_remote_code? I want more details.\n&quot;&quot;&quot;\nimport sys\n\nimport torch\nfrom peft import LoraConfig\n\nfrom transformers.modeling_utils import PreTrainedModel\n\nfrom pdb import set_trace as st\n\n\ndef test_bfloat16_int4(compute_dtype: torch.dtype,\n                       use_4bit,\n                       ):\n    &quot;&quot;&quot;\npython -c &quot;import torch; print(torch.cuda.get_device_capability());&quot;\n    todo: check other code test_bfloat16() do we need use_4bit?\n    &quot;&quot;&quot;\n    if compute_dtype == torch.float16 and use_4bit:\n        major, _ = torch.cuda.get_device_capability()\n        if major &gt;= 8:\n            print(&quot;=&quot; * 80)\n            print(&quot;Your GPU supports bfloat16, you can accelerate training with the argument --bfloat16&quot;)\n            print(&quot;=&quot; * 80)\n\n\ndef get_model_tokenizer_qlora_falcon7b(\n        # -- mode args\n        # model_id = &quot;tiiuae/falcon-7b&quot;\n        pretrained_model_name_or_path: str = &quot;ybelkada/falcon-7b-sharded-bf16&quot;,\n        use_cache: bool = True,\n        # -- lora args\n        lora_alpha=16,  # todo\n        lora_dropout=0.1,  # todo, evidence drop out really help? google, crfm, gpt4\n        lora_r=64,  # todo\n        bnb_4bit_compute_dtype=torch.float16,  # changed it from Guanaco hf\n\n        # -- training args\n        output_dir=&quot;./results&quot;,\n        per_device_train_batch_size=4,\n        gradient_accumulation_steps=4,\n        # paging so that the sudden mem gpu spikes don't cause the run to shut down\n        # (I think usually caused by too long seqs)\n        # todo: why 32 bit opt?\n        # todo: paged nadamw opt?\n        optim=&quot;paged_adamw_32bit&quot;,\n        save_steps=10,\n        logging_steps=10,\n        learning_rate=2e-4,\n        max_grad_norm=0.3,\n        max_steps=500,\n        warmup_ratio=0.03,\n        lr_scheduler_type=&quot;constant&quot;,\n        # -- quant. args (not recommended to be changed unless you know what your doing?)\n        load_in_4bit=True,  # load (usually huge) base model in 4 bits\n        bnb_4bit_quant_type=&quot;nf4&quot;,  # normal float 4 for the (large) base models qlora\n) -&gt; tuple:\n    &quot;&quot;&quot;\n    Load the Falcon 7B model, quantize it in 4bit and attach LoRA adapters on it.\n\n    bf16 = 1S, 7Exp, 8Mantissa\n    hypothesis: 7b trained due to 6.7 emergence rumour, I still don't think emergence is real.\n    Notes:\n        - ft a model is very specific to the model, tokenizer and training scheme. Thus we return\n            - model, tokenizer, ft config (peft config), training args\n\n    ref:\n        - https://colab.research.google.com/drive/1DOi8MFv4SWN9NImVornZ7t6BgmLoPQO-#scrollTo=AjB0WAqFSzlD\n    &quot;&quot;&quot;\n    from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoTokenizer\n\n    # - Get bnb config for bit-4 base model (bnb lib for using 4bit qlora quantization techniques by tim dettmers)\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=load_in_4bit,  # load (usually huge) base model in 4 bits\n        bnb_4bit_quant_type=bnb_4bit_quant_type,  # normal float 4 for the (usually huge) base model\n        bnb_4bit_compute_dtype=bnb_4bit_compute_dtype,  # if you can, during computation use bf16\n    )\n\n    # - Get falcon 4bit model\n    # todo, where is this being saved &amp; how to download quicker\n    model = AutoModelForCausalLM.from_pretrained(\n        pretrained_model_name_or_path=pretrained_model_name_or_path,\n        quantization_config=bnb_config,\n        trust_remote_code=True  # allows to execute custom code you download from the uploaded model code you are using\n    )\n    print(f'{type(model)=}')\n    print(f'{model=}')\n    # this is here to save gpu vram. Likely only needed when using 40b or when oom issues happen ref: https://stackoverflow.com/questions/76633335/why-does-hugging-face-falcon-model-use-mode-config-use-cache-false-why-wouldn\n    model.config.use_cache = use_cache\n    print(f'{type(model)=}')\n\n    # - Get falcon tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path,\n                                              trust_remote_code=True)  # execs code downloaded from hf hub\n    # tokenizer.pad_token = tokenizer.eos_token  # ref: https://stackoverflow.com/questions/76633368/why-does-the-falcon-qlora-tutorial-code-use-eos-token-as-pad-token\n    # tokenizer.add_special_tokens({'pad_token': '[PAD]'})  # I think this is fine if during the training pad is ignored\n    tokenizer.add_special_tokens({'pad_token': '&lt;|pad|&gt;'})  # I think this is fine if during the training pad is ignored\n\n    # - Modify model\n    # add pad token embed\n    model.resize_token_embeddings(len(tokenizer))  # todo: I think this is fine if during the training pad is ignored\n    model.transformer.word_embeddings.padding_idx = len(tokenizer) - 1\n    model.config.max_new_tokens = len(tokenizer)\n    # model.config.min_length = 1\n    print(f'{model=}')\n    print(f'{type(tokenizer)=}')\n    print(f'{tokenizer.pad_token=}')\n    # data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False) todo\n\n    # - Get falcon lora config\n    peft_config = LoraConfig(\n        lora_alpha=lora_alpha,\n        lora_dropout=lora_dropout,\n        r=lora_r,\n        bias=&quot;none&quot;,\n        task_type=&quot;CAUSAL_LM&quot;,\n        # model card for falcon tiiuae/falcon-7b: https://huggingface.co/tiiuae/falcon-7b/blob/main/modelling_RW.py\n        # does seem to include all trainable params as done by qlora on their own paper\n        target_modules=[\n            # word_embeddings,\n            &quot;query_key_value&quot;,\n            &quot;dense&quot;,\n            &quot;dense_h_to_4h&quot;,\n            &quot;dense_4h_to_h&quot;,\n            # &quot;lm_head&quot;\n        ]\n    )\n    print(f'{type(peft_config)=}')\n\n    # todo: print the num params of the lora = D1*r + D2*r and num of bytes by prec. (bytes) * num params\n    return model, tokenizer, peft_config\n\n\n# -- tests\n\ndef example_test_model_already_has_pad_token():\n    &quot;&quot;&quot;\n    if it already has pad token, it likely has a small prob, so we are done.\n\n    compare it's norm with other tokens to verify this is true.\n\npython ~/ultimate-utils/ultimate-utils-proj-src/uutils/hf_uu/model_tokenizer/falcon_uu_mdl_tok.py\n    &quot;&quot;&quot;\n    # - the get datasets todo: preprocessing, padding, streaming\n    from uutils.hf_uu.data_hf.common import get_guanaco_datsets_add_splits_train_test_only\n    trainset, _, testset = get_guanaco_datsets_add_splits_train_test_only()\n\n    # qlora flacon7b\n    from uutils.hf_uu.model_tokenizer.falcon_uu_mdl_tok import get_model_tokenizer_qlora_falcon7b\n    model, tokenizer, peft_config = get_model_tokenizer_qlora_falcon7b()\n    model: PreTrainedModel = model\n    print(f'{model=}')\n    sent = 'Dogs are great because they are '\n    print()\n\n    # print to see if pad tokens are present and if it ignores the tokens at the end\n    encoded_input = tokenizer(sent, padding='max_length', max_length=10, return_tensors='pt')\n    print(f'{encoded_input=}')\n\n    # Print all special tokens\n    print('\\n---- start Print all special tokens')\n    for token_name, token in tokenizer.special_tokens_map.items():\n        print(f&quot;{token_name}: {token}&quot;)\n    print('\\n---- end Print all special tokens')\n\n    # Get the ID for the '[PAD]' token\n    try:\n        pad_token_id = tokenizer.convert_tokens_to_ids('[PAD]')\n    except KeyError:\n        raise ValueError(&quot;Token [PAD] is not present in the tokenizer vocabulary.&quot;)\n\n    # Index into the model's embedding table\n    try:\n        print(f'{model.get_input_embeddings().weight.size()=}')\n        pad_embedding = model.get_input_embeddings().weight[pad_token_id]\n    except IndexError:\n        raise ValueError(f&quot;Token ID {pad_token_id} is not present in the model's embedding matrix.&quot;)\n\n    print(f'{pad_embedding=}')\n    print('Success!\\n')\n\n    # check it generates something sensible\n    # tokenizer.decode(model.generate(**tokenizer(sent, return_tensors='pt'), do_sample=True)[0])\n    input_ids, attention_mask = encoded_input['input_ids'], encoded_input['attention_mask']\n    predicted_tokens_ids_options = model.generate(input_ids=input_ids, attention_mask=attention_mask, do_sample=True)\n    predicted_tokens_ids = predicted_tokens_ids_options[0]\n    predicted_sent = tokenizer.decode(predicted_tokens_ids)\n    print(f'original sentence: {sent=}')\n    print(f'predicted sentence: {predicted_sent=}')\n    print('Success2!')\n\n\nif __name__ == '__main__':\n    import time\n\n    start_time = time.time()\n    example_test_model_already_has_pad_token()\n    print(f&quot;The main function executed in {time.time() - start_time} seconds.\\a&quot;)\n</code></pre>\n<p>it doesn't like the modifications to the model:</p>\n<pre><code>    model.transformer.word_embeddings.padding_idx = len(tokenizer) - 1\n    model.config.max_new_tokens = len(tokenizer)\n</code></pre>\n<p>How to fix?</p>\n<p>Errors:</p>\n<pre><code>/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/transformers/generation/utils.py:1259: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n  warnings.warn(\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/transformers/generation/utils.py:1452: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n  warnings.warn(\nTraceback (most recent call last):\n  File &quot;/lfs/hyperturing1/0/brando9/ultimate-utils/ultimate-utils-proj-src/uutils/hf_uu/model_tokenizer/falcon_uu_mdl_tok.py&quot;, line 211, in &lt;module&gt;\n    example_test_model_already_has_pad_token()\n  File &quot;/lfs/hyperturing1/0/brando9/ultimate-utils/ultimate-utils-proj-src/uutils/hf_uu/model_tokenizer/falcon_uu_mdl_tok.py&quot;, line 199, in example_test_model_already_has_pad_token\n    predicted_tokens_ids_options = model.generate(input_ids=input_ids, attention_mask=attention_mask, do_sample=True)\n  File &quot;/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/torch/utils/_contextlib.py&quot;, line 115, in decorate_context\n    return func(*args, **kwargs)\n  File &quot;/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/transformers/generation/utils.py&quot;, line 1572, in generate\n    return self.sample(\n  File &quot;/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/transformers/generation/utils.py&quot;, line 2633, in sample\n    next_token_scores = logits_warper(input_ids, next_token_scores)\n  File &quot;/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/transformers/generation/logits_process.py&quot;, line 92, in __call__\n    scores = processor(input_ids, scores)\n  File &quot;/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/transformers/generation/logits_process.py&quot;, line 302, in __call__\n    indices_to_remove = scores &lt; torch.topk(scores, top_k)[0][..., -1, None]\nRuntimeError: &quot;topk_cpu&quot; not implemented for 'Half'\n</code></pre>\n<hr />\n<h2>Bounty Section: Small GPT2 code example</h2>\n<p>Yes I agree that pad is assigned to eos. Eos is still eos. But during fine-tuning now the weights wrt to eos are unchanged. This might be an issue since the probability of eos has not shifted to the fine-tuning regime. One possibility is that eos is outputed with less chance. Yes we can still halt production when we see eos but we've not shifted the probability to output eos according to our fine-tuning distribution -- but all other tokens have changed distribution. I think this could be an issue because it's not like the old probability of eos is conserved since all tokens probs have changed except eos + even if the old eos prob was conserved, it's wrt wrong distribution (not the fine tuning one).</p>\n<p>e.g.,</p>\n<pre><code>if tokenizer.pad_token_id is None:\n    tokenizer.pad_token = tokenizer.eos_token\n ...\nraw_text_batch='a'\ntokenize_batch={'input_ids': tensor([[   64, 50256, 50256, 50256, 50256]]), 'attention_mask': tensor([[1, 0, 0, 0, 0]])}\n</code></pre>\n<p>but it would have been better to have</p>\n<pre><code>tokenize_batch={'input_ids': tensor([[   64, 50256, 50256, 50256, 50256]]), 'attention_mask': tensor([[1, 1, 0, 0, 0]])}\n</code></pre>\n<p>code</p>\n<pre><code>def test_eos_pad():\n    from datasets import load_dataset\n    import torch\n    from transformers import GPT2Tokenizer, GPT2LMHeadModel\n\n    raw_text_batch = 'a'\n\n    tokenizer = GPT2Tokenizer.from_pretrained(&quot;gpt2&quot;)\n    # print(f'{tokenizer.eos_token=}')\n    # print(f'{tokenizer.eos_token_id=}')\n    # print(f'{tokenizer.pad_token=}')\n    # print(f'{tokenizer.pad_token_id=}')\n\n    # print(f'{raw_text_batch=}')\n    # tokenize_batch = tokenizer(raw_text_batch, padding=&quot;max_length&quot;, max_length=5, truncation=True, return_tensors=&quot;pt&quot;)\n    # print(f'{tokenize_batch=}')\n\n    if tokenizer.pad_token_id is None:\n        tokenizer.pad_token = tokenizer.eos_token\n    probe_network = GPT2LMHeadModel.from_pretrained(&quot;gpt2&quot;)\n    device = torch.device(f&quot;cuda:{0}&quot; if torch.cuda.is_available() else &quot;cpu&quot;)\n    probe_network = probe_network.to(device)\n\n    print(f'{tokenizer.eos_token=}')\n    print(f'{tokenizer.eos_token_id=}')\n    print(f'{tokenizer.pad_token=}')\n    print(f'{tokenizer.pad_token_id=}')\n\n    print(f'{raw_text_batch=}')\n    tokenize_batch = tokenizer(raw_text_batch, padding=&quot;max_length&quot;, max_length=5, truncation=True, return_tensors=&quot;pt&quot;)\n    print(f'{tokenize_batch=}')\n    print('Done')\n</code></pre>\n<hr />\n<p>cross:</p>\n<ul>\n<li>hf discuss forum: <a href=\"https://discuss.huggingface.co/t/why-does-the-falcon-qlora-tutorial-code-use-eos-token-as-pad-token/45954\" rel=\"nofollow noreferrer\">https://discuss.huggingface.co/t/why-does-the-falcon-qlora-tutorial-code-use-eos-token-as-pad-token/45954</a></li>\n<li>pytorch forum discuss: <a href=\"https://discuss.pytorch.org/t/how-does-one-set-the-pad-token-correctly-not-to-eos-during-fine-tuning-to-avoid-model-not-predicting-eos/213619\" rel=\"nofollow noreferrer\">https://discuss.pytorch.org/t/how-does-one-set-the-pad-token-correctly-not-to-eos-during-fine-tuning-to-avoid-model-not-predicting-eos/213619</a></li>\n<li><a href=\"https://discord.com/channels/879548962464493619/1126681170957045770/1126681170957045770\" rel=\"nofollow noreferrer\">https://discord.com/channels/879548962464493619/1126681170957045770/1126681170957045770</a></li>\n<li>context peft pacman100 code: <a href=\"https://gist.github.com/pacman100/1731b41f7a90a87b457e8c5415ff1c14\" rel=\"nofollow noreferrer\">https://gist.github.com/pacman100/1731b41f7a90a87b457e8c5415ff1c14</a></li>\n<li>twitter tweet of this: <a href=\"https://twitter.com/BrandoHablando/status/1693676898013061337?s=20\" rel=\"nofollow noreferrer\">https://twitter.com/BrandoHablando/status/1693676898013061337?s=20</a></li>\n</ul>\n",
        "answer": "<h1>Update August/8/2024</h1>\n<p>One more improvement. Some models like DeepSeekCoder base 7B do have a pad token already. So no need to set the pad token to eos. But the code that pads up to 1st occurence of eos + pads the rest has to pad the rest <strong>assuming they are pad tokens</strong>. So that's the diff:</p>\n<pre class=\"lang-py prettyprint-override\"><code>def get_lm_examples_1st_eos_mask_remaining_eos(\n        examples,\n        tokenizer: AutoTokenizer, \n        \n        # desired_dataset_column: str = 'text',\n        # method_to_remove_columns: str = 'keys',\n\n        remove_to_long_seqs: bool = False,\n        # format: str = 'torch',\n        debug: bool = False,\n        ) -&gt; dict[str, torch.Tensor]:\n    &quot;&quot;&quot; \n    Train only on first occurence of eos. The remaining eos are masked out. If \n    - train up to 1st ocurrence of eos token, mask out the rest of the eos tokens.\n    - drop or not seqs that are too long, i.e., have no eos token.\n    \n    Assumes: pad == eos\n\n    ref: https://stackoverflow.com/questions/76633368/how-does-one-set-the-pad-token-correctly-not-to-eos-during-fine-tuning-to-avoi\n    &quot;&quot;&quot;\n    # - Get lm example\n    seq_length: int = examples['input_ids'].size(0)\n    print(f'{examples[&quot;input_ids&quot;].size()=}, {seq_length=}') if debug else None\n    examples[&quot;labels&quot;] = examples[&quot;input_ids&quot;].clone()  # labels is hardcoded in HF so put it!\n    eos_token_id = tokenizer.eos_token_id\n    # assert eos_token_id == tokenizer.pad_token_id, 'Error: pad should be eos token'\n    print(f'{tokenizer.pad_token_id=}, {tokenizer.eos_token_id=}') if debug else None\n    seqs_to_drop: list[int] = [] # store idx to drop (to long), we don't want to modify the two lists at the same time as we are looping through them\n    for idx, input_ids in enumerate(examples[&quot;input_ids&quot;]):\n        # Find all occurrences of eos_token\n        eos_positions = (input_ids == eos_token_id).nonzero(as_tuple=True)[0]\n        if eos_positions.nelement() &gt; 0:  # Check if eos_token is present --&gt; if yes then make sure to trian on it then mask the remaining eos (assumes pad == eos)\n            first_eos_position = eos_positions[0]\n            examples[&quot;attention_mask&quot;][idx, first_eos_position] = 1  # Set the mask value to 1\n            # Assert that the label for the first occurrence of eos_token is eos_token_id\n            assert examples[&quot;labels&quot;][idx, first_eos_position] == eos_token_id, &quot;The label for the first eos_token is incorrect!&quot;\n            # # For all subsequent occurrences of eos_token, set their labels to -100\n            # for subsequent_eos_position in eos_positions[1:]:\n            #     examples[&quot;labels&quot;][idx, subsequent_eos_position] = -100\n            #     assert examples[&quot;labels&quot;][idx, subsequent_eos_position] == -100, &quot;The label for the subsequent_eos_position incorrect! Should be -100.&quot;\n            # after first eos token mask everything (eos AND pad, hopefully that's all there but we can sanity check later)\n            for desired_mask_idx in range(first_eos_position, seq_length):\n                examples[&quot;labels&quot;][idx, desired_mask_idx] = -100\n                assert examples[&quot;labels&quot;][idx, desired_mask_idx] == -100, &quot;The label for the desired_mask_idx incorrect! Should be -100.&quot;\n        elif remove_to_long_seqs:\n            assert eos_positions.nelement() == 0, 'Error: there should be no eos if this if stmt is exexuted.'\n            # record to drop this seq, has no eos so too long + flag says to drop it\n            seqs_to_drop.append(idx)\n        else:\n            pass # nop: no eos in seq so too long, but keep it for training anyway\n    # assert len(examples[&quot;labels&quot;]) == 0, 'Error: no labels were set'\n    # -- Drop seqs with no eos\n    if seqs_to_drop:\n        examples[&quot;input_ids&quot;] = torch.stack([input_ids for idx, input_ids in enumerate(examples[&quot;input_ids&quot;]) if idx not in seqs_to_drop])\n        examples[&quot;attention_mask&quot;] = torch.stack([mask for idx, mask in enumerate(examples[&quot;attention_mask&quot;]) if idx not in seqs_to_drop])\n        examples[&quot;labels&quot;] = torch.stack([labels for idx, labels in enumerate(examples[&quot;labels&quot;]) if idx not in seqs_to_drop])\n    return examples\n</code></pre>\n<hr />\n<h1>Update, I made this better. Now if seq does not have eos at all you can remove that seq or chose to train on it</h1>\n<pre><code>def raw_ds_2_lm_ds_mask_eos_pad_toks(\n        raw_dataset, \n        tokenizer, \n        max_length: int,\n\n        raw_str_2_desired_str: Optional[callable] = None, # either return {'text': examples['text']} or preprocess str to get what you need e.g. {'text': f&quot;[ex['nl'] ex['fl'] {tok.eos_token}]&quot; for ex in examples}\n        desired_dataset_column: str = 'text', # good val to use if hf str ds already pre-processed for you: 'text',\n\n        method_to_remove_columns: str = 'keys',\n\n        padding: str = 'max_length',\n        truncation: bool = True, \n        return_tensors: str = 'pt',\n\n        batched: bool = True, # Setting `batched=True` in the `dataset.map` function of Hugging Face's datasets library processes the data in batches rather than one item at a time, significantly speeding up the tokenization and preprocessing steps.\n        streaming: bool = False,\n\n        format: str = 'torch',\n        # get_lm_examples_function = get_lm_examples_1st_eos_mask_remaining_eos,\n        ):\n    &quot;&quot;&quot; &quot;&quot;&quot;\n    # - Get desired str dataset\n    if raw_str_2_desired_str is None:\n        get_desired_examples_str_function = lambda examples: {'text': examples[desired_dataset_column]} if raw_str_2_desired_str is not None else raw_str_2_desired_str \n    else:\n        get_desired_examples_str_function = raw_str_2_desired_str\n    desired_examples_str_dataset = raw_dataset.map(get_desired_examples_str_function, batched=batched) # note: we can't remove all str columns here or we will remove the ones we want to tokenize by accident\n\n    # - Get tokenized data set\n    desired_examples_str_dataset = desired_examples_str_dataset.with_format(format)  # annoying that return tensors in the tokenizer on it's own doesn't put it into a pt tensor, so for now we keep both.\n    remove_str_columns = get_column_names(desired_examples_str_dataset, streaming, method_to_remove_columns)  # remove all keys that are not tensors to avoid bugs in collate function in task2vec's pytorch data loader\n    tokenize_function = lambda examples: tokenizer(examples[desired_dataset_column], padding=padding, max_length=max_length, truncation=truncation, return_tensors=return_tensors)\n    tokenized_datasets = desired_examples_str_dataset.map(tokenize_function, batched=batched, remove_columns=remove_str_columns)\n\n    # - Get lm data set\n    # get_lm_examples_function = lambda examples : group_texts(examples, block_size)\n    get_lm_examples_function = lambda examples : get_lm_examples_1st_eos_mask_remaining_eos(examples, tokenizer)\n    lm_dataset = tokenized_datasets.map(get_lm_examples_function, batched=batched)\n    return lm_dataset\n\ndef get_lm_examples_1st_eos_mask_remaining_eos(\n        examples,\n        tokenizer: AutoTokenizer, \n        \n        # desired_dataset_column: str = 'text',\n        # method_to_remove_columns: str = 'keys',\n\n        remove_to_long_seqs: bool = False,\n        # format: str = 'torch',\n        ) -&gt; dict[str, torch.Tensor]:\n    &quot;&quot;&quot; \n    Train only on first occurence of eos. The remaining eos are masked out. If \n    - train up to 1st ocurrence of eos token, mask out the rest of the eos tokens.\n    - drop or not seqs that are too long, i.e., have no eos token.\n    \n    Assumes: pad == eos\n\n    ref: https://stackoverflow.com/questions/76633368/how-does-one-set-the-pad-token-correctly-not-to-eos-during-fine-tuning-to-avoi\n    &quot;&quot;&quot;\n    # - Get lm example\n    examples[&quot;labels&quot;] = examples[&quot;input_ids&quot;].clone()  # labels is hardcoded in HF so put it!\n    eos_token_id = tokenizer.eos_token_id\n    assert eos_token_id == tokenizer.pad_token_id, 'Error: pad should be eos token'\n    seqs_to_drop: list[int] = [] # store idx to drop (to long), we don't want to modify the two lists at the same time as we are looping through them\n    for idx, input_ids in enumerate(examples[&quot;input_ids&quot;]):\n        # Find all occurrences of eos_token\n        eos_positions = (input_ids == eos_token_id).nonzero(as_tuple=True)[0]\n        if eos_positions.nelement() &gt; 0:  # Check if eos_token is present --&gt; if yes then make sure to trian on it then mask the remaining eos (assumes pad == eos)\n            first_eos_position = eos_positions[0]\n            examples[&quot;attention_mask&quot;][idx, first_eos_position] = 1  # Set the mask value to 1\n            # Assert that the label for the first occurrence of eos_token is eos_token_id\n            assert examples[&quot;labels&quot;][idx, first_eos_position] == eos_token_id, &quot;The label for the first eos_token is incorrect!&quot;\n            # For all subsequent occurrences of eos_token, set their labels to -100\n            for subsequent_eos_position in eos_positions[1:]:\n                examples[&quot;labels&quot;][idx, subsequent_eos_position] = -100\n                assert examples[&quot;labels&quot;][idx, subsequent_eos_position] == -100, &quot;The label for the subsequent_eos_position incorrect! Should be -100.&quot;\n        elif remove_to_long_seqs:\n            assert eos_positions.nelement() == 0, 'Error: there should be no eos if this if stmt is exexuted.'\n            # record to drop this seq, has no eos so too long + flag says to drop it\n            seqs_to_drop.append(idx)\n        else:\n            pass # nop: no eos in seq so too long, but keep it for training anyway\n    # assert len(examples[&quot;labels&quot;]) == 0, 'Error: no labels were set'\n    # -- Drop seqs with no eos\n    if seqs_to_drop:\n        examples[&quot;input_ids&quot;] = torch.stack([input_ids for idx, input_ids in enumerate(examples[&quot;input_ids&quot;]) if idx not in seqs_to_drop])\n        examples[&quot;attention_mask&quot;] = torch.stack([mask for idx, mask in enumerate(examples[&quot;attention_mask&quot;]) if idx not in seqs_to_drop])\n        examples[&quot;labels&quot;] = torch.stack([labels for idx, labels in enumerate(examples[&quot;labels&quot;]) if idx not in seqs_to_drop])\n    return examples\n</code></pre>\n<p>train script:</p>\n<pre><code>&quot;&quot;&quot;\nRefs:\n    - https://claude.ai/chat/ad5c9e18-beb4-48fb-9f43-a2ba463ce158\n    - https://chatgpt.com/c/349f2c8a-949e-444d-ae3c-8ca60ba77831\n&quot;&quot;&quot;\nimport glob\nimport os\nimport numpy as np\nimport torch\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel, TrainingArguments, Trainer\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom datasets import load_dataset, load_metric\nfrom typing import Dict, Tuple, Optional\nfrom pathlib import Path\nimport evaluate\n\nfrom utils import eval_hf\nfrom utils import raw_ds_2_lm_ds_mask_eos_pad_toks\n\ndef compute_metrics(eval_pred: Tuple[np.ndarray, np.ndarray],\n                    path: str = 'accuracy',\n                    ) -&gt; Dict[str, float]:\n    &quot;&quot;&quot;\n    Compute the accuracy of the model.\n\n    Args:\n    eval_pred: A tuple containing the model predictions and labels.\n\n    Returns:\n    A dictionary with the accuracy score.\n    \n    TODO: document properly what accuracy is. Is it tfa, ara, exact string match, avg acc (wrt length etc.) ref: https://huggingface.co/spaces/evaluate-metric/accuracy\n    &quot;&quot;&quot;\n    metric = evaluate.load(path=path)   # load metric from file or hf\n    predictions, references = eval_pred\n    predictions = np.argmax(predictions, axis=1)\n    return metric.compute(predictions=predictions, references=references)\n\ndef preprocess_function_proofnet_simple(examples: Dict[str, list], tokenizer: GPT2Tokenizer, max_length: int = 512) -&gt; Dict[str, torch.Tensor]:\n    &quot;&quot;&quot;\n    Preprocess the input data for the proofnet dataset.\n\n    Args:\n    examples: The examples to preprocess.\n    tokenizer: The tokenizer for encoding the texts.\n\n    Returns:\n    The processed model inputs.\n    &quot;&quot;&quot;\n    # - Get raw string ins,outs (so deal with HF data set columns at str level)\n    inputs: list[str] = [f&quot;{examples['nl_statement'][i]}{tokenizer.eos_token}{examples['formal_statement'][i]}&quot; for i in range(len(examples['nl_statement']))]\n    # - Get tokenized ins,outs (so remove irrelevant &quot;string&quot; columns to get only &quot;tensor&quot; relevant columns)\n    model_inputs = tokenizer(inputs, max_length=max_length, padding=&quot;max_length&quot;, truncation=True, return_tensors=&quot;pt&quot;)\n    # - Get lm ins,outs for training e.g., deal with padd, masks etc.\n    labels = model_inputs.input_ids.clone()\n    labels[labels == tokenizer.pad_token_id] = -100\n    model_inputs[&quot;labels&quot;] = labels\n    return model_inputs\n\ndef setup_and_train_proofnet(\n        # pretrained_model_name_or_path: str = &quot;gpt2&quot;, \n        # pretrained_model_name_or_path: str = &quot;openai-community/gpt2-xl&quot;, \n        pretrained_model_name_or_path: str = &quot;meta-llama/Meta-Llama-3.1-8B&quot;, \n        path: str = &quot;hoskinson-center/proofnet&quot;,\n        output_dir_train: str = '~/tmp/proofnet/train',\n        output_dir_val: Optional[str] = None,  # we are training on the val set so no val set\n        output_dir_test: str = '~/tmp/proofnet/test',\n        path_to_save_model: Optional[str] = None,  # suggested path: '~/tmp/proofnet/model' then expanduser in py code\n        num_train_epochs: int = 3,\n        per_device_train_batch_size: Optional[int] = 2,\n        per_device_eval_batch_size: Optional[int] = 2,\n        learning_rate: float = 5e-5,\n        weight_decay: float = 0.01,\n        max_grad_norm: float = 1.0, \n        lr_scheduler_type = 'cosine', # https://discord.com/channels/879548962464493619/1227708244697284724/1227708244697284724\n        warmup_ratio=0.01,  # copying alpaca for now, number of steps for a linear warmup,  https://discord.com/channels/879548962464493619/1227708244697284724/1227708244697284724\n        optim='paged_adamw_32bit',\n        gradient_accumulation_steps = 2, # Allows to process effective_batch_size = gradient_accumulation_steps * batch_size, num its to accumulate before opt update step\n        gradient_checkpointing: Optional[bool] = True,\n        report_to: str = 'none',  # recommended values 'wandb' or `none`\n        ) -&gt; None:\n    &quot;&quot;&quot;\n    Set up the environment, preprocess the dataset, and train the model.\n\n    export CUDA_VISIBLE_DEVICES=7\n\n    Args:\n    tokenizer_name: The name of the tokenizer.\n    model_name: The name of the model.\n    dataset_path: The path to the dataset.\n    &quot;&quot;&quot;\n    # Clear CUDA cache to free up memory\n    torch.cuda.empty_cache()\n\n    # Load tokenizer and model\n    if pretrained_model_name_or_path == &quot;gpt2&quot;:\n        tokenizer = GPT2Tokenizer.from_pretrained(pretrained_model_name_or_path, max_length=1024)\n        if tokenizer.pad_token_id is None:\n            tokenizer.pad_token = tokenizer.eos_token\n            print(f'{tokenizer.pad_token=}')\n        print(f'{tokenizer.eos_token=}\\n{tokenizer.eos_token_id=}')\n        model = GPT2LMHeadModel.from_pretrained(pretrained_model_name_or_path)\n        device = torch.device(f&quot;cuda:{0}&quot; if torch.cuda.is_available() else &quot;cpu&quot;)\n        model = model.to(device)\n        max_length: int = tokenizer.model_max_length\n        print(f'{max_length=}')\n    elif pretrained_model_name_or_path == &quot;openai-community/gpt2-xl&quot;:\n        tokenizer = GPT2Tokenizer.from_pretrained(pretrained_model_name_or_path, max_length=1024)\n        if tokenizer.pad_token_id is None:\n            tokenizer.pad_token = tokenizer.eos_token\n            print(f'{tokenizer.pad_token=}')\n        print(f'{tokenizer.eos_token=}\\n{tokenizer.eos_token_id=}')\n        model = GPT2LMHeadModel.from_pretrained(pretrained_model_name_or_path)\n        device = torch.device(f&quot;cuda:{0}&quot; if torch.cuda.is_available() else &quot;cpu&quot;)\n        model = model.to(device)\n        max_length: int = tokenizer.model_max_length\n        print(f'{max_length=}') \n    elif pretrained_model_name_or_path == &quot;meta-llama/Meta-Llama-3.1-8B&quot;:\n        torch_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float32 \n        model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, torch_dtype=torch_dtype)\n        # tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path, padding_side=&quot;right&quot;, use_auth_token=True)\n        tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path, padding_side=&quot;right&quot;)\n        print(f'{tokenizer.pad_token=} {tokenizer.eos_token_id=}')\n        tokenizer.pad_token = tokenizer.eos_token if tokenizer.pad_token_id is None else tokenizer.pad_token\n        print(f'{tokenizer.pad_token=} {tokenizer.eos_token_id=}')\n        # get context length for setting max length for training\n        if hasattr(model.config, &quot;context_length&quot;):\n            # SEEMS IT IS NOT IN THE model.config\n            print(&quot;Context length:&quot;, model.config.context_length)\n            max_length: int = model.config.context_length\n        else:\n            print(f&quot;Context length not found in model.config, so using your default or hardcoded value. Model is {pretrained_model_name_or_path=}.&quot;)\n            # max_length: int = 4096\n            max_length: int = 8192\n            # max_length: int = 128  # for debugging\n            # max_length: int = 128_000  # ref: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B\n            print(f'-&gt;{max_length=}')\n    else:\n        raise ValueError(f&quot;Model {pretrained_model_name_or_path} not supported.&quot;)\n    print(&quot;Number of parameters:&quot;, sum(p.numel() for p in model.parameters()))\n\n    # - Load the dataset\n    print(f'-Load the dataset')\n    ## Proofnet\n    # dataset_val = load_dataset(path, split='validation')\n    # dataset_test = load_dataset(path, split='test')\n    # # Preprocess the dataset\n    # if path == &quot;hoskinson-center/proofnet&quot;:\n    #     preprocess_function = preprocess_function_proofnet_simple\n    #     # note: text field is usually more common!\n    #     val_dataset = dataset_val.map(lambda examples: preprocess_function(examples, tokenizer), batched=True, remove_columns=[&quot;nl_statement&quot;, &quot;formal_statement&quot;])\n    #     test_dataset = dataset_test.map(lambda examples: preprocess_function(examples, tokenizer), batched=True, remove_columns=[&quot;nl_statement&quot;, &quot;formal_statement&quot;])\n    ## C4\n    # train_dataset = load_dataset(path='allenai/c4', name='en', split='train', streaming=True)\n    # eval_dataset = load_dataset(path='allenai/c4', name='en', split='validation', streaming=True)\n    # train_dataset = raw_ds_2_lm_ds_mask_eos_pad_toks(train_dataset, tokenizer, max_length)\n    # eval_dataset = raw_ds_2_lm_ds_mask_eos_pad_toks(eval_dataset, tokenizer, max_length)\n\n    # json files for putnam are not consistent and it seems they have to be: https://chatgpt.com/c/9cecca7d-d50d-42e2-b2d3-c1057bc21ef2 solve later\n    # ~/putnam-math/data/Putnam_MATH_variations_static3/original/test\n    # json_files = glob.glob(os.path.expanduser('~/putnam-math/data/Putnam_MATH_original_static3/test/**/*.json'), recursive=True)\n    # train_dataset = load_dataset('json', data_files=json_files)\n    # json_files = glob.glob(os.path.expanduser('~/putnam-math/data/Putnam_MATH_variations_static3/variations/test/**/*.json'), recursive=True)\n    # eval_dataset = load_dataset('json', data_files=json_files)\n    # train_dataset = raw_ds_2_lm_ds_mask_eos_pad_toks(train_dataset, tokenizer, max_length)\n    # eval_dataset = raw_ds_2_lm_ds_mask_eos_pad_toks(eval_dataset, tokenizer, max_length)\n\n    # Proofnet with 1st eos token train remaining eos not train\n    from train.utils import raw_str_2_desired_af_str\n    _raw_str_2_desired_af_str = lambda examples: raw_str_2_desired_af_str(examples, tokenizer)  # tokenizer needed to get eos tok to form right str to train on.\n    train_dataset = load_dataset(path, split='validation')\n    eval_dataset = load_dataset(path, split='test')\n    train_dataset = raw_ds_2_lm_ds_mask_eos_pad_toks(train_dataset, tokenizer, max_length, raw_str_2_desired_str=_raw_str_2_desired_af_str)\n    eval_dataset = train_dataset\n    print(f'-&gt;{len(train_dataset)=} {len(eval_dataset)=}')\n    # max_steps: int = (len(train_dataset) * num_train_epochs) // per_device_train_batch_size  # TODO: really?\n\n    # Training arguments\n    output_dir_train: Path = Path(output_dir_train).expanduser()\n    output_dir_train.mkdir(parents=True, exist_ok=True)\n    training_args = TrainingArguments(\n        output_dir=output_dir_train,\n        max_steps=2,  # TODO get rid of this in favour of 1 or 2 or 3 epochs\n        # num_train_epochs=num_train_epochs, \n        gradient_accumulation_steps=gradient_accumulation_steps,  # based on alpaca https://github.com/tatsu-lab/stanford_alpaca, allows to process effective_batch_size = gradient_accumulation_steps * batch_size, num its to accumulate before opt update step\n        gradient_checkpointing = gradient_checkpointing,  # TODO depending on hardware set to true?\n        per_device_train_batch_size=per_device_train_batch_size,\n        per_device_eval_batch_size=per_device_eval_batch_size,\n        learning_rate=learning_rate,\n        weight_decay=weight_decay, \n        max_grad_norm=max_grad_norm, # TODO once real training change?\n        lr_scheduler_type=lr_scheduler_type,  # TODO once real training change? using what I've seen most in vision \n        warmup_ratio=warmup_ratio,\n        optim=optim,\n        # logging_strategy='epoch', # TODO\n        save_steps=100, # Save checkpoint every 500 steps\n        save_total_limit=3, # save last 3\n        logging_steps=10,  # Frequency of logging steps\n        logging_first_step=True,\n        logging_dir=output_dir_train,\n        evaluation_strategy='no',  # &quot;no&quot;`: No evaluation is done during training. no can be good to avoid memory issues.\n        # evaluation_strategy=&quot;steps&quot;,  # TODO Evaluate model at specified steps\n        # eval_steps=110,  # TODO Evaluate every 100 steps\n        # remove_unused_columns=False,  # TODO https://stackoverflow.com/questions/76879872/how-to-use-huggingface-hf-trainer-train-with-custom-collate-function/76929999#76929999 , https://claude.ai/chat/475a4638-cee3-4ce0-af64-c8b8d1dc0d90\n        report_to=report_to,  # options I recommend: 'none', 'wandb'\n        fp16=False,  # never ever set to True\n        bf16=torch.cuda.is_bf16_supported(),\n        # full_determinism=True,  # TODO periphery, Ensure reproducibility\n        # torchdynamo=&quot;nvfuser&quot;,  # TODO periphery, Use NVFuser backend for optimized torch operations\n        # dataloader_prefetch_factor=2,  # TODO periphery, Number of batches to prefetch\n        # dataloader_pin_memory=True,  # TODO periphery, Pin memory in data loaders for faster transfer to GPU\n        # dataloader_num_workers=16,  # TODO Number of subprocesses for data loading\n    )\n\n    # Initialize the Trainer \n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset,  # set to None if eval is giving you memory issues\n        tokenizer=tokenizer,\n        compute_metrics=compute_metrics\n    )\n    # Train the model\n    trainer.train()\n\n    # Evaluate the model\n    if output_dir_test is not None:\n        output_dir_test: Path = Path(output_dir_test).expanduser()\n        output_dir_test.mkdir(parents=True, exist_ok=True)\n        eval_args = TrainingArguments(output_dir=output_dir_test, per_device_eval_batch_size=per_device_eval_batch_size, fp16=False, bf16=torch.cuda.is_bf16_supported(), report_to=report_to)\n        trainer = Trainer(model=model, args=eval_args, train_dataset=None, eval_dataset=eval_dataset)\n        # results: dict[str, float] = trainer.evaluate(test_dataset)\n        results: dict[str, float] = eval_hf(trainer, name='', path=path, split='test')\n        print(f'{path=} split=test {results=}')\n\n    # Save the trained model\n    if path_to_save_model is not None:\n        model.save_pretrained(path_to_save_model)\n\ndef main() -&gt; None:\n    &quot;&quot;&quot;\n    Main function to execute the model training and evaluation.\n    &quot;&quot;&quot;\n    setup_and_train_proofnet()\n\nif __name__ == &quot;__main__&quot;:\n    import time\n    start_time = time.time()\n    main()\n    print(f&quot;Time taken: {time.time() - start_time:.2f} seconds, or {(time.time() - start_time) / 60:.2f} minutes, or {(time.time() - start_time) / 3600:.2f} hours.\\a&quot;)\n</code></pre>\n<p>code repo: <a href=\"https://github.com/brando90/snap-cluster-setup/blob/9778140d8eb378f7c7873ec3fa906d0b01064031/py_src/train/simple_train2.py#L1\" rel=\"nofollow noreferrer\">https://github.com/brando90/snap-cluster-setup/blob/9778140d8eb378f7c7873ec3fa906d0b01064031/py_src/train/simple_train2.py#L1</a></p>\n<hr />\n<p>OK I think this is the code that train on first occurrence of <code>eos</code> and makes sure the rest are NOT trained on (feedback welcomed):</p>\n<pre class=\"lang-py prettyprint-override\"><code>def collate_fn_train_only_first_eos_token_mask_everything_after_it(data: list[dict[str, str]], \n                                                                    tokenizer: PreTrainedTokenizer, \n                                                                    max_length: int=1024,  # GPT2 default, likely worth you change it! This default might cause bugs.\n                                                                    ) -&gt; dict[str, torch.Tensor]:\n    &quot;&quot;&quot; Train only on first occurence of eos. The remaining eos are masked out.\n\n    Sometimes the model might not have a padding token. Sometimes people set the padding token to be the eos token.\n    But sometimes this seems to lead to the model to predict eos token to much. \n    So instead of actually using the pad token that was set to the eos token, we instead mask out all excesive eos tokens that act as pads \n    and leave the first eos token at the end to be predicted -- since that is the only one that semantically means end of sequence \n    and therby by not training on random eos at the end by masking it not unncesserily shift/amplify the distribution of eos. \n    \n    ref: https://discuss.huggingface.co/t/why-does-the-falcon-qlora-tutorial-code-use-eos-token-as-pad-token/45954/13?u=brando \n    ref: https://chat.openai.com/share/02d16770-a1f3-4bf4-8fc2-464286daa8a1\n    ref: https://claude.ai/chat/80565d1f-ece3-4fad-87df-364ce57aec15 on when to call .clone()\n    &quot;&quot;&quot;\n    # we are training full context length for llama so remove code bellow, if it tries to pad hopefully it throws an error\n    # -- Ensure tokenizer has a padding token\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n    # -- Extract sequences\n    # sequences: list[str] = [example.get(&quot;text&quot;, &quot;&quot;) or &quot;&quot; for example in data]\n    sequences: list[str] = []\n    for idx, example in enumerate(data):\n        # Retrieve the value for &quot;text&quot; from the dictionary or default to an empty string if not present or falsy. ref: https://chat.openai.com/share/bead51fe-2acf-4f05-b8f7-b849134bbfd4\n        text: str = example.get(&quot;text&quot;, &quot;&quot;) or &quot;&quot;\n        sequences.append(text)\n    # -- Tokenize the sequences\n    tokenized_data = tokenizer(sequences, padding=&quot;max_length&quot;, max_length=max_length, truncation=True, return_tensors=&quot;pt&quot;)\n    tokenized_data[&quot;labels&quot;] = tokenized_data[&quot;input_ids&quot;].clone()  # labels is hardcoded in HF so put it!\n    # -- Set the mask value for the first eos_token in each sequence to 1 and remaining to -100\n    eos_token_id = tokenizer.eos_token_id\n    for idx, input_ids in enumerate(tokenized_data[&quot;input_ids&quot;]):\n        # Find all occurrences of eos_token\n        eos_positions = (input_ids == eos_token_id).nonzero(as_tuple=True)[0]\n        if eos_positions.nelement() &gt; 0:  # Check if eos_token is present\n            first_eos_position = eos_positions[0]\n            tokenized_data[&quot;attention_mask&quot;][idx, first_eos_position] = 1  # Set the mask value to 1\n            \n            # Assert that the label for the first occurrence of eos_token is eos_token_id\n            assert tokenized_data[&quot;labels&quot;][idx, first_eos_position] == eos_token_id, &quot;The label for the first eos_token is incorrect!&quot;\n            \n            # For all subsequent occurrences of eos_token, set their labels to -100\n            for subsequent_eos_position in eos_positions[1:]:\n                tokenized_data[&quot;labels&quot;][idx, subsequent_eos_position] = -100\n                assert tokenized_data[&quot;labels&quot;][idx, subsequent_eos_position] == -100, &quot;The label for the subsequent_eos_position incorrect! Should be -100.&quot;\n    return tokenized_data\n</code></pre>\n<p>reference: <a href=\"https://discuss.huggingface.co/t/why-does-the-falcon-qlora-tutorial-code-use-eos-token-as-pad-token/45954/13?u=brando\" rel=\"nofollow noreferrer\">Why does the falcon QLoRA tutorial code use eos_token as pad_token?</a></p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 76633368,
            "link": "https://stackoverflow.com/questions/76633368/how-does-one-set-the-pad-token-correctly-not-to-eos-during-fine-tuning-to-avoi"
        }
    },
    {
        "question": "Get attention masks from HF pipelines\n<p>How should returned attention masks be accessed from the FeatureExtractionPipeline in Huggingface?</p>\n<p>The code below takes an embedding model, distributes it and a huggingface dataset across 8 GPUs on a single node, and performs inference on the inputs. The code requires the attention masks for mean pooling.</p>\n<p>Code example:</p>\n<pre class=\"lang-py prettyprint-override\"><code>from accelerate import Accelerator\nfrom accelerate.utils import tqdm\nfrom transformers import AutoTokenizer, AutoModel\nfrom optimum.bettertransformer import BetterTransformer\n\nimport torch\n\nfrom datasets import load_dataset\n\nfrom transformers import pipeline\n\naccelerator = Accelerator()\n\nmodel_name = &quot;BAAI/bge-large-en-v1.5&quot;\n\ntokenizer = AutoTokenizer.from_pretrained(model_name,)\n\nmodel = AutoModel.from_pretrained(model_name,)\n\npipe = pipeline(\n    &quot;feature-extraction&quot;,\n    model=model,\n    tokenizer=tokenizer,\n    max_length=512,\n    truncation=True,\n    padding=True,\n    pad_to_max_length=True,\n    batch_size=256,\n    framework=&quot;pt&quot;,\n    return_tensors=True,\n    return_attention_mask=True,\n    device=(accelerator.device)\n)\n\ndataset = load_dataset(\n    &quot;wikitext&quot;,\n    &quot;wikitext-2-v1&quot;,\n    split=&quot;train&quot;,\n)\n\n#Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n\n# Assume 8 processes\n\nwith accelerator.split_between_processes(dataset[&quot;text&quot;]) as data:\n\n    for out in pipe(data):\n\n        sentence_embeddings = mean_pooling(out, out[&quot;attention_mask&quot;])\n</code></pre>\n<p>I need the attention maks from pipe to use for mean pooling.</p>\n<p>Best,</p>\n<p>Enrico</p>\n",
        "answer": "<p>The <code>pipeline</code> object from the <code>transformers</code> library provides a convenient abstraction for quick inference of models, but for more customized solutions it's usually a good idea to use the models directly. For example:</p>\n<pre><code>text = 'This is a test.'\n\ntokenized = tokenizer(\n    text,\n    max_length=512,\n    truncation=True,\n    padding=True,\n    return_attention_mask=True,\n    return_tensors='pt').to(accelerator.device)\n\nout = model(**tokenized)\n\nembeddings = out.last_hidden_state\nattention_mask = tokenized['attention_mask']\n</code></pre>\n<p>You can then use the <code>embeddings</code> and <code>attention_mask</code> to compute the mean pooling. You may also consider using <code>out.pooler_output</code> instead of computing the mean pooling manually, however, I am not sure how the <code>pooler_output</code> is computed in this case, so be wary.</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 77603219,
            "link": "https://stackoverflow.com/questions/77603219/get-attention-masks-from-hf-pipelines"
        }
    },
    {
        "question": "Incomplete Output with LLM with max_new_tokens\n<p>I am experimenting with Huggingface LLM models.</p>\n<p>And one issue I noticed is that output of the model ends abruptly and I ideally want it to complete the paragraph/sentences/code which it was it between of. (or altogether try to complete the answer within some fixed num of tokens)</p>\n<p>Although I have provided max_new_tokens = 300 and also in prompt I write:\n&quot;Output should be maximum of 300 words.&quot;</p>\n<p>The response is always incomplete and ends abruptly. Any way I can ask for a complete output within desired number of output tokens?</p>\n<p>Code:</p>\n<pre><code>checkpoint = &quot;HuggingFaceH4/starchat-alpha&quot;\ndevice = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot; \nclass StarCoderModel:\n  def __init__(self):\n    self.tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n    # make sure `--gpus all` is provided in docker run command if gpu is required\n    self.model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map='auto')\n\n  def infer(self, input_text, token_count):\n    inputs = self.tokenizer.encode(input_text, return_tensors=&quot;pt&quot;).to(device)\n    outputs = self.model.generate(inputs,  max_new_tokens=token_count, pad_token_id=self.tokenizer.eos_token_id)\n    return self.tokenizer.decode(outputs[0])[len(input_text):]\n</code></pre>\n<p>Sample-Output:</p>\n<pre><code>private DataType FuntionName(String someId) {\n    // TODO: Replace with implementation that utilizes someId to obtain information\n    return DataType.Value;\n}\n\n\nThe comment:\n\n- If someId is present in the code, use the getAPI from Client with someId as a parameter to obtain some information.\n- If the\n\n</code></pre>\n",
        "answer": "<p>Do several iterations and make some sort of a pattern matching between number of tokens in the input prompt and max token parameter to get a complete output (as a percentage of input token length)</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 77061898,
            "link": "https://stackoverflow.com/questions/77061898/incomplete-output-with-llm-with-max-new-tokens"
        }
    },
    {
        "question": "Get attention masks from HF pipelines\n<p>How should returned attention masks be accessed from the FeatureExtractionPipeline in Huggingface?</p>\n<p>The code below takes an embedding model, distributes it and a huggingface dataset across 8 GPUs on a single node, and performs inference on the inputs. The code requires the attention masks for mean pooling.</p>\n<p>Code example:</p>\n<pre class=\"lang-py prettyprint-override\"><code>from accelerate import Accelerator\nfrom accelerate.utils import tqdm\nfrom transformers import AutoTokenizer, AutoModel\nfrom optimum.bettertransformer import BetterTransformer\n\nimport torch\n\nfrom datasets import load_dataset\n\nfrom transformers import pipeline\n\naccelerator = Accelerator()\n\nmodel_name = &quot;BAAI/bge-large-en-v1.5&quot;\n\ntokenizer = AutoTokenizer.from_pretrained(model_name,)\n\nmodel = AutoModel.from_pretrained(model_name,)\n\npipe = pipeline(\n    &quot;feature-extraction&quot;,\n    model=model,\n    tokenizer=tokenizer,\n    max_length=512,\n    truncation=True,\n    padding=True,\n    pad_to_max_length=True,\n    batch_size=256,\n    framework=&quot;pt&quot;,\n    return_tensors=True,\n    return_attention_mask=True,\n    device=(accelerator.device)\n)\n\ndataset = load_dataset(\n    &quot;wikitext&quot;,\n    &quot;wikitext-2-v1&quot;,\n    split=&quot;train&quot;,\n)\n\n#Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n\n# Assume 8 processes\n\nwith accelerator.split_between_processes(dataset[&quot;text&quot;]) as data:\n\n    for out in pipe(data):\n\n        sentence_embeddings = mean_pooling(out, out[&quot;attention_mask&quot;])\n</code></pre>\n<p>I need the attention maks from pipe to use for mean pooling.</p>\n<p>Best,</p>\n<p>Enrico</p>\n",
        "answer": "<p>You can often directly access the tokenizer from the pipe and call it with your string to get the attention mask:</p>\n<pre><code>&gt;&gt;&gt; pipe.tokenizer(&quot;Blah blah blah.&quot;)\n{'input_ids': [101, 27984, 27984, 27984, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1]}\n\n&gt;&gt;&gt; pipe.tokenizer(&quot;Blah blah blah.&quot;)['attention_mask']\n{'attention_mask': [1, 1, 1, 1, 1, 1]}\n</code></pre>\n<p>But even if that's not an option, it looks like you have access to the tokenizer at initialization.  Why not use that directly?</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 77603219,
            "link": "https://stackoverflow.com/questions/77603219/get-attention-masks-from-hf-pipelines"
        }
    },
    {
        "question": "ModuleNotFoundError: No module named &#39;huggingface_hub.inference._types&#39;\n<p>I am running a RAG pipeline, with LlamaIndex and quantized LLama3-8B-Instruct. I just installed these libraries:</p>\n<pre><code>!pip install --upgrade huggingface_hub \n!pip install --upgrade peft\n!pip install llama-index bitsandbytes accelerate llama-index-llms-huggingface llama-index-embeddings-huggingface\n!pip install --upgrade transformers\n!pip install --upgrade sentence-transformers\n</code></pre>\n<p>Then I was looking to run the quantization pipeline like this:</p>\n<pre><code>import torch\nfrom llama_index.llms.huggingface import HuggingFaceLLM\nfrom transformers import BitsAndBytesConfig\n\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_quant_type=&quot;nf4&quot;,\n    bnb_4bit_use_double_quant=True,\n)\n</code></pre>\n<p>However, I got this error returned to me: <code>ModuleNotFoundError: No module named 'huggingface_hub.inference._types'</code>. Last time I worked with this pipeline two months ago, the code worked, so I think LlamaIndex has changed something; especially since when I clicked on the error, it referenced to: <code> from huggingface_hub.inference._types import ConversationalOutput</code>, but <code>ConversationalOutput</code> module doesn't exist in HuggingFace docs.</p>\n<p>So, what should I do to fix this error and be able to run this RAG pipeline?</p>\n",
        "answer": "<h2>Root Cause and Fix</h2>\n<p><code>ModuleNotFoundError</code> indicates the code is importing a module that does not exist. Its due to a dependency mismatch between the version of <code>huggingface_hub</code> that you installed and the version compatible with <code>llama_index</code>.</p>\n<p><code>llama_index</code> uses a module that was recently deleted from the <code>huggingface_hub.inference</code> package called <code>_types</code>. We can infer this from the import error you posted:</p>\n<p><code>from huggingface_hub.inference._types import ConversationalOutput</code></p>\n<p>In <a href=\"https://github.com/huggingface/huggingface_hub/tree/v0.24.0/src/huggingface_hub/inference\" rel=\"nofollow noreferrer\">v0.24.0</a> the <code>_types</code> module exists. It was removed in <a href=\"https://github.com/huggingface/huggingface_hub/tree/v0.25.0/src/huggingface_hub/inference\" rel=\"nofollow noreferrer\">v0.25.0</a>.</p>\n<p>You need to uninstall <code>hugginface_hub</code> and install a version compatible with <code>llama_index</code>. I'd try 0.24.0 since it has the module currently causing the <code>ModuleNotFoundError</code>:</p>\n<pre><code>pip uninstall huggingface-hub\npip install huggingface-hub==0.24.0\n</code></pre>\n<h2>Managing Dependencies/Versions</h2>\n<p>Upgrades to external dependencies should not break an existing project's workflow because projects should carefully manage their dependencies and the versions associated with those dependencies.</p>\n<p>In Python, the convention is to use a <code>requirements.txt</code> file that lists the dependencies/versions required for a project. You can export your dependencies by running:</p>\n<pre><code>pip freeze &gt; requirements.txt\n</code></pre>\n<p>This will capture dependencies as well as versions. The dependencies specified in a requirements file can be imported back by running:</p>\n<pre><code>pip install -r requirements.txt\n</code></pre>\n<h2>Virtual Environments</h2>\n<p>To isolate dependencies for different projects, you should use virtual environments:</p>\n<pre><code>python -m venv llama\nsource llama/bin/activate\n</code></pre>\n<p>This will create an isolated python environment for your project that has no dependencies installed. It provides a fresh slate for every project and ensures one project's dependencies don't interfere with another's.</p>\n<p>When capturing dependencies in a requirements file, you should only include required dependencies for the current project, not all the dependencies on your system. Using pip freeze in conjunction with virtual environments makes this easy.</p>\n<h2>Handling Dependency Updates</h2>\n<p>You <strong>should</strong> upgrade an external dependencies when a new version is released. This ensures you have the library's latest features and is essential for safeguarding against known security vulnerabilities.</p>\n<p>This should be done in a structured manner:</p>\n<ol>\n<li>You should track releases for your dependencies.</li>\n<li>Upon a new release you should upgrade the version manually</li>\n<li>You should do regression testing on your application with the upgraded dependency to ensure nothing has broken due to the update</li>\n</ol>\n<p>If the update does not cause issues that's great. If it does you must either revert to the latest working version of the dependency or refactor your code to work with the new library.</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 79298879,
            "link": "https://stackoverflow.com/questions/79298879/modulenotfounderror-no-module-named-huggingface-hub-inference-types"
        }
    },
    {
        "question": "How does one set the pad token correctly (not to eos) during fine-tuning to avoid model not predicting EOS?\n<h2>tldr; what I really want to know is what is the official way to set pad token for <strong>fine tuning</strong> it wasn't set during original training, so that it doesn't not learn to predict EOS.</h2>\n<p>colab: <a href=\"https://colab.research.google.com/drive/1poFdFYmkR_rDM5U5Z2WWjTepMQ8hvzNc?usp=sharing\" rel=\"nofollow noreferrer\">https://colab.research.google.com/drive/1poFdFYmkR_rDM5U5Z2WWjTepMQ8hvzNc?usp=sharing</a></p>\n<hr />\n<p>The HF falcon tutorial has the following line:</p>\n<pre><code>tokenizer.pad_token = tokenizer.eos_token\n</code></pre>\n<p>it looks strange to me. It make sense pad and eos are the same but then why even make a difference between them in the first place in general?</p>\n<p>Note its wrong to do pad = eos. This means during fine-tuning the model will never be trained to output eos (most likely) since eos is treated as pad token and no back propagated:</p>\n<pre><code>I just observed that when I set tokenizer.pad_token = tokenizer.eos_token during training, the model won't stop generating during inference, since it was trained to not output the eos token (per discussions above).\n</code></pre>\n<p>I saw this (here <a href=\"https://github.com/huggingface/transformers/issues/22794\" rel=\"nofollow noreferrer\">https://github.com/huggingface/transformers/issues/22794</a>):</p>\n<pre><code>tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n</code></pre>\n<p>But this assumes the model has a pad_token. I think an additional check has to be done that it does have an embedding for pad_token so that there are no run time errors (~type errors in the matrix extraction from the embedding &quot;table&quot;/matrix).</p>\n<p>But if one does that some care might be needed to initialize the new token so that it dominates the generation: <a href=\"https://nlp.stanford.edu/%7Ejohnhew/vocab-expansion.html\" rel=\"nofollow noreferrer\">https://nlp.stanford.edu/~johnhew/vocab-expansion.html</a></p>\n<hr />\n<p>code:</p>\n<pre class=\"lang-py prettyprint-override\"><code>def get_model_tokenizer_qlora_falcon7b(model_name: str = &quot;ybelkada/falcon-7b-sharded-bf16&quot;,\n                                       config: wand.Config,  # todo\n                                       lora_alpha=16,  # todo\n                                       lora_dropout=0.1,  # todo\n                                       lora_r=64,  # todo\n                                       bnb_4bit_compute_dtype=torch.float16,  # changed it from Guanaco hf\n                                       ) -&gt; tuple:\n    &quot;&quot;&quot;\n    Load the Falcon 7B model, quantize it in 4bit and attach LoRA adapters on it.\n\n    bf16 = 1S, 7Exp, 8Mantissa\n\n    Do:\n        pip install bitsandbytes\n    ref:\n        - https://colab.research.google.com/drive/1DOi8MFv4SWN9NImVornZ7t6BgmLoPQO-#scrollTo=AjB0WAqFSzlD\n    &quot;&quot;&quot;\n    from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoTokenizer\n\n    # model_id = &quot;tiiuae/falcon-7b&quot;\n    # model_name: str = &quot;ybelkada/falcon-7b-sharded-bf16&quot;\n\n    # - get bnb config for bit-4 base model (bnb lib for using 4bit qlora quantization techniques by tim dettmers)\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,  # load (usually huge) base model in 4 bits\n        bnb_4bit_quant_type=&quot;nf4&quot;,  # normal float 4 for the (usually huge) base model. introduces error but fixed by ft\n        # ref: https://gist.github.com/pacman100/1731b41f7a90a87b457e8c5415ff1c14\n        bnb_4bit_compute_dtype=bnb_4bit_compute_dtype,\n    )\n\n    # - get falcon 4bit model\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        quantization_config=bnb_config,\n        trust_remote_code=True  # allows to execute custom code you download from the uploaded model code you are using\n    )\n    model.config.use_cache = False  # todo: why? https://stackoverflow.com/questions/76633335/why-does-hugging-face-falcon-model-use-mode-config-use-cache-false-why-wouldn\n\n    # get falcon tockenizer\n    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)  # execs code downloaded from hf hub\n    tokenizer.pad_token = tokenizer.eos_token\n</code></pre>\n<hr />\n<h2>Modifying model gives issues</h2>\n<p>This still not works:</p>\n<pre><code> UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n</code></pre>\n<p>code:</p>\n<pre><code>&quot;&quot;&quot;\nsfttrainer (likely using peft) best practices:\nhttps://huggingface.co/docs/trl/main/en/sft_trainer#best-practices\n\nBest practices\n\nPay attention to the following best practices when training a model with that trainer:\n\n- SFTTrainer always pads by default the sequences to the max_seq_length argument of the SFTTrainer. If none is passed, the trainer will retrieve that value from the tokenizer. Some tokenizers do not provide default value, so there is a check to retrieve the minimum between 2048 and that value. Make sure to check it before training.\n- For training adapters in 8bit, you might need to tweak the arguments of the prepare_model_for_int8_training method from PEFT, hence we advise users to use prepare_in_int8_kwargs field, or create the PeftModel outside the SFTTrainer and pass it.\n- For a more memory-efficient training using adapters, you can load the base model in 8bit, for that simply add load_in_8bit argument when creating the SFTTrainer, or create a base model in 8bit outside the trainer and pass it.\n- If you create a model outside the trainer, make sure to not pass to the trainer any additional keyword arguments that are relative to from_pretrained() method.\n\ntodo: why trust_remote_code? I want more details.\n&quot;&quot;&quot;\nimport sys\n\nimport torch\nfrom peft import LoraConfig\n\nfrom transformers.modeling_utils import PreTrainedModel\n\nfrom pdb import set_trace as st\n\n\ndef test_bfloat16_int4(compute_dtype: torch.dtype,\n                       use_4bit,\n                       ):\n    &quot;&quot;&quot;\npython -c &quot;import torch; print(torch.cuda.get_device_capability());&quot;\n    todo: check other code test_bfloat16() do we need use_4bit?\n    &quot;&quot;&quot;\n    if compute_dtype == torch.float16 and use_4bit:\n        major, _ = torch.cuda.get_device_capability()\n        if major &gt;= 8:\n            print(&quot;=&quot; * 80)\n            print(&quot;Your GPU supports bfloat16, you can accelerate training with the argument --bfloat16&quot;)\n            print(&quot;=&quot; * 80)\n\n\ndef get_model_tokenizer_qlora_falcon7b(\n        # -- mode args\n        # model_id = &quot;tiiuae/falcon-7b&quot;\n        pretrained_model_name_or_path: str = &quot;ybelkada/falcon-7b-sharded-bf16&quot;,\n        use_cache: bool = True,\n        # -- lora args\n        lora_alpha=16,  # todo\n        lora_dropout=0.1,  # todo, evidence drop out really help? google, crfm, gpt4\n        lora_r=64,  # todo\n        bnb_4bit_compute_dtype=torch.float16,  # changed it from Guanaco hf\n\n        # -- training args\n        output_dir=&quot;./results&quot;,\n        per_device_train_batch_size=4,\n        gradient_accumulation_steps=4,\n        # paging so that the sudden mem gpu spikes don't cause the run to shut down\n        # (I think usually caused by too long seqs)\n        # todo: why 32 bit opt?\n        # todo: paged nadamw opt?\n        optim=&quot;paged_adamw_32bit&quot;,\n        save_steps=10,\n        logging_steps=10,\n        learning_rate=2e-4,\n        max_grad_norm=0.3,\n        max_steps=500,\n        warmup_ratio=0.03,\n        lr_scheduler_type=&quot;constant&quot;,\n        # -- quant. args (not recommended to be changed unless you know what your doing?)\n        load_in_4bit=True,  # load (usually huge) base model in 4 bits\n        bnb_4bit_quant_type=&quot;nf4&quot;,  # normal float 4 for the (large) base models qlora\n) -&gt; tuple:\n    &quot;&quot;&quot;\n    Load the Falcon 7B model, quantize it in 4bit and attach LoRA adapters on it.\n\n    bf16 = 1S, 7Exp, 8Mantissa\n    hypothesis: 7b trained due to 6.7 emergence rumour, I still don't think emergence is real.\n    Notes:\n        - ft a model is very specific to the model, tokenizer and training scheme. Thus we return\n            - model, tokenizer, ft config (peft config), training args\n\n    ref:\n        - https://colab.research.google.com/drive/1DOi8MFv4SWN9NImVornZ7t6BgmLoPQO-#scrollTo=AjB0WAqFSzlD\n    &quot;&quot;&quot;\n    from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoTokenizer\n\n    # - Get bnb config for bit-4 base model (bnb lib for using 4bit qlora quantization techniques by tim dettmers)\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=load_in_4bit,  # load (usually huge) base model in 4 bits\n        bnb_4bit_quant_type=bnb_4bit_quant_type,  # normal float 4 for the (usually huge) base model\n        bnb_4bit_compute_dtype=bnb_4bit_compute_dtype,  # if you can, during computation use bf16\n    )\n\n    # - Get falcon 4bit model\n    # todo, where is this being saved &amp; how to download quicker\n    model = AutoModelForCausalLM.from_pretrained(\n        pretrained_model_name_or_path=pretrained_model_name_or_path,\n        quantization_config=bnb_config,\n        trust_remote_code=True  # allows to execute custom code you download from the uploaded model code you are using\n    )\n    print(f'{type(model)=}')\n    print(f'{model=}')\n    # this is here to save gpu vram. Likely only needed when using 40b or when oom issues happen ref: https://stackoverflow.com/questions/76633335/why-does-hugging-face-falcon-model-use-mode-config-use-cache-false-why-wouldn\n    model.config.use_cache = use_cache\n    print(f'{type(model)=}')\n\n    # - Get falcon tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path,\n                                              trust_remote_code=True)  # execs code downloaded from hf hub\n    # tokenizer.pad_token = tokenizer.eos_token  # ref: https://stackoverflow.com/questions/76633368/why-does-the-falcon-qlora-tutorial-code-use-eos-token-as-pad-token\n    # tokenizer.add_special_tokens({'pad_token': '[PAD]'})  # I think this is fine if during the training pad is ignored\n    tokenizer.add_special_tokens({'pad_token': '&lt;|pad|&gt;'})  # I think this is fine if during the training pad is ignored\n\n    # - Modify model\n    # add pad token embed\n    model.resize_token_embeddings(len(tokenizer))  # todo: I think this is fine if during the training pad is ignored\n    model.transformer.word_embeddings.padding_idx = len(tokenizer) - 1\n    model.config.max_new_tokens = len(tokenizer)\n    # model.config.min_length = 1\n    print(f'{model=}')\n    print(f'{type(tokenizer)=}')\n    print(f'{tokenizer.pad_token=}')\n    # data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False) todo\n\n    # - Get falcon lora config\n    peft_config = LoraConfig(\n        lora_alpha=lora_alpha,\n        lora_dropout=lora_dropout,\n        r=lora_r,\n        bias=&quot;none&quot;,\n        task_type=&quot;CAUSAL_LM&quot;,\n        # model card for falcon tiiuae/falcon-7b: https://huggingface.co/tiiuae/falcon-7b/blob/main/modelling_RW.py\n        # does seem to include all trainable params as done by qlora on their own paper\n        target_modules=[\n            # word_embeddings,\n            &quot;query_key_value&quot;,\n            &quot;dense&quot;,\n            &quot;dense_h_to_4h&quot;,\n            &quot;dense_4h_to_h&quot;,\n            # &quot;lm_head&quot;\n        ]\n    )\n    print(f'{type(peft_config)=}')\n\n    # todo: print the num params of the lora = D1*r + D2*r and num of bytes by prec. (bytes) * num params\n    return model, tokenizer, peft_config\n\n\n# -- tests\n\ndef example_test_model_already_has_pad_token():\n    &quot;&quot;&quot;\n    if it already has pad token, it likely has a small prob, so we are done.\n\n    compare it's norm with other tokens to verify this is true.\n\npython ~/ultimate-utils/ultimate-utils-proj-src/uutils/hf_uu/model_tokenizer/falcon_uu_mdl_tok.py\n    &quot;&quot;&quot;\n    # - the get datasets todo: preprocessing, padding, streaming\n    from uutils.hf_uu.data_hf.common import get_guanaco_datsets_add_splits_train_test_only\n    trainset, _, testset = get_guanaco_datsets_add_splits_train_test_only()\n\n    # qlora flacon7b\n    from uutils.hf_uu.model_tokenizer.falcon_uu_mdl_tok import get_model_tokenizer_qlora_falcon7b\n    model, tokenizer, peft_config = get_model_tokenizer_qlora_falcon7b()\n    model: PreTrainedModel = model\n    print(f'{model=}')\n    sent = 'Dogs are great because they are '\n    print()\n\n    # print to see if pad tokens are present and if it ignores the tokens at the end\n    encoded_input = tokenizer(sent, padding='max_length', max_length=10, return_tensors='pt')\n    print(f'{encoded_input=}')\n\n    # Print all special tokens\n    print('\\n---- start Print all special tokens')\n    for token_name, token in tokenizer.special_tokens_map.items():\n        print(f&quot;{token_name}: {token}&quot;)\n    print('\\n---- end Print all special tokens')\n\n    # Get the ID for the '[PAD]' token\n    try:\n        pad_token_id = tokenizer.convert_tokens_to_ids('[PAD]')\n    except KeyError:\n        raise ValueError(&quot;Token [PAD] is not present in the tokenizer vocabulary.&quot;)\n\n    # Index into the model's embedding table\n    try:\n        print(f'{model.get_input_embeddings().weight.size()=}')\n        pad_embedding = model.get_input_embeddings().weight[pad_token_id]\n    except IndexError:\n        raise ValueError(f&quot;Token ID {pad_token_id} is not present in the model's embedding matrix.&quot;)\n\n    print(f'{pad_embedding=}')\n    print('Success!\\n')\n\n    # check it generates something sensible\n    # tokenizer.decode(model.generate(**tokenizer(sent, return_tensors='pt'), do_sample=True)[0])\n    input_ids, attention_mask = encoded_input['input_ids'], encoded_input['attention_mask']\n    predicted_tokens_ids_options = model.generate(input_ids=input_ids, attention_mask=attention_mask, do_sample=True)\n    predicted_tokens_ids = predicted_tokens_ids_options[0]\n    predicted_sent = tokenizer.decode(predicted_tokens_ids)\n    print(f'original sentence: {sent=}')\n    print(f'predicted sentence: {predicted_sent=}')\n    print('Success2!')\n\n\nif __name__ == '__main__':\n    import time\n\n    start_time = time.time()\n    example_test_model_already_has_pad_token()\n    print(f&quot;The main function executed in {time.time() - start_time} seconds.\\a&quot;)\n</code></pre>\n<p>it doesn't like the modifications to the model:</p>\n<pre><code>    model.transformer.word_embeddings.padding_idx = len(tokenizer) - 1\n    model.config.max_new_tokens = len(tokenizer)\n</code></pre>\n<p>How to fix?</p>\n<p>Errors:</p>\n<pre><code>/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/transformers/generation/utils.py:1259: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n  warnings.warn(\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/transformers/generation/utils.py:1452: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n  warnings.warn(\nTraceback (most recent call last):\n  File &quot;/lfs/hyperturing1/0/brando9/ultimate-utils/ultimate-utils-proj-src/uutils/hf_uu/model_tokenizer/falcon_uu_mdl_tok.py&quot;, line 211, in &lt;module&gt;\n    example_test_model_already_has_pad_token()\n  File &quot;/lfs/hyperturing1/0/brando9/ultimate-utils/ultimate-utils-proj-src/uutils/hf_uu/model_tokenizer/falcon_uu_mdl_tok.py&quot;, line 199, in example_test_model_already_has_pad_token\n    predicted_tokens_ids_options = model.generate(input_ids=input_ids, attention_mask=attention_mask, do_sample=True)\n  File &quot;/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/torch/utils/_contextlib.py&quot;, line 115, in decorate_context\n    return func(*args, **kwargs)\n  File &quot;/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/transformers/generation/utils.py&quot;, line 1572, in generate\n    return self.sample(\n  File &quot;/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/transformers/generation/utils.py&quot;, line 2633, in sample\n    next_token_scores = logits_warper(input_ids, next_token_scores)\n  File &quot;/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/transformers/generation/logits_process.py&quot;, line 92, in __call__\n    scores = processor(input_ids, scores)\n  File &quot;/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/transformers/generation/logits_process.py&quot;, line 302, in __call__\n    indices_to_remove = scores &lt; torch.topk(scores, top_k)[0][..., -1, None]\nRuntimeError: &quot;topk_cpu&quot; not implemented for 'Half'\n</code></pre>\n<hr />\n<h2>Bounty Section: Small GPT2 code example</h2>\n<p>Yes I agree that pad is assigned to eos. Eos is still eos. But during fine-tuning now the weights wrt to eos are unchanged. This might be an issue since the probability of eos has not shifted to the fine-tuning regime. One possibility is that eos is outputed with less chance. Yes we can still halt production when we see eos but we've not shifted the probability to output eos according to our fine-tuning distribution -- but all other tokens have changed distribution. I think this could be an issue because it's not like the old probability of eos is conserved since all tokens probs have changed except eos + even if the old eos prob was conserved, it's wrt wrong distribution (not the fine tuning one).</p>\n<p>e.g.,</p>\n<pre><code>if tokenizer.pad_token_id is None:\n    tokenizer.pad_token = tokenizer.eos_token\n ...\nraw_text_batch='a'\ntokenize_batch={'input_ids': tensor([[   64, 50256, 50256, 50256, 50256]]), 'attention_mask': tensor([[1, 0, 0, 0, 0]])}\n</code></pre>\n<p>but it would have been better to have</p>\n<pre><code>tokenize_batch={'input_ids': tensor([[   64, 50256, 50256, 50256, 50256]]), 'attention_mask': tensor([[1, 1, 0, 0, 0]])}\n</code></pre>\n<p>code</p>\n<pre><code>def test_eos_pad():\n    from datasets import load_dataset\n    import torch\n    from transformers import GPT2Tokenizer, GPT2LMHeadModel\n\n    raw_text_batch = 'a'\n\n    tokenizer = GPT2Tokenizer.from_pretrained(&quot;gpt2&quot;)\n    # print(f'{tokenizer.eos_token=}')\n    # print(f'{tokenizer.eos_token_id=}')\n    # print(f'{tokenizer.pad_token=}')\n    # print(f'{tokenizer.pad_token_id=}')\n\n    # print(f'{raw_text_batch=}')\n    # tokenize_batch = tokenizer(raw_text_batch, padding=&quot;max_length&quot;, max_length=5, truncation=True, return_tensors=&quot;pt&quot;)\n    # print(f'{tokenize_batch=}')\n\n    if tokenizer.pad_token_id is None:\n        tokenizer.pad_token = tokenizer.eos_token\n    probe_network = GPT2LMHeadModel.from_pretrained(&quot;gpt2&quot;)\n    device = torch.device(f&quot;cuda:{0}&quot; if torch.cuda.is_available() else &quot;cpu&quot;)\n    probe_network = probe_network.to(device)\n\n    print(f'{tokenizer.eos_token=}')\n    print(f'{tokenizer.eos_token_id=}')\n    print(f'{tokenizer.pad_token=}')\n    print(f'{tokenizer.pad_token_id=}')\n\n    print(f'{raw_text_batch=}')\n    tokenize_batch = tokenizer(raw_text_batch, padding=&quot;max_length&quot;, max_length=5, truncation=True, return_tensors=&quot;pt&quot;)\n    print(f'{tokenize_batch=}')\n    print('Done')\n</code></pre>\n<hr />\n<p>cross:</p>\n<ul>\n<li>hf discuss forum: <a href=\"https://discuss.huggingface.co/t/why-does-the-falcon-qlora-tutorial-code-use-eos-token-as-pad-token/45954\" rel=\"nofollow noreferrer\">https://discuss.huggingface.co/t/why-does-the-falcon-qlora-tutorial-code-use-eos-token-as-pad-token/45954</a></li>\n<li>pytorch forum discuss: <a href=\"https://discuss.pytorch.org/t/how-does-one-set-the-pad-token-correctly-not-to-eos-during-fine-tuning-to-avoid-model-not-predicting-eos/213619\" rel=\"nofollow noreferrer\">https://discuss.pytorch.org/t/how-does-one-set-the-pad-token-correctly-not-to-eos-during-fine-tuning-to-avoid-model-not-predicting-eos/213619</a></li>\n<li><a href=\"https://discord.com/channels/879548962464493619/1126681170957045770/1126681170957045770\" rel=\"nofollow noreferrer\">https://discord.com/channels/879548962464493619/1126681170957045770/1126681170957045770</a></li>\n<li>context peft pacman100 code: <a href=\"https://gist.github.com/pacman100/1731b41f7a90a87b457e8c5415ff1c14\" rel=\"nofollow noreferrer\">https://gist.github.com/pacman100/1731b41f7a90a87b457e8c5415ff1c14</a></li>\n<li>twitter tweet of this: <a href=\"https://twitter.com/BrandoHablando/status/1693676898013061337?s=20\" rel=\"nofollow noreferrer\">https://twitter.com/BrandoHablando/status/1693676898013061337?s=20</a></li>\n</ul>\n",
        "answer": "<h1>When Models already has EOS and Pad and you tell the tokenizer to add eos, you don't need the above code in this SO post</h1>\n<h2>Why the Additional Code is Not Needed</h2>\n<ol>\n<li><p><strong>Correct Handling of <code>&lt;eos&gt;</code> by the Tokenizer</strong>:</p>\n<ul>\n<li>The <code>AutoTokenizer</code> with <code>add_eos_token=True</code> ensures the <code>&lt;eos&gt;</code> token is correctly added at the end of each input sequence.</li>\n<li>The attention mask generated by the tokenizer is already accurate, marking the padding tokens and <code>&lt;eos&gt;</code> appropriately.</li>\n</ul>\n</li>\n<li><p><strong>Redundant Masking Logic</strong>:</p>\n<ul>\n<li>The provided code manually modifies the attention mask and labels to only train on the first <code>&lt;eos&gt;</code>. However, this is unnecessary since the tokenizer's default behavior already handles the <code>&lt;eos&gt;</code> token correctly.</li>\n</ul>\n</li>\n<li><p><strong>Simplified Training Data Preparation</strong>:</p>\n<ul>\n<li>The Hugging Face Trainer expects <code>labels</code> to align with <code>input_ids</code>, where tokens like <code>&lt;pad&gt;</code> and extra <code>&lt;eos&gt;</code> are automatically ignored during loss computation when using <code>-100</code>.</li>\n<li>The manual masking of subsequent <code>&lt;eos&gt;</code> tokens adds unnecessary complexity.</li>\n</ul>\n</li>\n<li><p><strong>Sequence Length Management</strong>:</p>\n<ul>\n<li>The tokenizer ensures sequences are truncated or padded to fit within the model’s maximum length. The code to drop sequences exceeding the maximum length is redundant if preprocessing already handles this.</li>\n</ul>\n</li>\n</ol>\n<h3>Conclusion</h3>\n<p>The additional code introduces manual handling of tasks that the tokenizer and Hugging Face Trainer already manage. Therefore, it can be replaced with a simpler preprocessing function that clones <code>input_ids</code> into <code>labels</code> without additional modifications.</p>\n<pre class=\"lang-py prettyprint-override\"><code># ref: https://gist.github.com/brando90/4cd94ad3730218dca75dba779f770c9d\nfrom transformers import AutoTokenizer\n\ndef analyze_tokenizer_output(model_name, text, pad_token=&quot;&lt;pad&gt;&quot;, eos_token=&quot;&lt;/s&gt;&quot;, max_length=20):\n    &quot;&quot;&quot;\n    Analyzes the tokenizer output, including the attention mask and labels, \n    when eos_token and pad_token are present.\n    &quot;&quot;&quot;\n    # Load the tokenizer\n    tok = AutoTokenizer.from_pretrained(model_name, padding_side=&quot;right&quot;, trust_remote_code=True, add_eos_token=True)\n\n    # Tokenize the input text\n    encoded = tok(\n        text,\n        padding=&quot;max_length&quot;,\n        truncation=True,\n        max_length=max_length,\n        return_tensors=&quot;pt&quot;\n    )\n\n    # Add labels for training\n    encoded[&quot;labels&quot;] = encoded[&quot;input_ids&quot;].clone()\n    encoded[&quot;labels&quot;][encoded[&quot;input_ids&quot;] == tok.pad_token_id] = -100  # Ignore padding in labels\n\n    # Display the tokenizer outputs\n    print(f&quot;Input Text: {text}&quot;)\n    print(f&quot;Tokenized IDs: {encoded['input_ids']}&quot;)\n    print(f&quot;Attention Mask: {encoded['attention_mask']}&quot;)\n    print(f&quot;Labels: {encoded['labels']}&quot;)\n    print(f&quot;Decoded Tokens: {tok.decode(encoded['input_ids'][0])}&quot;)\n    print(f&quot;Pad Token ID: {tok.pad_token_id}, EOS Token ID: {tok.eos_token_id}&quot;)\n    print()\n\n# Example usage\nif __name__ == &quot;__main__&quot;:\n    model_name = &quot;google/gemma-2-2b&quot;\n    test_text = &quot;This is a test input for Gemma tokenizer.&quot;\n    max_length = 15\n    analyze_tokenizer_output(model_name, test_text, max_length=max_length)\n\n&quot;&quot;&quot;\nTokenized IDs: tensor([[     2,   1596,    603,    476,   2121,   3772,    604, 137061, 142224,\n         235265,      1,      0,      0,      0,      0]])\nAttention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]])\nLabels: tensor([[     2,   1596,    603,    476,   2121,   3772,    604, 137061, 142224,\n         235265,      1,   -100,   -100,   -100,   -100]])\nDecoded Tokens: &lt;bos&gt;This is a test input for Gemma tokenizer.&lt;eos&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;\nPad Token ID: 0, EOS Token ID: 1\n&quot;&quot;&quot;\n</code></pre>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 76633368,
            "link": "https://stackoverflow.com/questions/76633368/how-does-one-set-the-pad-token-correctly-not-to-eos-during-fine-tuning-to-avoi"
        }
    },
    {
        "question": "Use HuggingFace models locally\n<p>I would like to use transformers especially HuggingFace Models as a part of my programming</p>\n<p>my question is; Can I use and implement transformers and HuggingFace Models offline and in Spyder IDE (or any other IDE that I can use locally? (Of course, after downloading and installing all needed packages).</p>\n<p>Thanks in advance.</p>\n",
        "answer": "<p>If I understand your question correctly, you want to do something like this:</p>\n<pre><code>import requests\nimport numpy as np\n\nAPI_URL = &quot;https://api-inference.huggingface.co/pipeline/feature-extraction/deepset/roberta-base-squad2&quot;\nheaders = {&quot;Authorization&quot;: f&quot;Bearer {HUGGINGFACE_TOKEN}&quot;}\n\ndef query(payload):\n    response = requests.post(API_URL, headers=headers, json=payload)\n    return response.json()\n\ndata = query({&quot;inputs&quot;: &quot;This is a more detailed example.&quot;})\ndata = np.array(data)[0]\nembedding = np.mean(data, axis=0)\nlen(embedding), embedding\n</code></pre>\n<p>Note that the api url contains</p>\n<ol>\n<li>the task (here: <code>feature-extraction</code>) and</li>\n<li>the model (here: <code>deepset/roberta-base-squad2</code>).</li>\n</ol>\n<p>You can look up available tasks here:\n<a href=\"https://huggingface.co/docs/transformers/en/main_classes/pipelines\" rel=\"nofollow noreferrer\">https://huggingface.co/docs/transformers/en/main_classes/pipelines</a></p>\n<p>And you can choose an appropriate model there:\n<a href=\"https://huggingface.co/models\" rel=\"nofollow noreferrer\">https://huggingface.co/models</a></p>\n<p>Usually, you don't want to expose your <code>HUGGINGFACE_TOKEN</code>. Here is a video that demonstrates how to hide it:\n<a href=\"https://www.youtube.com/watch?v=_tYNm5nqpxE\" rel=\"nofollow noreferrer\">https://www.youtube.com/watch?v=_tYNm5nqpxE</a></p>\n<p>I hope this helps.</p>\n<p>P.s.: Here is a <a href=\"https://colab.research.google.com/drive/1_Ovbynwk2QQpdr5PcXP2ArWeA0IDYndh?usp=sharing#scrollTo=fgltsNsyrE9R\" rel=\"nofollow noreferrer\">link to a Colab notebook</a>.</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 77905301,
            "link": "https://stackoverflow.com/questions/77905301/use-huggingface-models-locally"
        }
    },
    {
        "question": "Not able to download and save huggingface model - jinaai/jina-reranker-v2-base-multilingual\n<p>I am trying to download and save the following model from huggingface for later use. Here is the snippet.</p>\n<pre><code>from transformers import AutoModelForSequenceClassification, \nAutoTokenizer,AutoModelForCausalLM\n\nmodel_name='jinaai/jina-reranker-v2-base-multilingual'\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.save_pretrained(model_name)\n\n\nmodel = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\nmodel.save_pretrained(f&quot;model/{model_name}&quot;)\n</code></pre>\n<p>Getting this error</p>\n<pre><code>ValueError: Unrecognized configuration class &lt;class 'transformers_modules.jina- \nreranker-v2-base-multilingual.configuration_xlm_roberta.XLMRobertaFlashConfig'&gt; for \nthis kind of AutoModel: AutoModelForCausalLM.\nModel type should be one of BartConfig, BertConfig, BertGenerationConfig, \nBigBirdConfig, BigBirdPegasusConfig.......\n</code></pre>\n<p>Folder Structure for the project</p>\n<pre><code>root/aimodel.py (where the code snippet is written)\nroot/jinai/jina-reranker-v2-base-multilingual/ &lt; all files from this url - [https://huggingface.co/jinaai/jina-reranker-v2-base-multilingual/tree/main][1] &gt;\n</code></pre>\n",
        "answer": "<p>This model is a sentence pair classification model that was fine-tuned on multilingual sentence similarity/retrieval. It is not a generic text generation model, so you can not load it as such. Therefore you should use <code>AutoModelForSequenceClassification</code> instead of <code>AutoModelForCausalLM</code>, see the <a href=\"https://huggingface.co/jinaai/jina-reranker-v2-base-multilingual#usage\" rel=\"nofollow noreferrer\">documentation</a>.</p>\n<p>Here's an example of how to use it:</p>\n<pre><code>from transformers import AutoModelForSequenceClassification\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    'jinaai/jina-reranker-v2-base-multilingual',\n    torch_dtype=&quot;auto&quot;,\n    trust_remote_code=True,\n).to('cuda').eval()\n\ncandidates = [&quot;the car goes fast&quot;, &quot;the car goes slow&quot;, &quot;the bicycle goes slow&quot;]\nsentence_pairs = [[&quot;the car drives fast&quot;, doc] for doc in candidates]\n\nscores = model.compute_score(sentence_pairs, max_length=1024)\nprint(scores)\n\n&gt; [0.7217432260513306, 0.14128141105175018, 0.0384661927819252]\n</code></pre>\n<p>If you want to use/fine-tune a model with <code>AutoModelForCausalLM</code>, check out the list of text generation model on huggingface: <a href=\"https://huggingface.co/models?pipeline_tag=text-generation&amp;sort=downloads\" rel=\"nofollow noreferrer\">https://huggingface.co/models?pipeline_tag=text-generation&amp;sort=downloads</a></p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 79097200,
            "link": "https://stackoverflow.com/questions/79097200/not-able-to-download-and-save-huggingface-model-jinaai-jina-reranker-v2-base-m"
        }
    },
    {
        "question": "How to Compute Teacher-Forced Accuracy (TFA) for Hugging Face Models While Handling EOS Tokens?\n<p>I am trying to compute Teacher-Forced Accuracy (TFA) for Hugging Face models, ensuring the following:</p>\n<ol>\n<li>EOS Token Handling: The model should be rewarded for predicting the first EOS token.</li>\n<li>Ignoring Padding: Any padding tokens (beyond the first EOS) should be ignored during accuracy calculation.</li>\n<li>Right-Shifted Input: The inputs are shifted correctly for teacher-forced training.</li>\n<li>List item</li>\n</ol>\n<p>Here’s the full code I wrote:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ndef compute_tfa(model, tokenizer, input_texts):\n    &quot;&quot;&quot;\n    Computes Teacher-Forced Accuracy (TFA), rewarding the model for correctly predicting\n    the first EOS token while ignoring predictions for padding tokens.\n\n    Parameters:\n        model: The language model (Hugging Face CausalLM).\n        tokenizer: The tokenizer corresponding to the model.\n        input_texts: List of input texts to compute TFA.\n\n    Returns:\n        TFA score as a float.\n    &quot;&quot;&quot;\n    # Tokenize input texts\n    tokenizer.pad_token = tokenizer.eos_token  # Use EOS as the pad token\n    inputs = tokenizer(input_texts, return_tensors='pt', padding=True, truncation=True)\n    input_ids = inputs['input_ids']\n    \n    # Create right-shifted input by adding the EOS token at the beginning\n    eos_token_id = tokenizer.eos_token_id\n    right_shifted_input_ids = torch.cat([\n        torch.full((input_ids.shape[0], 1), eos_token_id, dtype=torch.long),  # Add EOS token\n        input_ids[:, :-1]\n    ], dim=1)\n\n    # Perform a forward pass with the right-shifted inputs\n    with torch.no_grad():\n        outputs = model(input_ids=right_shifted_input_ids)\n        logits = outputs.logits  # Shape: (batch_size, sequence_length, vocab_size)\n\n    # Compute predictions\n    predicted_token_ids = torch.argmax(logits, dim=-1)  # Shape: (batch_size, sequence_length)\n\n    # Find the first EOS position in each sequence\n    eos_positions = (input_ids == eos_token_id).int().argmax(dim=1)  # Shape: (batch_size,)\n\n    # Mask to ignore tokens after the first EOS\n    sequence_lengths = input_ids.size(1)\n    mask = torch.arange(sequence_lengths).unsqueeze(0).to(input_ids.device)\n    mask = mask &lt; eos_positions.unsqueeze(1)\n\n    # Include the first EOS token in the mask\n    mask.scatter_(1, eos_positions.unsqueeze(1), 1)\n\n    # Apply the mask to filter predictions and labels\n    filtered_predictions = predicted_token_ids[mask]\n    filtered_labels = input_ids[mask]\n\n    # Compute accuracy\n    correct_predictions = (filtered_predictions == filtered_labels).float()\n    accuracy = correct_predictions.mean().item()\n\n    return accuracy\n\ndef main():\n    # Define models and their URLs\n    models_and_urls = {\n        &quot;google/gemma-2-2b&quot;: &quot;https://huggingface.co/google/gemma-2-2b&quot;,\n        &quot;meta-llama/Llama-3.1-8B&quot;: &quot;https://huggingface.co/meta-llama/Llama-3.1-8B&quot;,\n        &quot;gpt2&quot;: &quot;https://huggingface.co/gpt2&quot;\n    }\n\n    # Define input texts\n    input_texts = [\n        &quot;The quick brown fox jumps over the lazy dog.&quot;,\n        &quot;Artificial Intelligence is transforming the world of science.&quot;\n    ]\n\n    # Test each model\n    for model_name, model_url in models_and_urls.items():\n        print(f&quot;Testing model: {model_name} ({model_url})&quot;)\n        tokenizer = AutoTokenizer.from_pretrained(model_name)\n        model = AutoModelForCausalLM.from_pretrained(model_name)\n\n        # Compute TFA\n        tfa_score = compute_tfa(model, tokenizer, input_texts)\n        print(f&quot;Teacher-Forced Accuracy (TFA) for {model_name}: {tfa_score:.4f}\\n&quot;)\n\nif __name__ == &quot;__main__&quot;:\n    main()\n</code></pre>\n<p><strong>What I Need Help With</strong>:</p>\n<ol>\n<li><p><strong>EOS Token Masking</strong>: Is the masking logic I implemented for ignoring tokens after the first EOS correct? Specifically, I used:</p>\n<pre class=\"lang-py prettyprint-override\"><code>mask = torch.arange(sequence_lengths).unsqueeze(0).to(input_ids.device)\nmask = mask &lt; eos_positions.unsqueeze(1)\nmask.scatter_(1, eos_positions.unsqueeze(1), 1)\n</code></pre>\n<p>Is this the best way to ensure only tokens up to and including the first EOS are considered?</p>\n</li>\n<li><p><strong>Right-Shifted Input</strong>: I prepend the EOS token to the input like this:</p>\n<pre class=\"lang-py prettyprint-override\"><code>right_shifted_input_ids = torch.cat([\n    torch.full((input_ids.shape[0], 1), eos_token_id, dtype=torch.long),\n    input_ids[:, :-1]\n], dim=1)\n</code></pre>\n<p>Is this a standard way to handle the right-shift for teacher-forced evaluation?</p>\n</li>\n<li><p><strong>Generalization</strong>: The code is designed to evaluate multiple models, such as <code>google/gemma-2-2b</code>, <code>meta-llama/Llama-3.1-8B</code>, and <code>gpt2</code>. Are there any additional considerations or best practices I should follow for TFA computation across diverse models?</p>\n</li>\n<li><p><strong>Performance Optimization</strong>: Is there a more efficient way to compute the mask and apply it to the predictions and labels? My current method seems to work but might be suboptimal for larger datasets.</p>\n</li>\n<li><p>cross: <a href=\"https://discuss.huggingface.co/t/how-to-compute-teacher-forced-accuracy-tfa-for-hugging-face-models-while-handling-eos-tokens/126403\" rel=\"nofollow noreferrer\">https://discuss.huggingface.co/t/how-to-compute-teacher-forced-accuracy-tfa-for-hugging-face-models-while-handling-eos-tokens/126403</a></p>\n</li>\n<li><p>cross2: <a href=\"https://discuss.pytorch.org/t/how-to-compute-teacher-forced-accuracy-tfa-for-hugging-face-models-while-handling-eos-tokens/213435\" rel=\"nofollow noreferrer\">https://discuss.pytorch.org/t/how-to-compute-teacher-forced-accuracy-tfa-for-hugging-face-models-while-handling-eos-tokens/213435</a></p>\n</li>\n</ol>\n<hr />\n<h1>Bounty: Fixing the off by 1 error</h1>\n<p>I get my teacher forced accuracy (tfa) is always zero. Current code:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import torch\nimport torch.nn as nn\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    PreTrainedTokenizer,\n    PreTrainedModel,\n)\nfrom typing import List, Optional\nimport os\n\n\ndef seed_everything(seed: int = 0) -&gt; None:\n    &quot;&quot;&quot;\n    Seed all relevant libraries for reproducibility.\n    &quot;&quot;&quot;\n    import random\n    import numpy as np\n    from transformers import set_seed as hf_set_seed\n\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    hf_set_seed(seed)\n\n\ndef compute_tfa(\n    model: nn.Module,\n    tokenizer: PreTrainedTokenizer,\n    input_texts: List[str],\n    bos_token_id: Optional[int] = None,\n    eos_token_id: Optional[int] = None,\n    pad_token_id: Optional[int] = None,\n) -&gt; float:\n    &quot;&quot;&quot;\n    Calculate Teacher-Forced Accuracy (TFA) for a model.\n    &quot;&quot;&quot;\n    eos_token_id = eos_token_id or tokenizer.eos_token_id\n    bos_token_id = bos_token_id or tokenizer.bos_token_id or eos_token_id\n    pad_token_id = pad_token_id or tokenizer.pad_token_id or eos_token_id\n    tokenizer.pad_token_id = pad_token_id\n\n    inputs = tokenizer(input_texts, return_tensors='pt', padding=True, truncation=True)\n    input_ids = inputs['input_ids']\n\n    if input_ids[0, 0].item() != bos_token_id:\n        right_shifted_input_ids = torch.cat(\n            [torch.full((input_ids.size(0), 1), bos_token_id), input_ids[:, :-1]], dim=1\n        )\n        labels = input_ids\n    else:\n        right_shifted_input_ids = input_ids[:, :-1]\n        labels = input_ids[:, 1:]\n\n    with torch.no_grad():\n        logits = model(input_ids=right_shifted_input_ids).logits\n\n    predicted_ids = torch.argmax(logits, dim=-1)\n    eos_positions = (labels == eos_token_id).int().argmax(dim=1)\n    seq_len = labels.size(1)\n    mask = torch.arange(seq_len).unsqueeze(0).to(input_ids.device) &lt;= eos_positions.unsqueeze(1)\n\n    filtered_preds = predicted_ids[mask]\n    filtered_labels = labels[mask]\n\n    accuracy = (filtered_preds == filtered_labels).float().mean().item()\n    return accuracy\n\n\ndef main() -&gt; None:\n    os.environ['CUDA_VISIBLE_DEVICES'] = '7'\n    seed_everything(seed=123)\n\n    model_configs = [\n        {&quot;name&quot;: &quot;Meta-Llama-3-8B-Instruct&quot;, &quot;repo&quot;: &quot;meta-llama/Meta-Llama-3-8B-Instruct&quot;}\n    ]\n\n    input_texts = [&quot;The happy dog.&quot;]\n    code_input = [&quot;&lt;|fim_prefix|&gt;def solution():\\n    return True&lt;|fim_suffix|&gt;&lt;|fim_middle|&gt;&quot;]\n\n    for config in model_configs:\n        name, repo = config[&quot;name&quot;], config[&quot;repo&quot;]\n        print(f&quot;Evaluating {name} from {repo}&quot;)\n        model = AutoModelForCausalLM.from_pretrained(repo, trust_remote_code=True)\n        tokenizer = AutoTokenizer.from_pretrained(repo, trust_remote_code=True)\n\n        bos_id, eos_id, pad_id = tokenizer.bos_token_id, tokenizer.eos_token_id, tokenizer.pad_token_id\n\n        tfa_score = compute_tfa(model, tokenizer, input_texts, bos_id, eos_id, pad_id)\n        assert 0.0 &lt;= tfa_score &lt;= 1.0, f&quot;TFA out of range: {tfa_score}&quot;\n        print(f&quot;[{name}] TFA (General): {tfa_score:.4f}&quot;)\n\n        if &quot;codegemma-2b&quot; in name.lower():\n            tfa_code = compute_tfa(model, tokenizer, code_input, bos_id, eos_id, pad_id)\n            assert 0.0 &lt;= tfa_code &lt;= 1.0, f&quot;TFA (Code) out of range: {tfa_code}&quot;\n            print(f&quot;[{name}] TFA (Code Input): {tfa_code:.4f}&quot;)\n\n        print()\n\n\nif __name__ == &quot;__main__&quot;:\n    main()\n</code></pre>\n<h3>What might be wrong? TFA is always zero. Need careful reasoning and suggestions.</h3>\n<p>I’m trying to compute Teacher-Forced Accuracy (TFA) for a language model. The expected behavior is that the model predicts the input correctly under teacher-forcing, where the inputs are right-shifted by one token (prepended with <code>BOS</code> if necessary). However, the TFA score always comes out as <strong>zero</strong>, which is highly suspicious.</p>\n<h4>My Reasoning:</h4>\n<p>There are two cases regarding the presence of the <code>BOS</code> token, and my code handles them as follows:</p>\n<ol>\n<li><p><strong>When the tokenizer includes <code>BOS</code>:</strong></p>\n<ul>\n<li>The output from the tokenizer also includes <code>EOS</code> because my tokenizer code assumes <code>EOS</code> is present (and all models should have <code>EOS</code>).</li>\n<li>For the model input:\n<ul>\n<li>The input is right-shifted by one token.</li>\n<li>It’s fine to remove <code>EOS</code> from the input to avoid interfering with predictions.</li>\n</ul>\n</li>\n<li>For evaluation:\n<ul>\n<li>Remove the <code>BOS</code> token from the model's output to align predictions with labels.</li>\n<li>Ensure <code>EOS</code> in the predictions matches the labels correctly.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p><strong>When the tokenizer does not include <code>BOS</code>:</strong></p>\n<ul>\n<li>In this case, <code>BOS</code> is not set in the tokenizer (and I’ve already verified it’s absent).</li>\n<li>For the model input:\n<ul>\n<li>I prepend <code>BOS</code> to the input manually.</li>\n<li>It’s fine to remove <code>EOS</code> as before.</li>\n</ul>\n</li>\n<li>For evaluation:\n<ul>\n<li>Remove the <code>BOS</code> token from predictions and ensure proper alignment between predictions and labels.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ol>\n<h4>My Questions:</h4>\n<ul>\n<li>Isn’t this logic correct for computing TFA? It ensures alignment between inputs, predictions, and labels in both cases.</li>\n<li>If the code is logically correct, why is TFA always zero? A score of exactly zero is highly suspicious.</li>\n<li>Could there be an edge case or implementation detail that I’m missing?</li>\n</ul>\n<h4>What I Need:</h4>\n<ul>\n<li><strong>Reasoning:</strong> Please reason carefully through my explanation and code. Is my reasoning correct, or am I misunderstanding something about teacher-forcing or token alignment?</li>\n<li><strong>Suggestions:</strong> If something seems wrong or could be improved, what tests or changes should I make? I’d also appreciate specific code fixes if necessary.</li>\n<li><strong>Tests to Debug:</strong> What specific cases or checks should I run to pinpoint the issue?</li>\n</ul>\n<h1>Bounty 2:</h1>\n<p>Bounty 2 comment: Test TFA with the ProofNet dataset. This is the same sanity check we did\nfor the StackOverflow answer: <a href=\"https://stackoverflow.com/a/79379540/1601580\">https://stackoverflow.com/a/79379540/1601580</a>\nIt seems we decided this was right because the TFA was none-zero if I remember correctly.\nBut I'm not sure. It sanitcy check that definitively shows tfa is correct\nwould be nice.</p>\n",
        "answer": "<p>There are some improvements that can be made eg an example with chat template but that can be improved later if needed:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import os\nimport time\nimport torch\nfrom typing import Optional\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, PreTrainedModel\nfrom datasets import load_dataset\n\ndef seed_everything(seed: int = 42):\n    &quot;&quot;&quot;\n    Seed Python, NumPy, and PyTorch for reproducibility.\n    &quot;&quot;&quot;\n    import random\n    import numpy as np\n    from transformers import set_seed as hf_set_seed\n\n    print(f&quot;Seeding everything with seed={seed}&quot;)\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n    if torch.cuda.is_available():\n        hf_set_seed(seed)\n    else:\n        print(&quot;Warning: Full determinism is best on GPU.&quot;)\n\ndef teacher_forced_accuracy_tfa(\n    prompt: str,\n    response: str,\n    model: PreTrainedModel,\n    repo: str,\n    device: str = &quot;cuda&quot;\n) -&gt; float:\n    &quot;&quot;&quot;\n    Computes teacher-forced token-level accuracy for a prompt + response.\n    &quot;&quot;&quot;\n    combined_text = prompt + &quot;\\n\\n&quot; + response\n    tokenizer = AutoTokenizer.from_pretrained(repo, trust_remote_code=True)\n    enc = tokenizer(combined_text, return_tensors=&quot;pt&quot;)\n    input_ids = enc[&quot;input_ids&quot;].to(device)\n    attention_mask = enc[&quot;attention_mask&quot;].to(device)\n\n    with torch.no_grad():\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n    logits = outputs.logits\n    preds = torch.argmax(logits, dim=-1)\n\n    response_enc = tokenizer(response, add_special_tokens=False)\n    len_response = len(response_enc[&quot;input_ids&quot;])\n    prompt_enc = tokenizer(prompt, add_special_tokens=False)\n    len_prompt = len(prompt_enc[&quot;input_ids&quot;])\n    total_seq_len = input_ids.size(1)\n\n    if len_prompt + len_response &gt;= total_seq_len:\n        return 0.0\n\n    # Model predicts the next token, so offset labels by +1 in the response area\n    pred_slice = preds[:, len_prompt : len_prompt + len_response]\n    label_slice = input_ids[:, (len_prompt + 1) : (len_prompt + 1 + len_response)]\n\n    if pred_slice.size(1) == 0 or label_slice.size(1) == 0:\n        return 0.0\n\n    correctness = (pred_slice == label_slice).float()\n    return correctness.mean().item()\n\ndef compute_tfa_for_subds(\n    sub_ds,\n    model: PreTrainedModel,\n    repo: str,\n    prompt_format_fn=None,\n    device: str = &quot;cuda&quot;\n) -&gt; float:\n    &quot;&quot;&quot;\n    Computes the average teacher-forced accuracy over a subset of data.\n    &quot;&quot;&quot;\n    sum_acc = 0.0\n    count = 0\n\n    for i, example in enumerate(sub_ds):\n        nl_statement = example[&quot;nl_statement&quot;]\n        formal_statement = example[&quot;formal_statement&quot;]\n\n        if prompt_format_fn is not None:\n            prompt = prompt_format_fn(nl_statement)\n        else:\n            prompt = (\n                &quot;Translate the natural language version of the math statement &quot;\n                f&quot;to a formal Lean version:\\n{nl_statement}\\n&quot;\n            )\n\n        acc_i = teacher_forced_accuracy_tfa(\n            prompt=prompt,\n            response=formal_statement,\n            model=model,\n            repo=repo,\n            device=device\n        )\n        sum_acc += acc_i\n        count += 1\n        print(f&quot;Example {i}: TFA = {acc_i:.4f}&quot;)\n\n    return sum_acc / count if count &gt; 0 else 0.0\n\ndef main():\n    start_time = time.time()\n    os.environ['CUDA_VISIBLE_DEVICES'] = '4'\n    seed_everything()\n\n    ds = load_dataset(&quot;hoskinson-center/proofnet&quot;, split=&quot;validation&quot;)\n    N = 5\n    sub_ds = ds.select(range(min(N, len(ds))))\n\n    model_token_configs = [\n        {&quot;name&quot;: &quot;internlm2-math-plus-1_8b&quot;, &quot;repo&quot;: &quot;internlm/internlm2-math-plus-1_8b&quot;},\n        {&quot;name&quot;: &quot;google/gemma-2-2b&quot;, &quot;repo&quot;: &quot;google/gemma-2-2b&quot;},\n        {&quot;name&quot;: &quot;Mistral-7B-v0.1&quot;, &quot;repo&quot;: &quot;mistralai/Mistral-7B-v0.1&quot;},\n        {&quot;name&quot;: &quot;google/codegemma-2b&quot;, &quot;repo&quot;: &quot;google/codegemma-2b&quot;},\n        {&quot;name&quot;: &quot;Meta-Llama-3-8B&quot;, &quot;repo&quot;: &quot;meta-llama/Meta-Llama-3-8B&quot;},\n        {&quot;name&quot;: &quot;Meta-Llama-3-8B-Instruct&quot;, &quot;repo&quot;: &quot;meta-llama/Meta-Llama-3-8B-Instruct&quot;},\n        {&quot;name&quot;: &quot;google/gemma-2-2b-it&quot;, &quot;repo&quot;: &quot;google/gemma-2-2b-it&quot;},\n        {&quot;name&quot;: &quot;GPT-2 (small)&quot;, &quot;repo&quot;: &quot;gpt2&quot;},\n    ]\n\n    def my_prompt_format(nl_statement: str) -&gt; str:\n        return (\n            &quot;Translate the natural language version of the mathematical statement &quot;\n            f&quot;to a formal Lean version:\\n{nl_statement}\\n&quot;\n        )\n\n    device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;\n\n    for config in model_token_configs:\n        model_name = config[&quot;name&quot;]\n        repo = config[&quot;repo&quot;]\n\n        print(f&quot;\\nEvaluating {model_name} on {N} example(s).&quot;)\n        model = AutoModelForCausalLM.from_pretrained(repo, trust_remote_code=True).to(device)\n\n        avg_tfa = compute_tfa_for_subds(\n            sub_ds=sub_ds,\n            model=model,\n            repo=repo,\n            prompt_format_fn=my_prompt_format,\n            device=device\n        )\n        print(f&quot; =&gt; Average TFA for {model_name} = {avg_tfa:.4f}&quot;)\n\n    total_seconds = time.time() - start_time\n    print(f&quot;\\nDone. Total run time: {total_seconds:.2f} s.&quot;)\n\nif __name__ == &quot;__main__&quot;:\n    main()\n</code></pre>\n<p>eg with</p>\n<p><a href=\"https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct\" rel=\"nofollow noreferrer\">https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct</a></p>\n<pre class=\"lang-py prettyprint-override\"><code>from transformers import AutoModelForCausalLM, AutoTokenizer\n\ndevice = &quot;cuda&quot; # the device to load the model onto\n\nmodel = AutoModelForCausalLM.from_pretrained(&quot;mistralai/Mistral-7B-Instruct-v0.1&quot;)\ntokenizer = AutoTokenizer.from_pretrained(&quot;mistralai/Mistral-7B-Instruct-v0.1&quot;)\n\nmessages = [\n    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;What is your favourite condiment?&quot;},\n    {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!&quot;},\n    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Do you have mayonnaise recipes?&quot;}\n]\n\nencodeds = tokenizer.apply_chat_template(messages, return_tensors=&quot;pt&quot;)\n\nmodel_inputs = encodeds.to(device)\nmodel.to(device)\n\ngenerated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample=True)\ndecoded = tokenizer.batch_decode(generated_ids)\nprint(decoded[0])\n</code></pre>\n<p>or\n<a href=\"https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1\" rel=\"nofollow noreferrer\">https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1</a></p>\n<pre class=\"lang-py prettyprint-override\"><code>from transformers import AutoModelForCausalLM, AutoTokenizer\n\ndevice = &quot;cuda&quot; # the device to load the model onto\n\nmodel = AutoModelForCausalLM.from_pretrained(&quot;mistralai/Mistral-7B-Instruct-v0.1&quot;)\ntokenizer = AutoTokenizer.from_pretrained(&quot;mistralai/Mistral-7B-Instruct-v0.1&quot;)\n\nmessages = [\n    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;What is your favourite condiment?&quot;},\n    {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!&quot;},\n    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Do you have mayonnaise recipes?&quot;}\n]\n\nencodeds = tokenizer.apply_chat_template(messages, return_tensors=&quot;pt&quot;)\n\nmodel_inputs = encodeds.to(device)\nmodel.to(device)\n\ngenerated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample=True)\ndecoded = tokenizer.batch_decode(generated_ids)\nprint(decoded[0])\n</code></pre>\n<p>It could also be vectorized for speed.</p>\n<p>More references:</p>\n<ul>\n<li><a href=\"https://github.com/kaifronsdal/TheoremSense/blob/main/theoremsense/model_generate.py\" rel=\"nofollow noreferrer\">https://github.com/kaifronsdal/TheoremSense/blob/main/theoremsense/model_generate.py</a></li>\n<li><a href=\"https://github.com/kaifronsdal/TheoremSense/blob/main/theoremsense/old/compute_TFA.py\" rel=\"nofollow noreferrer\">https://github.com/kaifronsdal/TheoremSense/blob/main/theoremsense/old/compute_TFA.py</a></li>\n</ul>\n<p>sample output when running above TFA with ProofNet Lean3:</p>\n<pre><code>(zip_fit) brando9@skampere1~/ZIP-FIT $  cd /lfs/skampere1/0/brando9/ZIP-FIT ; /usr/bin/env /lfs/skampere1/0/brando9/miniconda/envs/zip_fit/bin/python /lfs/skampere1/0/brando9/.vscode-server-insiders/extensions/ms-python.debugpy-2024.14.0-linux-x64/bundled/libs/debugpy/adapter/../../debugpy/launcher 56493 -- /lfs/skampere1/0/brando9/ZIP-FIT/zip_fit/tfa.py \n/lfs/skampere1/0/brando9/miniconda/envs/zip_fit/lib/python3.11/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\nSetting random seed = 42\n\nEvaluating internlm2-math-plus-1_8b from internlm/internlm2-math-plus-1_8b on 5 example(s) of ProofNet validation.\nA new version of the following files was downloaded from https://huggingface.co/internlm/internlm2-math-plus-1_8b:\n- configuration_internlm2.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nA new version of the following files was downloaded from https://huggingface.co/internlm/internlm2-math-plus-1_8b:\n- modeling_internlm2.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nA new version of the following files was downloaded from https://huggingface.co/internlm/internlm2-math-plus-1_8b:\n- tokenization_internlm2_fast.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n Example 0: TFA = 0.5789\nA new version of the following files was downloaded from https://huggingface.co/internlm/internlm2-math-plus-1_8b:\n- tokenization_internlm2_fast.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n Example 1: TFA = 0.7742\nA new version of the following files was downloaded from https://huggingface.co/internlm/internlm2-math-plus-1_8b:\n- tokenization_internlm2_fast.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n Example 2: TFA = 0.7424\nA new version of the following files was downloaded from https://huggingface.co/internlm/internlm2-math-plus-1_8b:\n- tokenization_internlm2_fast.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n Example 3: TFA = 0.8269\nA new version of the following files was downloaded from https://huggingface.co/internlm/internlm2-math-plus-1_8b:\n- tokenization_internlm2_fast.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n Example 4: TFA = 0.7500\n =&gt; Average TFA for internlm2-math-plus-1_8b on these 5 example(s) = 0.7345\n =&gt; Time to compute TFA for internlm2-math-plus-1_8b: 12.18 seconds.\n\nEvaluating google/gemma-2-2b from google/gemma-2-2b on 5 example(s) of ProofNet validation.\nLoading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00&lt;00:00,  9.25it/s]\nThe 'batch_size' argument of HybridCache is deprecated and will be removed in v4.49. Use the more precisely named 'max_batch_size' argument instead.\nThe 'batch_size' attribute of HybridCache is deprecated and will be removed in v4.49. Use the more precisely named 'self.max_batch_size' attribute instead.\n Example 0: TFA = 0.5000\n Example 1: TFA = 0.5185\n Example 2: TFA = 0.4225\n Example 3: TFA = 0.6591\n Example 4: TFA = 0.6571\n =&gt; Average TFA for google/gemma-2-2b on these 5 example(s) = 0.5515\n =&gt; Time to compute TFA for google/gemma-2-2b: 6.12 seconds.\n\nEvaluating Mistral-7B-v0.1 from mistralai/Mistral-7B-v0.1 on 5 example(s) of ProofNet validation.\nDownloading shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00&lt;00:00, 10485.76it/s]\nLoading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01&lt;00:00,  1.21it/s]\n Example 0: TFA = 0.6429\n Example 1: TFA = 0.7500\n Example 2: TFA = 0.7125\n Example 3: TFA = 0.7407\n Example 4: TFA = 0.6923\n =&gt; Average TFA for Mistral-7B-v0.1 on these 5 example(s) = 0.7077\n =&gt; Time to compute TFA for Mistral-7B-v0.1: 6.43 seconds.\n\nEvaluating google/codegemma-2b from google/codegemma-2b on 5 example(s) of ProofNet validation.\nDownloading shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00&lt;00:00, 10525.23it/s]\nLoading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00&lt;00:00,  2.28it/s]\n Example 0: TFA = 0.3611\n Example 1: TFA = 0.4815\n Example 2: TFA = 0.3521\n Example 3: TFA = 0.4318\n Example 4: TFA = 0.4286\n =&gt; Average TFA for google/codegemma-2b on these 5 example(s) = 0.4110\n =&gt; Time to compute TFA for google/codegemma-2b: 6.24 seconds.\n\nEvaluating Meta-Llama-3-8B from meta-llama/Meta-Llama-3-8B on 5 example(s) of ProofNet validation.\nDownloading shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00&lt;00:00, 12965.39it/s]\nLoading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:02&lt;00:00,  1.70it/s]\n Example 0: TFA = 0.5882\n Example 1: TFA = 0.6923\n Example 2: TFA = 0.7049\n Example 3: TFA = 0.8293\n Example 4: TFA = 0.7353\n =&gt; Average TFA for Meta-Llama-3-8B on these 5 example(s) = 0.7100\n =&gt; Time to compute TFA for Meta-Llama-3-8B: 8.12 seconds.\n\nEvaluating Meta-Llama-3-8B-Instruct from meta-llama/Meta-Llama-3-8B-Instruct on 5 example(s) of ProofNet validation.\nDownloading shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00&lt;00:00, 14004.35it/s]\nLoading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:01&lt;00:00,  2.10it/s]\n Example 0: TFA = 0.5000\n Example 1: TFA = 0.7308\n Example 2: TFA = 0.6721\n Example 3: TFA = 0.8537\n Example 4: TFA = 0.8235\n =&gt; Average TFA for Meta-Llama-3-8B-Instruct on these 5 example(s) = 0.7160\n =&gt; Time to compute TFA for Meta-Llama-3-8B-Instruct: 7.55 seconds.\n\nEvaluating google/gemma-2-2b-it from google/gemma-2-2b-it on 5 example(s) of ProofNet validation.\nDownloading shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00&lt;00:00, 4888.47it/s]\nLoading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00&lt;00:00,  2.35it/s]\n Example 0: TFA = 0.3611\n Example 1: TFA = 0.4815\n Example 2: TFA = 0.4366\n Example 3: TFA = 0.5000\n Example 4: TFA = 0.5429\n =&gt; Average TFA for google/gemma-2-2b-it on these 5 example(s) = 0.4644\n =&gt; Time to compute TFA for google/gemma-2-2b-it: 6.26 seconds.\n\nEvaluating GPT-2 (small) from gpt2 on 5 example(s) of ProofNet validation.\n Example 0: TFA = 0.2821\n Example 1: TFA = 0.1852\n Example 2: TFA = 0.2800\n Example 3: TFA = 0.2553\n Example 4: TFA = 0.2162\n =&gt; Average TFA for GPT-2 (small) on these 5 example(s) = 0.2438\n =&gt; Time to compute TFA for GPT-2 (small): 1.42 seconds.\n\nDone. Total run time for all models: 55.71 seconds.\n</code></pre>\n<h1>Reproduction</h1>\n<p>Indeed the above reproduces: <a href=\"https://wandb.ai/brando/zip-fit-tfa-tests/runs/sxi8y9w3/overview\" rel=\"nofollow noreferrer\">https://wandb.ai/brando/zip-fit-tfa-tests/runs/sxi8y9w3/overview</a></p>\n<p>Code that reproduced it:</p>\n<pre><code>import os\nimport time\nfrom typing import Dict, Any\n\nfrom datasets import load_dataset\nimport torch\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nfrom zip_fit.metrics.tfa import compute_tfa_for_subds, tfa_teacher_forced_accuracy\nfrom zip_fit.utils import seed_everything\n\ndef test_tfa(config: Dict[str, Any] = {}):\n    # - Seed everything\n    seed_everything(config.get('seed', 42))\n\n    # - Load model and tokenizer\n    model_name: str = config.get('model_name', 'meta-llama/Meta-Llama-3-8B-Instruct')\n    # tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n    model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\n    \n    # Move model to CUDA if available\n    device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;\n    model = model.to(device)\n\n    # - \n    prompt: str = &quot;Question: 1 + 1 = ?\\n\\n&quot;\n    gold_response: str = &quot;Solution: The answer is 2.\\n\\n&quot;\n    tfa: float = tfa_teacher_forced_accuracy(prompt, gold_response, model, model_name, device=device)\n    print(f'{tfa=}')\n\ndef test_tfa_with_proof_net():\n    &quot;&quot;&quot;\n    Test TFA with the ProofNet dataset. This is the same sanity check we did \n    for the StackOverflow answer: https://stackoverflow.com/a/79379540/1601580 \n    It seems we decided this was right because the TFA was none-zero if I remember correctly.\n    But I'm not sure. It sanitcy check that definitively shows tfa is correct\n    would be nice.\n\n    wanbd run: https://wandb.ai/brando/zip-fit-tfa-tests/runs/sxi8y9w3/overview \n    &quot;&quot;&quot;\n    global_start_time = time.time()  # Start overall timer\n    os.environ['CUDA_VISIBLE_DEVICES'] = '4'  # choose GPU\n    seed_everything()\n\n    # 1) Load the ProofNet validation set\n    ds = load_dataset(&quot;hoskinson-center/proofnet&quot;, split=&quot;validation&quot;)\n\n    # load_datasetstom prompt format function\n    def my_prompt_format(prompt: str) -&gt; str:\n        return (\n            &quot;Translate the natural language version of the mathematical statement &quot;\n            f&quot;to a formal Lean version:\\n{prompt}\\n&quot;\n        )\n    ds = ds.map(lambda example: {'prompt': \n                                 my_prompt_format(example['nl_statement']), \n                                 'gold_response': example['formal_statement']}, \n                                 num_proc=48)\n\n    # We'll just do the first N examples for demonstration\n    N = 5\n    sub_ds = ds.select(range(min(N, len(ds))))\n\n    # 2) Our model list (including all desired models, even if some remain commented)\n    model_token_configs = [\n        {\n            &quot;name&quot;: &quot;internlm2-math-plus-1_8b&quot;,\n            &quot;repo&quot;: &quot;internlm/internlm2-math-plus-1_8b&quot;,\n        },\n        {\n            &quot;name&quot;: &quot;google/gemma-2-2b&quot;,\n            &quot;repo&quot;: &quot;google/gemma-2-2b&quot;,\n        },\n        {\n            &quot;name&quot;: &quot;Mistral-7B-v0.1&quot;,\n            &quot;repo&quot;: &quot;mistralai/Mistral-7B-v0.1&quot;,\n        },\n        {\n            &quot;name&quot;: &quot;google/codegemma-2b&quot;,\n            &quot;repo&quot;: &quot;google/codegemma-2b&quot;,\n        },\n        # {\n        #     &quot;name&quot;: &quot;GPT-2 (small)&quot;,\n        #     &quot;repo&quot;: &quot;gpt2&quot;,\n        # },\n    ]\n    device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;\n\n    for config in model_token_configs:\n        model_name = config[&quot;name&quot;]\n        repo = config[&quot;repo&quot;]\n\n        print(f&quot;\\nEvaluating {model_name} from {repo} on {N} example(s) of ProofNet validation.&quot;)\n\n        # Start per-model timer\n        model_start_time = time.time()\n\n        model = AutoModelForCausalLM.from_pretrained(repo, trust_remote_code=True).to(device)\n        avg_tfa = compute_tfa_for_subds(\n            sub_ds=sub_ds,\n            model=model,\n            repo=repo,\n            device=device\n        )\n\n        # End per-model timer\n        model_end_time = time.time()\n        model_seconds = model_end_time - model_start_time\n\n        print(f&quot; =&gt; Average TFA for {model_name} on these {N} example(s) = {avg_tfa:.4f}&quot;)\n        print(f&quot; =&gt; Time to compute TFA for {model_name}: {model_seconds:.2f} seconds.&quot;)\n\n    # End overall timer\n    global_end_time = time.time()\n    total_seconds = global_end_time - global_start_time\n    print(f&quot;\\nDone. Total run time for all models: {total_seconds:.2f} seconds.&quot;)\n\ndef main():\n    &quot;&quot;&quot; Runa all tests. &quot;&quot;&quot;\n    test_tfa()\n    test_tfa_with_proof_net()\n\ndef _main(**kwargs):\n    from datetime import datetime\n    from socket import gethostname\n    import wandb\n    today = datetime.now().strftime('%Y_m%m_d%d_t%Hh_%Mm_%Ss') # eg '2024_m01_d22_t13h_00m_30s'\n    tmux_sess_num = None\n    kwargs = kwargs | {'today': today, 'tmux_sess_num': tmux_sess_num, 'hostname': gethostname()}\n    run_name = f'{kwargs}' \n    project: str = kwargs.get('project', 'zip-fit-tfa-tests')\n    run = wandb.init(mode=kwargs.get('mode', 'dryrun'), project=project, name=run_name, save_code=True, config=kwargs)\n    wandb.save(__file__) # save current code now, don't wait to wandb.finish, also useful: wandb.save(&quot;*.py&quot;) # upload all .py files in current directory\n    print(f'Kwargs to run:\\n{kwargs}')\n    main()\n    run.alert(title=&quot;Run Completed&quot;, text=f&quot;Run finished, run url: {run.get_url()}&quot;)\n    print(f'{run.get_url()=}')\n    wandb.finish()\n\nif __name__ == &quot;__main__&quot;:\n    import fire\n    import time\n    start_time = time.time()\n    fire.Fire(_main)\n    print(f&quot;\\aTime taken: {time.time() - start_time:.2f} seconds, or {(time.time() - start_time) / 60:.2f} minutes, or {(time.time() - start_time) / 3600:.2f} hours.\\a&quot;)\n\n\n</code></pre>\n<p>and the tfa.py file:</p>\n<pre><code>import torch\nfrom transformers import AutoTokenizer, PreTrainedModel\nfrom datasets import Dataset\n\ndef tfa_teacher_forced_accuracy(\n    prompt: str,\n    gold_response: str,\n    model: PreTrainedModel,\n    repo: str, # HF repo name needed to load the correct tokenizer\n    device: str = &quot;cuda&quot;\n) -&gt; float:\n    &quot;&quot;&quot;\n    Teacher-forced accuracy (token-level) on `gold_response` given a concatenated text = prompt + gold_response.\n\n    Steps:\n      1) Combined text = prompt + &quot;\\n\\n&quot; + gold_response\n      2) Tokenize combined text =&gt; shape: (1, total_seq_len)\n      3) Forward pass =&gt; logits shape: (1, total_seq_len, vocab_size)\n      4) Identify the token range for the gold_response\n      5) Compare the predicted tokens in that range with the reference gold_response tokens\n      6) Return fraction matched in [0, 1]\n\n    Notes about BOS/EOS/PAD:\n      - Because we do per-example calls (prompt+gold_response) only, no extra padding is needed.\n      - We do not forcibly add BOS or EOS here. We skip it to match a &quot;bare-bones&quot; style,\n        similar to the updated tfa.py that also ignores explicit BOS/EOS tokens.\n      - If the combined text is truncated or too short, we return 0.0 as a fallback.\n\n    Correctness:\n      - We allow the to\n    &quot;&quot;&quot;\n    # 1) Combine text\n    combined_text = prompt + &quot;\\n\\n&quot; + gold_response\n\n    # 2) Get tokenizer from the `repo` -- getting from repo to ensure user doesn't give the wrong tokenizer TODO verify this statement with more sanity checks\n    tokenizer = AutoTokenizer.from_pretrained(repo, trust_remote_code=True)\n\n    # 3) Tokenize entire reference\n    enc = tokenizer(combined_text, return_tensors=&quot;pt&quot;)\n    # shape: (1, total_seq_len)\n    input_ids = enc[&quot;input_ids&quot;].to(device)\n    attention_mask = enc[&quot;attention_mask&quot;].to(device)\n\n    with torch.no_grad():\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n    logits = outputs.logits  # shape: (1, total_seq_len, vocab_size)\n    preds = torch.argmax(logits, dim=-1)  # shape: (1, total_seq_len)\n\n    # 4) Tokenize the gold_response alone to find how many tokens it has\n    gold_response_enc = tokenizer(gold_response, add_special_tokens=False)\n    len_gold_response = len(gold_response_enc[&quot;input_ids&quot;])\n\n    # Tokenize the prompt alone for length\n    prompt_enc = tokenizer(prompt, add_special_tokens=False)\n    len_prompt = len(prompt_enc[&quot;input_ids&quot;])\n\n    total_seq_len = input_ids.size(1)\n\n    # If the combined text is too short or the gold_response doesn't fit, skip\n    if len_prompt + len_gold_response &gt;= total_seq_len:\n        return 0.0\n\n    # Teacher forcing alignment:\n    #   model's position t attempts to predict token at position t+1\n    pred_slice = preds[:, len_prompt : (len_prompt + len_gold_response)]\n    label_slice = input_ids[:, (len_prompt + 1) : (len_prompt + 1 + len_gold_response)]\n\n    if pred_slice.size(1) == 0 or label_slice.size(1) == 0:\n        return 0.0\n\n    correctness = (pred_slice == label_slice).float()  # shape: (1, number_of_gold_response_tokens)\n    acc = correctness.mean().item()\n    return acc\n\n\ndef compute_tfa_for_subds(\n    sub_ds: Dataset,\n    model: PreTrainedModel,\n    repo: str, # HF repo name needed to load the correct tokenizer\n    device: str = &quot;cuda&quot;,\n    debug: bool = False,\n) -&gt; float:\n    &quot;&quot;&quot;\n    Process an entire subset of data (sub_ds) and compute the average TFA across all examples.\n\n    Parameters:\n      sub_ds: The subset of the dataset (like a HuggingFace 'Dataset' slice).\n      model:  A language model (transformers PreTrainedModel).\n      repo:   The model repo string, used to load the correct tokenizer in tfa_teacher_forced_accuracy.\n      device: 'cuda' or 'cpu'.\n\n    Returns:\n      float: The average TFA over all examples in sub_ds.\n    &quot;&quot;&quot;\n    sum_acc = 0.0\n    count = 0\n\n    for i, example in enumerate(sub_ds):\n        prompt = example[&quot;prompt&quot;]\n        gold_response = example[&quot;gold_response&quot;]\n\n        acc_i = tfa_teacher_forced_accuracy(\n            prompt=prompt,\n            gold_response=gold_response,\n            model=model,\n            repo=repo,\n            device=device\n        )\n        sum_acc += acc_i\n        count += 1\n\n        print(f&quot; Example {i}: TFA = {acc_i:.4f}&quot;) if debug else None\n\n    return sum_acc / count if count &gt; 0 else 0.0\n</code></pre>\n<p>What a relief, enjoy! But hopefully indeed this is correct.</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 79209319,
            "link": "https://stackoverflow.com/questions/79209319/how-to-compute-teacher-forced-accuracy-tfa-for-hugging-face-models-while-handl"
        }
    },
    {
        "question": "&quot;Inconsistent Predictions in PyTorch Model: Single Image vs. Batch Processing&quot;\n<p>I am noticing a significant difference in model predictions when running predictions on a single image versus the whole dataset. The model, which was trained using PyTorch, gives drastically different predictions for the same image when processed individually versus in a batch. Is there any way to ensure that the predictions are consistent for the same image when processed individually and in a batch?</p>\n<pre><code>from transformers import Trainer, TrainingArguments, PreTrainedModel, PretrainedConfig\nfrom torch.utils.data import Dataset\nimport torch\nimport torch.nn.functional as F\nimport numpy as np\n\n# Number of Features\nnum_of_features = 128\n\n# Dataset Class\nclass SequenceDataset(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        return {&quot;input_ids&quot;: self.X[idx], &quot;labels&quot;: self.y[idx]}\n\n\n# Configuration Class\nclass SequenceConfig(PretrainedConfig):\n    model_type = &quot;sequence_transformer&quot;\n\n    def __init__(self, num_features=num_of_features, num_classes=3, d_model=1024, nhead=4, num_layers=4, dim_feedforward=512, **kwargs):\n        self.num_features = num_features\n        self.num_classes = num_classes\n        self.d_model = d_model\n        self.nhead = nhead\n        self.num_layers = num_layers\n        self.dim_feedforward = dim_feedforward\n        super().__init__(**kwargs)\n\n\n# Transformer Model\nclass SequenceTransformer(PreTrainedModel):\n    config_class = SequenceConfig\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.embedding = torch.nn.Linear(config.num_features, config.d_model)\n        self.positional_encoding = torch.nn.Parameter(torch.zeros(1, config.d_model))\n        encoder_layer = torch.nn.TransformerEncoderLayer(\n            d_model=config.d_model, \n            nhead=config.nhead, \n            dim_feedforward=config.dim_feedforward, \n            batch_first=True\n        )\n        self.transformer_encoder = torch.nn.TransformerEncoder(encoder_layer, num_layers=config.num_layers)\n        self.fc = torch.nn.Linear(config.d_model, config.num_classes)\n\n    def forward(self, input_ids, labels=None):\n        src = self.embedding(input_ids) + self.positional_encoding\n        output = self.transformer_encoder(src)\n        logits = self.fc(output)\n        probs = F.softmax(logits, dim=-1)\n\n        loss = None\n        if labels is not None:\n            loss_fct = torch.nn.CrossEntropyLoss()\n            loss = loss_fct(logits, labels)\n            \n        return {&quot;loss&quot;: loss, &quot;logits&quot;: logits, &quot;probs&quot;: probs} if labels is not None else logits\n\n\n# Training Code\nconfig = SequenceConfig()\nmodel = SequenceTransformer(config)\n\n# Training Arguments\n    batchSize=32\n    numWarmUpSteps=int(np.shape(train_image)[0]/batchSize/numOfBreakpointsPerEpoch/10)\n    training_args = TrainingArguments(\n        output_dir=path,\n        num_train_epochs=1, \n        per_device_train_batch_size=batchSize,\n        per_device_eval_batch_size=320,\n        warmup_steps=numWarmUpSteps,\n        weight_decay=0.1,\n        logging_strategy='no',\n        eval_strategy=&quot;epoch&quot;,\n        save_strategy=&quot;epoch&quot;,\n        metric_for_best_model=&quot;accuracy&quot;,\n        save_only_model=True,\n    )\n\n# Trainer Initialization\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=compute_metrics\n)\n\n# Train the Model\ntrain_output = trainer.train()\n\n# Save Model and Training Arguments\ntrainer.save_model(&quot;./SavedModels&quot;)\ntorch.save(training_args, &quot;./SavedModels/training_args.bin&quot;)\n\n# Prediction Code\ntraining_args_loaded = torch.load(&quot;./SavedModels/training_args.bin&quot;)\nmodel_save_path = &quot;./SavedModels/&quot;\nmodel = SequenceTransformer(config).from_pretrained(model_save_path)\n\ntrainer = Trainer(model=model, compute_metrics=compute_metrics, args=training_args_loaded)\ntest_data = np.random.rand(10, num_of_features)  # Example test data\ntest_predictions = trainer.predict(torch.tensor(test_data, dtype=torch.float32))\n\n# Output Test Predictions\nprint(test_predictions)\n</code></pre>\n<p>For single image its [0.37732467 0.2642143 0.35846105]\nand for that same image in batch its [0.3185594 0.40971586 0.2717247 ].</p>\n",
        "answer": "<p>I have run into the same issue myself and to my best understanding these tiny differences (≈2.3e-10) are due to floating point non-associativity or microkernel behavior in batched matrix ops. Apparently, It is expected in complex models with Attention layers.</p>\n<p>These differences are not a bug! They are not fixed by setting options like <strong><code>torch.use_deterministic_algorithms(True)</code></strong>, and they are not due to randomness, but due to numerical artifacts of batching.</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 79294216,
            "link": "https://stackoverflow.com/questions/79294216/inconsistent-predictions-in-pytorch-model-single-image-vs-batch-processing"
        }
    },
    {
        "question": "How to finetune an LLM model on your own codebase?\n<p>I have 10 code repositories in Javascript (VueJS) (Each repository corresponds to 1 Theme)</p>\n<p>I want to train an LLM model on these 10 code repositories to generate new themes using prompts.</p>\n<p>The LLM model takes the context of 10 code repositories as a reference (since the file structure is similar for all repositories)</p>\n<p>I'm a complete beginner with LLMs and ML.</p>\n<p>How to finetune an LLM model on my codebase?</p>\n",
        "answer": "<p>So what you can actually do is create a custom prompt and pass in one or two examples such that the llm learns from one shot or few shot. Later in the custom prompt you can pass in the paramters on which you want the llm to take decision and give you the output. For such tasks I feel OpenAI models like GPT-3 and other's work well.</p>\n<p>Hope it answer's your question:)</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 76471292,
            "link": "https://stackoverflow.com/questions/76471292/how-to-finetune-an-llm-model-on-your-own-codebase"
        }
    },
    {
        "question": "How to force langchain to use HF_DATA environment variable to load the model from local disk instead of Internet\n<p>How to force langchain to use <code>HF_DATA</code> environment variable to load the model.</p>\n<p>The <code>Snowflake/snowflake-arctic-embed-l</code> model files have been downloaded to <code>$HF_HOME/Snowflake/snowflake-arctic-embed-l</code>.</p>\n<pre><code>$ echo $HF_HOME\n/tmp-data\n\n$ls /tmp-data/Snowflake/snowflake-arctic-embed-l\n1_Pooling    README.md    config_sentence_transformers.json  model.safetensors  sentence_bert_config.json  tokenizer.json         vocab.txt\n2_Normalize  config.json  hoge.tgz                           modules.json       special_tokens_map.json    tokenizer_config.json\n</code></pre>\n<p>Python runtime acknowledges <code>HF_DATA</code> environment variable.</p>\n<pre><code>&gt;&gt;&gt; import os\n&gt;&gt;&gt; os.getenv(&quot;HF_HOME&quot;)\n'/tmp-data'\n</code></pre>\n<p>However, it tries to download the model from the internet.</p>\n<pre><code>from langchain.text_splitter import SentenceTransformersTokenTextSplitter\n\nsplitter=SentenceTransformersTokenTextSplitter(\n  model_name=&quot;Snowflake/snowflake-arctic-embed-l&quot;, \n  tokens_per_chunk=500, \n  chunk_overlap=50\n)\n</code></pre>\n<pre><code>No sentence-transformers model found with name Snowflake/snowflake-arctic-embed-l. Creating a new one with mean pooling.\nTraceback (most recent call last):\n  File &quot;/usr/local/lib/python3.10/dist-packages/urllib3/connection.py&quot;, line 198, in _new_conn\n    sock = connection.create_connection(\n  File &quot;/usr/local/lib/python3.10/dist-packages/urllib3/util/connection.py&quot;, line 60, in create_connection\n    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n  File &quot;/usr/lib/python3.10/socket.py&quot;, line 955, in getaddrinfo\n    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\nsocket.gaierror: [Errno -2] Name or service not known\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File &quot;/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py&quot;, line 787, in urlopen\n    response = self._make_request(\n  File &quot;/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py&quot;, line 488, in _make_request\n    raise new_e\n  File &quot;/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py&quot;, line 464, in _make_request\n    self._validate_conn(conn)\n  File &quot;/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py&quot;, line 1093, in _validate_conn\n    conn.connect()\n  File &quot;/usr/local/lib/python3.10/dist-packages/urllib3/connection.py&quot;, line 704, in connect\n    self.sock = sock = self._new_conn()\n  File &quot;/usr/local/lib/python3.10/dist-packages/urllib3/connection.py&quot;, line 205, in _new_conn\n    raise NameResolutionError(self.host, self, e) from e\nurllib3.exceptions.NameResolutionError: &lt;urllib3.connection.HTTPSConnection object at 0x7d1562b96680&gt;: Failed to resolve 'huggingface.co' ([Errno -2] Name or service not known)\n</code></pre>\n<p>Giving the full path to the local model directory fixes the issue but need to utilise <code>HF_HOME</code>.</p>\n<pre><code>splitter=SentenceTransformersTokenTextSplitter(model_name=&quot;/tmp-data/Snowflake/snowflake-arctic-embed-l&quot;, tokens_per_chunk=500, chunk_overlap=50)\nYou try to use a model that was created with version 3.4.1, however, your version is 3.0.0. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n</code></pre>\n",
        "answer": "<p>There's no built-in support for <code>HF_HOME</code> in langchain (or it's dependencies - <a href=\"https://github.com/UKPLab/sentence-transformers\" rel=\"nofollow noreferrer\">https://github.com/UKPLab/sentence-transformers</a> in this case). You'll just have to set up a method to prefix your model-names with the <code>HF_HOME</code> variable on your end. Unless you're open to monkey patching the library, but I'm not suggesting that for something like this.</p>\n<p>Something like this perhaps:</p>\n<pre><code>from langchain.text_splitter import SentenceTransformersTokenTextSplitter\nimport os\n\n\nHF_HOME = os.environ.get(&quot;HF_HOME&quot;)\n\n\ndef prefix_hf_home(model_name):\n    return os.path.join(HF_HOME, model_name)\n\n\nsplitter=SentenceTransformersTokenTextSplitter(\n  model_name=prefix_hf_home(&quot;Snowflake/snowflake-arctic-embed-l&quot;), \n  tokens_per_chunk=500, \n  chunk_overlap=50\n)\n</code></pre>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 79469538,
            "link": "https://stackoverflow.com/questions/79469538/how-to-force-langchain-to-use-hf-data-environment-variable-to-load-the-model-fro"
        }
    },
    {
        "question": "`repo_type` argument if needed\n<p>Now trying to run a python script, loading a model from hugging face.\nIn the terminal it gives an error:</p>\n<pre><code>huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': 'Jzuluaga/accent-id-commonaccent_ecapa/data'. Use `repo_type` argument if needed.\n</code></pre>\n<p>Sure that there is no typo in the name of model.</p>\n<p>Im sure as well that the model is public, but have no idea why it is not letting me in.</p>\n",
        "answer": "<p>There's an extra /data at the end of <code>Jzuluaga/accent-id-commonaccent_ecapa/data</code> - try <code>Jzuluaga/accent-id-commonaccent_ecapa</code> as the message implies.</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 79217834,
            "link": "https://stackoverflow.com/questions/79217834/repo-type-argument-if-needed"
        }
    },
    {
        "question": "Image segmentation ONNX from huggingface produces very diferent results when used in ML.Net\n<p>I have been trying to get an image segmentation model from huggingface (<a href=\"https://huggingface.co/briaai/RMBG-2.0\" rel=\"nofollow noreferrer\">RMBG-2.0</a>) to work for inference using ML.NET. After a lot of trial and error, I finally got the code to compile and produce an output but it is wildly different from the result i get from using the demo on huggingface.</p>\n<p>The code:</p>\n<pre><code>public static void RemoveGreenBackgroundAI2(string imagePath, string outputfile)\n{\n    string modelPath = Path.Combine( Application.StartupPath,&quot;ONNX&quot;,&quot;model.onnx&quot;); \n    MLContext mlContext = new MLContext();\n\n    var imageData = new ImageInputData\n    {\n        Image = MLImage.CreateFromFile (imagePath)\n    };\n\n\n    var imageDataView = mlContext.Data.LoadFromEnumerable(new[] { imageData });\n   \n   var pipeline = mlContext.Transforms.ResizeImages(\n                        outputColumnName: &quot;input&quot;,\n                        imageWidth: 1024,\n                        imageHeight: 1024,\n                        inputColumnName: nameof(ImageInputData.Image))\n                  .Append(mlContext.Transforms.ExtractPixels(\n                        outputColumnName: &quot;out1&quot;,\n                        inputColumnName: &quot;input&quot;,\n                        interleavePixelColors: true,\n                        scaleImage: 1f / 255f,\n                        offsetImage: 0,\n                        outputAsFloatArray: true))\n                    .Append(mlContext.Transforms.CustomMapping&lt;CustomMappingInput, CustomMappingOutput&gt;( \n                       mapAction: (input, output) =&gt;\n                        {\n                            output.pixel_values = new float[input.out1.Length];\n                            for (int i = 0; i &lt; input.out1.Length; i += 3)\n                            {\n                                // R\n                                output.pixel_values[i] = (input.out1[i] - 0.485f) / 0.229f;\n\n                                //G\n                                output.pixel_values[i + 1] = (input.out1[i + 1] - 0.456f) / 0.224f;\n\n                                //B\n                                output.pixel_values[i + 2] = (input.out1[i + 2] - 0.406f) / 0.225f;\n                            }\n                        }, contractName: null))\n                  .Append(mlContext.Transforms.ApplyOnnxModel(\n                        modelFile: modelPath,\n                        outputColumnNames: new[] { &quot;alphas&quot; },\n                        inputColumnNames: new[] { &quot;pixel_values&quot; },\n                        shapeDictionary: new Dictionary&lt;string, int[]&gt;\n                        {\n                            { &quot;pixel_values&quot;, new[] { 1, 3, 1024, 1024 } }\n\n                        },\n                        fallbackToCpu:true,\n                        gpuDeviceId:null\n                        ));\n\n    \n    var model = pipeline.Fit(imageDataView);\n    var predictionEngine = mlContext.Model.CreatePredictionEngine&lt;ImageInputData, ModelOutput&gt;(model);\n    var prediction = predictionEngine.Predict(imageData);\n    ApplyMaskAndSaveImage(imagePath, prediction, outputfile);\n\n}\n\npublic static void ApplyMaskAndSaveImage(string originalImagepath, ModelOutput prediction, string outputPath)\n{\n    int width = 1024;\n    int height = 1024;\n    float[] outputData = prediction.Output;\n\n    Bitmap originalImage = (Bitmap)Bitmap.FromFile(originalImagepath);\n    int originalWidth = originalImage.Width;\n    int originalHeight = originalImage.Height;\n\n    Bitmap resizedImage = new Bitmap(originalImage, new System.Drawing.Size(width, height));\n    Bitmap outputImage = new Bitmap(width, height, PixelFormat.Format32bppArgb);\n\n    for (int y = 0; y &lt; height; y++)\n    {\n        for (int x = 0; x &lt; width; x++)\n        {\n            float maskValue = outputData[y * width + x];\n            float threshold = 0.5f;\n            byte alpha = maskValue &gt;= threshold ? (byte)255 : (byte)0;\n            Color pixelColor = resizedImage.GetPixel(x, y);\n            Color newColor = Color.FromArgb(alpha, pixelColor.R, pixelColor.G, pixelColor.B);\n            outputImage.SetPixel(x, y, newColor);\n        }\n    }      \n    outputImage.Save(outputPath, ImageFormat.Png);\n}\n\npublic class ModelOutput\n{\n    [ColumnName(&quot;alphas&quot;)]\n    [VectorType(1, 1, 1024, 1024)]\n    public float[] Output { get; set; }\n}\npublic class ImageInputData\n{\n    [ColumnName(&quot;Image&quot;)]\n    [ImageType(1024, 1024)]\n    public MLImage Image { get; set; }\n}\npublic class CustomMappingInput\n{\n    [VectorType(3, 1024, 1024)]\n    public float[] out1 { get; set; }\n}\npublic class CustomMappingOutput\n{\n    [VectorType(3, 1024, 1024)]\n    public float[] pixel_values { get; set; } \n}\n</code></pre>\n<p>I know the code is far from optimal (<code>GetPixel()</code>and <code>SetPixel()</code>have to be replaced amongst other things), and that the aspect ratio of my result is wrong because I have not scaled the image back to the original dimensions. First I would like to get the background removal working correctly.</p>\n<p>Any advice or idea of what I might be doind incorrectly?</p>\n<p>BTW, the onnx file is available in the RMBG-2.0 link at the beginning. There is also a code snippet in python for using the model and that is why I am applying thosee transformations to the image in the pipeline.</p>\n<p><a href=\"https://i.sstatic.net/oTJSO60A.jpg\" rel=\"nofollow noreferrer\">Input Image</a></p>\n<p><a href=\"https://i.sstatic.net/ZLouZcpm.png\" rel=\"nofollow noreferrer\">Expected result</a></p>\n<p><a href=\"https://i.sstatic.net/pB1FKT6f.png\" rel=\"nofollow noreferrer\">Result I am getting</a></p>\n",
        "answer": "<p>I finally solved the problem going at it from another angle. Using the Ml.OnnxRuntime and ImageSharp greatly simplified the task.</p>\n<p>Here is the working code:</p>\n<pre><code>public class ImageSegmentationService : IDisposable\n{\n    private readonly InferenceSession _session;\n    private const int ImageSize = 1024;\n\n    public ImageSegmentationService(string modelPath)\n    {\n        _session = new InferenceSession(modelPath);\n    }\n\n    public float[] ProcessImage(string imagePath)\n    {\n        using var image = Image.Load&lt;Rgb24&gt;(imagePath);\n        image.Mutate(x =&gt; x.Resize(ImageSize, ImageSize));\n    \n        // Prepare input tensor (normalize to [0,1] and convert to NCHW)\n        var inputTensor = new DenseTensor&lt;float&gt;(new[] { 1, 3, ImageSize, ImageSize });\n    \n        for (int y = 0; y &lt; ImageSize; y++)\n        {\n            for (int x = 0; x &lt; ImageSize; x++)\n            {\n                var pixel = image[x, y];\n                inputTensor[0, 0, y, x] = pixel.R / 255f;\n                inputTensor[0, 1, y, x] = pixel.G / 255f;\n                inputTensor[0, 2, y, x] = pixel.B / 255f;\n            }\n        }\n\n        // Run inference\n        var inputs = new List&lt;NamedOnnxValue&gt; \n        { \n            NamedOnnxValue.CreateFromTensor(&quot;pixel_values&quot;, inputTensor) \n        };\n\n        using var outputs = _session.Run(inputs);\n        var alphas = outputs.First().AsTensor&lt;float&gt;();\n        \n        return alphas.ToArray();\n    }\n\n    public void Dispose()\n    {\n        _session?.Dispose();\n    }\n\n    public class RmbgInput\n    {\n        [VectorType(1, 3, 1024, 1024)]\n        public float[] pixel_values { get; set; }\n    }\n\n    public class RmbgOutput\n    {\n        [VectorType(1, 1024, 1024)]\n        public float[] alphas { get; set; }\n    }\n}\n</code></pre>\n<p>The result of <code>ProcessImage(string imagePath)</code> is the alpha mask that should be applied to the original image (in 1024*1024 dimensions) to remove the background.</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 79411192,
            "link": "https://stackoverflow.com/questions/79411192/image-segmentation-onnx-from-huggingface-produces-very-diferent-results-when-use"
        }
    },
    {
        "question": "How to configure HuggingFaceEndpoint in Langchain\n<p>I'm trying to use this model</p>\n<pre><code>from langchain_huggingface import HuggingFaceEndpoint\nrepo_id=&quot;google/flan-t5-large&quot;\nhuggingface_llm = HuggingFaceEndpoint(\nhuggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN,\nrepo_id=repo_id,\ntemperature=0,\nmax_new_tokens=200)\n\nfrom langchain.prompts import PromptTemplate\ndef flan_process(tema, pregunta):\ntemplate = &quot;Eres un experto asistente en {tema}. Responde a la siguiente pregunta: {pregunta}&quot;\nprompt=PromptTemplate(template=template,input_variables=[&quot;tema&quot;,&quot;pregunta&quot;])\n\nflan_chain = prompt | huggingface_llm\n\nrespuesta=flan_chain.invoke({&quot;tema&quot;:tema, &quot;pregunta&quot;:pregunta})\n\nreturn respuesta\n\ntema=input(&quot;Ingrese el tema: &quot;)\npregunta=input(&quot;Ingrese la pregunta: &quot;)\n\nflan_reply=flan_process(tema, pregunta)\nprint(f&quot;Respuesta Flan: {flan_reply}&quot;)\n</code></pre>\n<p>But I always get this error The following <code>model_kwargs</code> are not used by the model: ['return_full_text', 'watermark', 'stop_sequences', 'stop'] (note: typos in the generate arguments will also show up in this list)</p>\n<p>Any idea please?</p>\n<p>Thanks</p>\n",
        "answer": "<p>I encountered the same problem, it seems like it is a bug in the implementation of the <code>HuggingFaceEndpoint</code> class that makes it not work with certain models like the <code>google/flan-t5-large</code> one, which you (and also I) are using. I read this on an issue on GitHub.\n<a href=\"https://github.com/langchain-ai/langchain/issues/18321\" rel=\"nofollow noreferrer\">Link to issue</a></p>\n<p>I hope this helps you!</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 78617530,
            "link": "https://stackoverflow.com/questions/78617530/how-to-configure-huggingfaceendpoint-in-langchain"
        }
    },
    {
        "question": "Low score and wrong answer for Flan-T5-XXL &quot;question-answering&quot; task\n<p>I'm trying to run Flan-T5-XXL model for a &quot;question-answering&quot; task.\nHere's how I loaded and executed the model:</p>\n<pre><code>model_id = &quot;~/Downloads/test_LLM/flan-t5-xxl&quot;\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_id, return_dict=False).to(DEVICE)\n\nqa_T5XXL = pipeline(&quot;question-answering&quot;, model=model, tokenizer=tokenizer)\n\nquestion = &quot;What is 42?&quot;\ncontext = &quot;42 is the answer to life, the universe and everything&quot;\n\nresult = qa_T5XXL({\n    &quot;question&quot;: question,\n    &quot;context&quot;: context\n})\n</code></pre>\n<p>However, I get a low score and a wrong answer:</p>\n<pre><code>{'score': 0.03840925544500351, 'start': 0, 'end': 2, 'answer': '42'}\n</code></pre>\n<p>Could you please help me make changes to achieve the correct answer?\nThanks in advance.</p>\n",
        "answer": "<p><strong>Pre/Script:</strong> This is more of a science experiment design or product development question than a programming question, so most probably someone will flag to close this question on Stackoverflow eventually. But here's an attempt to answer.</p>\n<h1>In Short</h1>\n<p>There is a couple of things to consider before someone can help to answer the question.</p>\n<ul>\n<li>What is ultimate goal of getting the answers right?</li>\n<li>How do you determine what is the right answers?</li>\n<li>How do you measure what is right? Is there a metric you use? Or a fix test dataset of question, context, answer triplets?</li>\n</ul>\n<h1>In Long</h1>\n<p>Here's a few QnA to clarify somethings about LLM and QnA.</p>\n<h2>Q: What does the &quot;start&quot; and &quot;end&quot; mean in the results?</h2>\n<p>A: Given these,</p>\n<p>[in]:</p>\n<pre><code>question = &quot;What is 42?&quot;\ncontext = &quot;42 is the answer to life, the universe and everything&quot;\n</code></pre>\n<p>[out]:</p>\n<pre><code>results = {'score': 0.03840925544500351, 'start': 0, 'end': 2, 'answer': '42'}\n</code></pre>\n<p>We see the &quot;start&quot; and &quot;end&quot; indices. That indicates that the model you are using is an extractive QnA system, i.e. given a question and context find the answer inside the context string.</p>\n<p>So, the &quot;answer&quot; in the result dictionary is:</p>\n<pre><code>context[results['start']:result['end']  # i.e. &quot;42&quot;\n</code></pre>\n<h2>Q: Then, what does the &quot;score&quot; mean in the results?</h2>\n<p>A: From <a href=\"https://github.com/huggingface/transformers/blob/main/src/transformers/pipelines/question_answering.py#L46\" rel=\"nofollow noreferrer\">https://github.com/huggingface/transformers/blob/main/src/transformers/pipelines/question_answering.py#L46</a>, we see:</p>\n<pre class=\"lang-py prettyprint-override\"><code>def decode_spans(\n    start: np.ndarray, end: np.ndarray, topk: int, max_answer_len: int, undesired_tokens: np.ndarray\n) -&gt; Tuple:\n    &quot;&quot;&quot;\n    Take the output of any `ModelForQuestionAnswering` and will generate probabilities for each span to be the actual\n    answer.\n\n    In addition, it filters out some unwanted/impossible cases like answer len being greater than max_answer_len or\n    answer end position being before the starting position. The method supports output the k-best answer through the\n    topk argument.\n\n    Args:\n        start (`np.ndarray`): Individual start probabilities for each token.\n        end (`np.ndarray`): Individual end probabilities for each token.\n        topk (`int`): Indicates how many possible answer span(s) to extract from the model output.\n        max_answer_len (`int`): Maximum size of the answer to extract from the model's output.\n        undesired_tokens (`np.ndarray`): Mask determining tokens that can be part of the answer\n    &quot;&quot;&quot;\n    # Ensure we have batch axis\n    if start.ndim == 1:\n        start = start[None]\n\n    if end.ndim == 1:\n        end = end[None]\n\n    # Compute the score of each tuple(start, end) to be the real answer\n    outer = np.matmul(np.expand_dims(start, -1), np.expand_dims(end, 1))\n\n    # Remove candidate with end &lt; start and end - start &gt; max_answer_len\n    candidates = np.tril(np.triu(outer), max_answer_len - 1)\n\n    #  Inspired by Chen &amp; al. (https://github.com/facebookresearch/DrQA)\n    scores_flat = candidates.flatten()\n    if topk == 1:\n        idx_sort = [np.argmax(scores_flat)]\n    elif len(scores_flat) &lt; topk:\n        idx_sort = np.argsort(-scores_flat)\n    else:\n        idx = np.argpartition(-scores_flat, topk)[0:topk]\n        idx_sort = idx[np.argsort(-scores_flat[idx])]\n\n    starts, ends = np.unravel_index(idx_sort, candidates.shape)[1:]\n    desired_spans = np.isin(starts, undesired_tokens.nonzero()) &amp; np.isin(ends, undesired_tokens.nonzero())\n    starts = starts[desired_spans]\n    ends = ends[desired_spans]\n    scores = candidates[0, starts, ends]\n\n    return starts, ends, scores\n</code></pre>\n<h3>Q: Showing <code>np.matmul(np.expand_dims(start, -1), np.expand_dims(end, 1))</code> doesn't tell me anything about what the scores mean... What happens when we allow more than 1 answers?</h3>\n<p><code>flan-t5-xxl</code> is too big demonstrate the results, so lets try tiny,</p>\n<pre><code>from transformers import pipeline\n\npipe = pipeline(&quot;question-answering&quot;, model=&quot;google/flan-t5-small&quot;)\n\nquestion = &quot;What is 42?&quot;\ncontext = &quot;42 is the answer to life, the universe and everything&quot;\n\nresult = pipe({\n    &quot;question&quot;: question,\n    &quot;context&quot;: context\n}, top_k=5)\n</code></pre>\n<p>[out]:</p>\n<pre><code>[{'score': 0.011151721701025963, 'start': 25, 'end': 29, 'answer': ' the'},\n {'score': 0.01089030597358942,\n  'start': 5,\n  'end': 29,\n  'answer': ' the answer to life, the'},\n {'score': 0.0108568724244833,\n  'start': 0,\n  'end': 29,\n  'answer': '42 is the answer to life, the'},\n {'score': 0.010814748704433441,\n  'start': 16,\n  'end': 29,\n  'answer': ' to life, the'},\n {'score': 0.01060018502175808,\n  'start': 19,\n  'end': 29,\n  'answer': ' life, the'}]\n</code></pre>\n<h3>Q: Linda umzuzu! (Wait a minute!), does that mean it will compute possibilities of any possible span within the context?</h3>\n<p>Yes, kinda! That's the goal of extractive QnA. The answer is in the context, the brute force way is to score all possible spans to get the best answer. The <code>top_k</code> argument controls how many candidates to consider.</p>\n<pre><code>question = &quot;What is 42?&quot;\ncontext = &quot;42 is the answer to life, the universe and everything&quot;\n\nresult = pipe({\n    &quot;question&quot;: question,\n    &quot;context&quot;: context\n}, top_k=30)\n</code></pre>\n<p>[out]:</p>\n<pre><code>[{'score': 0.011151721701025963, 'start': 25, 'end': 29, 'answer': ' the'},\n {'score': 0.01089030597358942,\n  'start': 5,\n  'end': 29,\n  'answer': ' the answer to life, the'},\n {'score': 0.0108568724244833,\n  'start': 0,\n  'end': 29,\n  'answer': '42 is the answer to life, the'},\n {'score': 0.010814748704433441,\n  'start': 16,\n  'end': 29,\n  'answer': ' to life, the'},\n {'score': 0.01060018502175808,\n  'start': 19,\n  'end': 29,\n  'answer': ' life, the'},\n {'score': 0.010392689146101475,\n  'start': 19,\n  'end': 29,\n  'answer': ' life, the'},\n {'score': 0.010242749936878681,\n  'start': 9,\n  'end': 29,\n  'answer': ' answer to life, the'},\n {'score': 0.009692603722214699,\n  'start': 5,\n  'end': 16,\n  'answer': ' the answer'},\n {'score': 0.00966284703463316,\n  'start': 0,\n  'end': 16,\n  'answer': '42 is the answer'},\n {'score': 0.009410168044269085,\n  'start': 2,\n  'end': 29,\n  'answer': ' is the answer to life, the'},\n {'score': 0.00911626499146223, 'start': 9, 'end': 16, 'answer': ' answer'},\n {'score': 0.00905834324657917,\n  'start': 25,\n  'end': 38,\n  'answer': ' the universe'},\n {'score': 0.008912604302167892,\n  'start': 29,\n  'end': 38,\n  'answer': ' universe'},\n {'score': 0.008845999836921692,\n  'start': 5,\n  'end': 38,\n  'answer': ' the answer to life, the universe'},\n {'score': 0.008818842470645905,\n  'start': 0,\n  'end': 38,\n  'answer': '42 is the answer to life, the universe'},\n {'score': 0.008786008693277836,\n  'start': 38,\n  'end': 53,\n  'answer': ' and everything'},\n {'score': 0.008784625679254532,\n  'start': 16,\n  'end': 38,\n  'answer': ' to life, the universe'},\n {'score': 0.00861033983528614,\n  'start': 19,\n  'end': 38,\n  'answer': ' life, the universe'},\n {'score': 0.008441794663667679,\n  'start': 19,\n  'end': 38,\n  'answer': ' life, the universe'},\n {'score': 0.008398951031267643, 'start': 0, 'end': 5, 'answer': '42 is'},\n {'score': 0.008392956107854843,\n  'start': 5,\n  'end': 25,\n  'answer': ' the answer to life,'},\n {'score': 0.008388272486627102, 'start': 5, 'end': 9, 'answer': ' the'},\n {'score': 0.008375249803066254,\n  'start': 2,\n  'end': 16,\n  'answer': ' is the answer'},\n {'score': 0.008367189206182957,\n  'start': 0,\n  'end': 25,\n  'answer': '42 is the answer to life,'},\n {'score': 0.008362519554793835, 'start': 0, 'end': 9, 'answer': '42 is the'},\n {'score': 0.00833472516387701, 'start': 16, 'end': 25, 'answer': ' to life,'},\n {'score': 0.00832000095397234,\n  'start': 9,\n  'end': 38,\n  'answer': ' answer to life, the universe'},\n {'score': 0.008312774822115898,\n  'start': 25,\n  'end': 53,\n  'answer': ' the universe and everything'},\n {'score': 0.00820864923298359,\n  'start': 42,\n  'end': 53,\n  'answer': ' everything'},\n {'score': 0.008179032243788242,\n  'start': 29,\n  'end': 53,\n  'answer': ' universe and everything'}]\n</code></pre>\n<h3>Q: Does that mean that I'll not always get the right answer?</h3>\n<p>A: Yes, correct. LLM or pre-trained models are tuned to whatever the data is.</p>\n<p>First, we have to ask:</p>\n<ul>\n<li><strong>What exactly is the desired answer?</strong>\n<ul>\n<li>In the context of your question, I guess you are expecting &quot;the answer to life, the universe and everything&quot;</li>\n<li>But consider the valid statement of a tautology, &quot;X is X&quot;, so the answer &quot;42 is 42&quot; is a valid for the &quot;What is 42?&quot; question.</li>\n</ul>\n</li>\n</ul>\n<p>Next, we need to ask:</p>\n<ul>\n<li>Does the training data use to fine-tune or train the model contains the expected behavior?</li>\n<li>Or does it contain tautology such that the model learns to emulate that during inference?</li>\n</ul>\n<p>Given that for most models you'll see in the wild, it's hard to determine the above questions, your next practical question would be:</p>\n<h2>Q: Okay, so no model is perfect. How do I make the model output what I need?</h2>\n<p>A: First, you'll need to consider,</p>\n<ul>\n<li>Is the pre-trained model you chosen tuned our your task or domain? Question and Answer is a very wide task, knowing how to answer on stackoverflow don't make you an expert on answering legal questions or pop-culture questions.</li>\n<li>Have you fine-tuned the model to your task or domain? Did it perform better than the original pre-trained model?</li>\n</ul>\n<p>Then you need to consider:</p>\n<ul>\n<li>Does one failed inference instance affect the system you want to build? What is the &quot;fidelity&quot; of the system you are building? E.g. if it's a medical domain, would you kill someone if the model answer wrongly?</li>\n<li>How exactly is the model going to be deployed and how you want to measure the success?\n<ul>\n<li>Is it just based on (i) random anecdotal examples, or (ii) is there a specific metric that the model should improve towards?\n<ul>\n<li>if (i), do you have a large enough sample of anecdotes examples to test on?</li>\n<li>if (ii), would you be able to overlook the fact that the score outperforms the original pre-trained model but fail on some anecdotal examples.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h2>Q: Yeah, yeah, I know all you are insinuating but I just want to %^&amp;*-ing solve this question to give the right answer?</h2>\n<p>A: Consider, some business logic. Many a times, big tech handles common Q with manually edited A. Using that you need to fine-tune the model or using some caching mechanism to achieve the desired result if you don't want to spend time/effort re-training / fine-tuning the model.</p>\n<p>i.e.</p>\n<pre><code>if &quot;question&quot; == &quot;What is 42?&quot;\n    result = {&quot;answer&quot;: &quot;answer to life, the universe and everything&quot;}\nelse:\n    result = qa_T5XXL({\n    &quot;question&quot;: question,\n    &quot;context&quot;: context\n})\n</code></pre>\n<p>For reference, see:</p>\n<ul>\n<li><a href=\"https://stackoverflow.com/questions/59956670/parsing-city-of-origin-destination-city-from-a-string\">Parsing city of origin / destination city from a string</a></li>\n<li><a href=\"https://hackernoon.com/tmnt-translation-memory-and-neural-translation\" rel=\"nofollow noreferrer\">https://hackernoon.com/tmnt-translation-memory-and-neural-translation</a></li>\n</ul>\n<h2>Q: I don't want to have any rule-base, just tell me how to hack the code to get the right answer already.</h2>\n<p>A: You will most probably not get the right answer in the top-1 answer easily. You have to manage the noise in the top_k outputs and try to extend the candidate possibilities to everything possible in the context, e.g.</p>\n<pre><code>from transformers import pipeline\n\npipe = pipeline(&quot;question-answering&quot;, model=&quot;google/flan-t5-small&quot;)\n\nquestion = &quot;What is 42?&quot;\ncontext = &quot;42 is the answer to life, the universe and everything&quot;\n\nresult = pipe({\n    &quot;question&quot;: question,\n    &quot;context&quot;: context\n}, top_k=500, max_answer_len=len(context))\n\nwhich_rank = [(i,r) for i, r in enumerate(result) if r['answer'].strip() == &quot;the answer to life, the universe and everything&quot;]\n</code></pre>\n<p>[out]:</p>\n<pre><code>[(34,\n  {'score': 0.008117909543216228,\n   'start': 5,\n   'end': 53,\n   'answer': ' the answer to life, the universe and everything'})]\n</code></pre>\n<p>Voila, the right answer is ranked 34 out of 500!</p>\n<h3>Q: But that is meaningless, I want it to be the top answer. If not, how do I evaluate how good the model is?</h3>\n<p>A: Now, that is a good question. If you have a test set with the right answers and you are not getting the right answers in the top-1 or even top-10 candidates. You might need to reconsider how you evaluate the model.</p>\n<p>Depending on the ultimate goal,</p>\n<ul>\n<li><p>if the goal is to improve the model until the correct answer goes up to the top-1, then consider evaluating the model in terms of rank reciprocal, e.g. MRR, NDCG, see <a href=\"https://scikit-learn.org/stable/modules/model_evaluation.html#label-ranking-average-precision\" rel=\"nofollow noreferrer\">https://scikit-learn.org/stable/modules/model_evaluation.html#label-ranking-average-precision</a></p>\n</li>\n<li><p>if the goal is to just make sure that the model works off-the-shelf, then I'll consider more customized solutions, e.g.</p>\n<ul>\n<li>pay more for better closed-APIs that has a service level agreement you can negotiate to get the right answers you want, or</li>\n<li>somehow get some service to train/fine-tune the model</li>\n<li>try prompt engineering for &quot;one-shot&quot;, &quot;few-shot&quot; learning, if you google around these days, there should be quite a lot to advise how to go around that, e.g. <a href=\"https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/\" rel=\"nofollow noreferrer\">https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/</a></li>\n</ul>\n</li>\n<li><p>if the goal is to ultimately get a model with that correctly answers on any questions you have in mind, and you have the compute, human and time resources, consider fine-tuning the model on relevant domain data, then re-evaluate the accuracy score, with something like <code>seqeval</code> or rogue scores.</p>\n</li>\n</ul>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 76963864,
            "link": "https://stackoverflow.com/questions/76963864/low-score-and-wrong-answer-for-flan-t5-xxl-question-answering-task"
        }
    },
    {
        "question": "How do I successfully set and retrieve metadata information for a HuggingfaceDataset on the Huggingface Hub?\n<p>I have a number of datasets, which I create from a dictionary like so:</p>\n<pre><code>info = DatasetInfo(\n        description=&quot;my happy lil dataset&quot;,\n        version=&quot;0.0.1&quot;,\n        homepage=&quot;https://www.myhomepage.co.uk&quot;\n    )\ntrain_dataset = Dataset.from_dict(prepare_data(data[&quot;train&quot;]), info=info)\ntest_dataset = Dataset.from_dict(prepare_data(data[&quot;test&quot;]), info=info)\nvalidation_dataset = Dataset.from_dict(prepare_data(data[&quot;validation&quot;]),info=info)\n</code></pre>\n<p>I then combine these into a DatasetDict.</p>\n<pre><code># Create a DatasetDict\ndataset = DatasetDict(\n    {&quot;train&quot;: train_dataset, &quot;test&quot;: test_dataset, &quot;validation&quot;: validation_dataset}\n)\n</code></pre>\n<p>So far, so good. If I access <code>dataset['train'].info.description</code> I see the expected result of &quot;My happy lil dataset&quot;.</p>\n<p>So I push to the hub, like so:</p>\n<pre><code>dataset.push_to_hub(f&quot;{organization}/{repo_name}&quot;, commit_message=&quot;Some commit message&quot;)\n</code></pre>\n<p>And this succeeds too.</p>\n<p>However, when I come to pull the dataset back down from the hub, and access the information associated with it, rather than getting the description of my dataset, I just get an empty string; like so:</p>\n<pre><code>pulled_data = full = load_dataset(&quot;f{organization}/{repo_name}&quot;, use_auth_token = True)\n\n# I expect the following to print out &quot;my happy lil dataset&quot;\nprint(pulled_data[&quot;train&quot;].info.description)\n# However, instead it returns ''\n</code></pre>\n<p>Am I loading my data in from the hub incorrectly? Am I pushing only my dataset and not the info somehow?\nI feel like I’m missing something obvious, but I’m really not sure.</p>\n",
        "answer": "<p>It might be due to version caching of dataset. Without explicit version attribute, the library's default versioning may not preserve all metadata like Description.</p>\n<p>Please include VERSION in a wrapper class like:</p>\n<pre><code>import datasets\n\nclass My_dataset(datasets.GeneratorBasedBuilder):\n\n    VERSION = datasets.Version(&quot;1.0.0&quot;)\n    def _info(self) -&gt; datasets.DatasetInfo:\n        return datasets.DatasetInfo(\n            description=&quot;my happy lil dataset&quot;,\n            features=datasets.Features(\n                {\n                    &quot;f1&quot;: datasets.Value(&quot;string&quot;), # list of features provided by your dataset with their types\n                    &quot;f2&quot;: datasets.Value(&quot;string&quot;),\n                }\n            ),\n            homepage=&quot;https://www.myhomepage.co.uk&quot;,\n        )  \n</code></pre>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 78759790,
            "link": "https://stackoverflow.com/questions/78759790/how-do-i-successfully-set-and-retrieve-metadata-information-for-a-huggingfacedat"
        }
    },
    {
        "question": "Fine-tuning TheBloke/Llama-2-13B-chat-GPTQ model with Hugging Face Transformers library throws Exllama error\n<p>I am trying to fine-tune the TheBloke/Llama-2-13B-chat-GPTQ model using the Hugging Face Transformers library. I am using a JSON file for the training and validation datasets. However, I am encountering an error related to Exllama backend when I try to run the script.</p>\n<p>Here is my code:</p>\n<pre class=\"lang-py prettyprint-override\"><code>from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\nfrom datasets import load_dataset\nimport torch\n\n# Check GPU availability\nprint(&quot;Available GPU devices:&quot;, torch.cuda.device_count())\nprint(&quot;Name of the first available GPU:&quot;, torch.cuda.get_device_name(0))\n\n# Load model and tokenizer\nmodel_name = &quot;TheBloke/Llama-2-13B-chat-GPTQ&quot;\n\n# Load tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\n\n# Move the model to GPU\nmodel.to('cuda')\n\n# Load training and validation data\ntrain_data = load_dataset('json', data_files='train_data.jsonl')\nval_data = load_dataset('json', data_files='val_data.jsonl')\n\n# Function to format the data\ndef formatting_func(example):\n    return tokenizer(example['input'], example.get('output', ''), truncation=True, padding='max_length')\n\n# Prepare training and validation data\ntrain_data = train_data.map(formatting_func)\nval_data = val_data.map(formatting_func)\n\n# Set training arguments\ntraining_args = TrainingArguments(\n    output_dir=&quot;./output&quot;,\n    overwrite_output_dir=True,\n    num_train_epochs=1,\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=64,\n    save_steps=10_000,\n    save_total_limit=2,\n)\n\n# Create trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_data,\n    eval_dataset=val_data,\n)\n\n# Start training\ntrainer.train()\n\n# Save the model\nmodel.save_pretrained(&quot;./output&quot;)\n\n</code></pre>\n<p>The error message I get is:</p>\n<pre><code>ValueError: Found modules on cpu/disk. Using Exllama backend requires all the \nmodules to be on GPU. You can deactivate exllama backend by setting \n`disable_exllama=True` in the quantization config object.\n</code></pre>\n<p>I have already moved the model to GPU using model.to('cuda'), but the error persists. Any help would be greatly appreciated.</p>\n<p>I tried moving the model to the GPU using model.to('cuda') before initiating the training process, as suggested in the Hugging Face documentation. I also ensured that my environment has all the required packages and dependencies installed. I was expecting the model to fine-tune on my custom JSON dataset without any issues.</p>\n<p>However, despite moving the model to the GPU, I still encounter the Exllama backend error. I am not sure why this is happening, as the model should be on the GPU as per my code. I am looking for a way to resolve this error and successfully fine-tune the model on my custom dataset.</p>\n",
        "answer": "<p>Add the below line to the <code>quantization_config</code> part of the model <code>config.json</code> file:</p>\n<pre><code>&quot;quantization_config&quot;: {\n    …,\n    &quot;disable_exllama&quot;: true\n}\n</code></pre>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 76983305,
            "link": "https://stackoverflow.com/questions/76983305/fine-tuning-thebloke-llama-2-13b-chat-gptq-model-with-hugging-face-transformers"
        }
    },
    {
        "question": "How to load a huggingface dataset from local path?\n<p>Take a simple example in this website, <a href=\"https://huggingface.co/datasets/Dahoas/rm-static\" rel=\"nofollow noreferrer\">https://huggingface.co/datasets/Dahoas/rm-static</a>:</p>\n<p>if I want to load this dataset online, I just directly use,</p>\n<pre><code>from datasets import load_dataset\ndataset = load_dataset(&quot;Dahoas/rm-static&quot;) \n</code></pre>\n<p>What if I want to load dataset from local path, so I download the files and keep the same folder structure from web <code>Files and versions</code> fristly,</p>\n<pre><code>-data\n|-test-00000-of-00001-bf4c733542e35fcb.parquet\n|-train-00000-of-00001-2a1df75c6bce91ab.parquet\n-.gitattributes\n-README.md\n-dataset_infos.json\n</code></pre>\n<p>Then, put them into my folder, but shows error when loading:</p>\n<pre><code>dataset_path =&quot;/data/coco/dataset/Dahoas/rm-static&quot;\ntmp_dataset = load_dataset(dataset_path)\n</code></pre>\n<p>It shows <code>FileNotFoundError: No (supported) data files or dataset script found in /data/coco/dataset/Dahoas/rm-static.</code></p>\n",
        "answer": "<p>I solved this question by myself, it is easy to use:</p>\n<pre><code>data_files = {“train”:“train-00000-of-00001-2a1df75c6bce91ab.parquet”,“test”:“test-00000-of-00001-8c7c51afc6d45980.parquet”}\nraw_datasets = load_dataset(“parquet”, data_dir=‘/Your/Path/Dahoas/rm-static/data’, data_files=data_files)\n</code></pre>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 77020278,
            "link": "https://stackoverflow.com/questions/77020278/how-to-load-a-huggingface-dataset-from-local-path"
        }
    },
    {
        "question": "How to fix Index put requires the source and destination dtypes match` with `google/gemma-2-2b` in Transformers?\n<p>I’m trying to train a language model using <code>google/gemma-2-2b</code> with the Hugging Face Transformers <code>Trainer</code>. The same training script works fine for other models like <code>gpt2</code> and <code>meta-llama/Meta-Llama-3-8B</code>, but with Gemma-2-2B it fails during evaluation, showing:</p>\n<pre><code>RuntimeError: Index put requires the source and destination dtypes match, got Float for the destination and BFloat16 for the source.\n</code></pre>\n<p>Below is the full console output (and the relevant code excerpt at the end). Note that I already attempted the following:</p>\n<ul>\n<li>Setting <code>attn_implementation='eager'</code> for Gemma-2-2B.</li>\n<li>Switching out of <code>paged_adamw_32bit</code>.</li>\n<li>(Un)commenting <code>gradient_checkpointing</code>.</li>\n</ul>\n<p>I still get this dtype mismatch error at eval time. Any ideas on how to resolve or work around this?</p>\n<hr />\n<p><strong>Full console output</strong>:</p>\n<pre><code>Kwargs to run:\n{'mode': 'dryrun', 'project': 'self-opt-train-uncompiled-py-2-gsm8k', 'num_train_epochs': 1, 'model_name': 'google/gemma-2-2b', 'today': '2025_m02_d07_t07h_20m_14s', 'tmux_sess_num': None, 'hostname': 'skampere1'}\nSetting random seed = 42\nvLLM not installed or vllm set seed has a bug, skipping vLLM seed setting.\nCurrently logged in as: brando\n\nLoading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████| 3/3 [00:00&lt;00:00,  5.63it/s]\nblock_size=1024\nlen(ds_train)=18612\nlen(ds_train)=2740\n/lfs/skampere1/0/brando9/miniconda/envs/zip_fit/lib/python3.11/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/lfs/skampere1/0/brando9/ZIP-FIT/zip_fit/train/train.py:371: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\nDetected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\nwandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. ...\n  0%|                                                                                                    | 0/342 [00:00&lt;?, ?it/s]The 'batch_size' argument of HybridCache is deprecated and will be removed in v4.49...\nThe 'batch_size' attribute of HybridCache is deprecated and will be removed in v4.49...\nTraceback (most recent call last):\n  File &quot;/lfs/skampere1/0/brando9/ZIP-FIT/zip_fit/train/train.py&quot;, line 564, in &lt;module&gt;\n    fire.Fire(_main)\n  File &quot;/lfs/skampere1/0/brando9/miniconda/envs/zip_fit/lib/python3.11/site-packages/fire/core.py&quot;, line 135, in Fire\n    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n  File &quot;/lfs/skampere1/0/brando9/miniconda/envs/zip_fit/lib/python3.11/site-packages/fire/core.py&quot;, line 468, in _Fire\n    component, remaining_args = _CallAndUpdateTrace(\n  File &quot;/lfs/skampere1/0/brando9/miniconda/envs/zip_fit/lib/python3.11/site-packages/fire/core.py&quot;, line 684, in _CallAndUpdateTrace\n    component = fn(*varargs, **kwargs)\n  File &quot;/lfs/skampere1/0/brando9/ZIP-FIT/zip_fit/train/train.py&quot;, line 554, in _main\n    main_train(kwargs)\n  File &quot;/lfs/skampere1/0/brando9/ZIP-FIT/zip_fit/train/train.py&quot;, line 383, in main_train\n    trainer.train()\n  File &quot;/lfs/skampere1/0/brando9/miniconda/envs/zip_fit/lib/python3.11/site-packages/transformers/trainer.py&quot;, line 2171, in train\n    return inner_training_loop(\n  File &quot;/lfs/skampere1/0/brando9/miniconda/envs/zip_fit/lib/python3.11/site-packages/transformers/trainer.py&quot;, line 2440, in _inner_training_loop\n    self._evaluate(trial, ignore_keys_for_eval, skip_scheduler=True)\n  File &quot;/lfs/skampere1/0/brando9/miniconda/envs/zip_fit/lib/python3.11/site-packages/transformers/trainer.py&quot;, line 3025, in _evaluate\n    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)\n  File &quot;/lfs/skampere1/0/brando9/miniconda/envs/zip_fit/lib/python3.11/site-packages/transformers/trainer.py&quot;, line 4076, in evaluate\n    output = eval_loop(\n  File &quot;/lfs/skampere1/0/brando9/miniconda/envs/zip_fit/lib/python3.11/site-packages/transformers/trainer.py&quot;, line 4270, in evaluation_loop\n    losses, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)\n  File &quot;/lfs/skampere1/0/brando9/miniconda/envs/zip_fit/lib/python3.11/site-packages/transformers/trainer.py&quot;, line 4486, in prediction_step\n    loss, outputs = self.compute_loss(model, inputs, return_outputs=True)\n  File &quot;/lfs/skampere1/0/brando9/miniconda/envs/zip_fit/lib/python3.11/site-packages/transformers/trainer.py&quot;, line 3734, in compute_loss\n    outputs = model(**inputs)\n  File &quot;/lfs/skampere1/0/brando9/miniconda/envs/zip_fit/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File &quot;/lfs/skampere1/0/brando9/miniconda/envs/zip_fit/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n  File &quot;/lfs/skampere1/0/brando9/miniconda/envs/zip_fit/lib/python3.11/site-packages/accelerate/utils/operations.py&quot;, line 819, in forward\n    return model_forward(*args, **kwargs)\n  File &quot;/lfs/skampere1/0/brando9/miniconda/envs/zip_fit/lib/python3.11/site-packages/accelerate/utils/operations.py&quot;, line 807, in __call__\n    return convert_to_fp32(self.model_forward(*args, **kwargs))\n  File &quot;/lfs/skampere1/0/brando9/miniconda/envs/zip_fit/lib/python3.11/site-packages/torch/amp/autocast_mode.py&quot;, line 44, in decorate_autocast\n    return func(*args, **kwargs)\n  File &quot;/lfs/skampere1/0/brando9/miniconda/envs/zip_fit/lib/python3.11/site-packages/transformers/models/gemma2/modeling_gemma2.py&quot;, line 842, in forward\n    outputs = self.model(\n  File &quot;/lfs/skampere1/0/brando9/miniconda/envs/zip_fit/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File &quot;/lfs/skampere1/0/brando9/miniconda/envs/zip_fit/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n  File &quot;/lfs/skampere1/0/brando9/miniconda/envs/zip_fit/lib/python3.11/site-packages/transformers/models/gemma2/modeling_gemma2.py&quot;, line 629, in forward\n    layer_outputs = decoder_layer(\n  File &quot;/lfs/skampere1/0/brando9/miniconda/envs/zip_fit/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File &quot;/lfs/skampere1/0/brando9/miniconda/envs/zip_fit/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n  File &quot;/lfs/skampere1/0/brando9/miniconda/envs/zip_fit/lib/python3.11/site-packages/transformers/models/gemma2/modeling_gemma2.py&quot;, line 299, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File &quot;/lfs/skampere1/0/brando9/miniconda/envs/zip_fit/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File &quot;/lfs/skampere1/0/brando9/miniconda/envs/zip_fit/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n  File &quot;/lfs/skampere1/0/brando9/miniconda/envs/zip_fit/lib/python3.11/site-packages/transformers/models/gemma2/modeling_gemma2.py&quot;, line 224, in forward\n    key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n  File &quot;/lfs/skampere1/0/brando9/miniconda/envs/zip_fit/lib/python3.11/site-packages/transformers/cache_utils.py&quot;, line 1717, in update\n    return update_fn(\n  File &quot;/lfs/skampere1/0/brando9/miniconda/envs/zip_fit/lib/python3.11/site-packages/transformers/cache_utils.py&quot;, line 1695, in _static_update\n    v_out[:, :, cache_position] = value_states\n    ~~~~~^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: Index put requires the source and destination dtypes match, got Float for the destination and BFloat16 for the source.\n</code></pre>\n<hr />\n<p><strong>Key snippet where I try to force eager attention for Gemma-2</strong>:</p>\n<pre class=\"lang-py prettyprint-override\"><code>torch_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float32\nif 'gemma-2' not in model_name:\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name, \n        torch_dtype=torch_dtype\n    ).to(device)\nelse:\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name, \n        attn_implementation='eager'\n    ).to(device)\n</code></pre>\n<p>I also switched from <code>paged_adamw_32bit</code> to standard <code>adamw_torch</code>, and toggled <code>gradient_checkpointing</code>:</p>\n<pre class=\"lang-py prettyprint-override\"><code># gradient_checkpointing=config.get('gradient_checkpointing', True),  # known to cause issues\n# optim=config.get('optim', 'paged_adamw_32bit'),  # switched out of paged optim\n</code></pre>\n<p>But the error persists. Any suggestions on how to fix or debug this <code>Index put</code> dtype mismatch for Gemma-2? Note it works for LLama3-8b and Gpt2.</p>\n<hr />\n<p>Code:</p>\n<ul>\n<li>tfa.py: <a href=\"https://gist.github.com/brando90/315872b2be8f2dc9935ed7e20ffb1bdc\" rel=\"nofollow noreferrer\">https://gist.github.com/brando90/315872b2be8f2dc9935ed7e20ffb1bdc</a></li>\n<li>train.py: <a href=\"https://gist.github.com/brando90/efebf7a87a4e7cd50e0433a52998e731\" rel=\"nofollow noreferrer\">https://gist.github.com/brando90/efebf7a87a4e7cd50e0433a52998e731</a></li>\n<li>cross: <a href=\"https://discuss.pytorch.org/t/how-to-fix-index-put-requires-the-source-and-destination-dtypes-match-with-google-gemma-2-2b-in-transformers/216650\" rel=\"nofollow noreferrer\">https://discuss.pytorch.org/t/how-to-fix-index-put-requires-the-source-and-destination-dtypes-match-with-google-gemma-2-2b-in-transformers/216650</a></li>\n</ul>\n",
        "answer": "<p>You have a mismatch between data type of your neural network and the data you push through the network. It will be pretty hard to debug given the scope of the source code you have attached.</p>\n<p>Specifically these lines (formatted for readability):</p>\n<pre class=\"lang-py prettyprint-override\"><code>    device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;\n    torch_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float32\n    model: AutoModelForCausalLM = (\n        AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch_dtype).to(\n            device\n        )\n        if &quot;gemma-2&quot; not in model_name\n        else AutoModelForCausalLM.from_pretrained(\n            model_name, attn_implementation=&quot;eager&quot;\n        ).to(device)\n    )\n</code></pre>\n<ul>\n<li>I think you should not specify the <code>dtype</code> here, leave it to <a href=\"https://huggingface.co/docs/transformers/v4.48.2/en/main_classes/trainer#transformers.TrainingArguments.bf16\" rel=\"nofollow noreferrer\"><code>TrainingArguments</code></a>, casting to <code>device</code> should be fine though</li>\n<li>Try to <strong>not specify</strong> <code>dtype</code> for the whole model, as some submodules may require higher precision (e.g. accumulation in batch normalization), some lower, specifying one <code>dtype</code> for the whole model might backfire, at least for <code>fp16</code> mixed precision</li>\n<li>Try to debug without mixed precision, will be easier</li>\n<li>Follow the steps defined in <a href=\"https://huggingface.co/docs/transformers/perf_train_gpu_one\" rel=\"nofollow noreferrer\">Huggingface optimization tutorial</a></li>\n</ul>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 79421423,
            "link": "https://stackoverflow.com/questions/79421423/how-to-fix-index-put-requires-the-source-and-destination-dtypes-match-with-goo"
        }
    },
    {
        "question": "SSLError: HTTPSConnectionPool(host=&#39;huggingface.co&#39;, port=443): Max retries exceeded with url: /dslim/bert-base-NER/resolve/main/tokenizer_config.json\n<p>I am facing below issue while loading the pretrained BERT model from HuggingFace due to SSL certificate error.</p>\n<h2>Error:</h2>\n<blockquote>\n<p>SSLError: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /dslim/bert-base-NER/resolve/main/tokenizer_config.json (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self signed certificate in certificate chain (_ssl.c:1108)')))</p>\n</blockquote>\n<h2>The line that is causing the issue is:</h2>\n<pre><code>tokenizer = AutoTokenizer.from_pretrained(&quot;dslim/bert-base-NER&quot;)\n</code></pre>\n<h2>Source code:</h2>\n<pre><code>from transformers import AutoTokenizer, AutoModelForTokenClassification\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(&quot;dslim/bert-base-NER&quot;)\nmodel = AutoModelForTokenClassification.from_pretrained(&quot;dslim/bert-base-NER&quot;)\n</code></pre>\n<p>I am expecting to download pre-trained models while running the code in jupyter lab on Windows.</p>\n",
        "answer": "<p>A working solution for this is <strong>Enable your device for development</strong>.</p>\n<blockquote>\n<p>if you're writing software with Visual Studio on a computer for the first time, you will need to enable Developer Mode on both the development PC and on any devices you'll use to test your code.</p>\n</blockquote>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 75110981,
            "link": "https://stackoverflow.com/questions/75110981/sslerror-httpsconnectionpoolhost-huggingface-co-port-443-max-retries-exce"
        }
    },
    {
        "question": "Cannot load a gated model from hugginface despite having access and logging in\n<p>I am training a Llama-3.1-8B-Instruct model for a specific task.\nI have request the access to the huggingface repository, and got access, confirmed on the huggingface webapp dashboard.</p>\n<p>I tried calling the <code>huggingface_hub.login</code> function with the token to login and then download the model in the same script. I get an error, saying that I need to be logged in to access gated repositories.</p>\n<p>Then I tried loging in via the <code>huggingface-cli login</code> command, which succeeded. I got the same error after running the script.</p>\n<p>Then I tried the first approach again, but didn't pass the token, the documentation says I should get prompter for the token. The login function however seems to block after showing the HF logo, but does not show a prompt for the token.</p>\n<p>Is there something I'm missing here in order to access the models?</p>\n<p>My code:</p>\n<pre><code>hf_login()\n\nbase_model_name = 'meta-llama/Llama-3.1-8B-Instruct'\ntokenizer = AutoTokenizer.from_pretrained(base_model_name)  # this line causes error\nmodel = AutoModelForCausalLM.from_pretrained(base_model_name)\n</code></pre>\n<p>Error:</p>\n<pre><code>OSError: You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct.\n401 Client Error. (Request ID: Root=1-673f47aa-6b11aae44cd9c6523654070c;5816d1af-49a5-4262-bec0-dab7ecad66e4)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.\n</code></pre>\n<p>I'm sure I have access to the meta-llama/Llama-3.1 models. The <code>huggingface-cli whoami</code> command correctly returns my username, so I'm also logged in.</p>\n<p>My token is set to read access, I'm also trying with a write access one.</p>\n<p>EDIT: I generated a new write-access token. The login via the function <code>huggingface_hub.login</code> was successful. The models still weren't actually downloading. I tried from the windows terminal instead of the pycharm built-in terminal, and now it is working. Still don't know why it works now.</p>\n",
        "answer": "<p>you need to loging using huggingface acces token , befor getting to access of gated model, for get , if you have not any acces token, you can create from <strong>Access token</strong> section.</p>\n<p>from huggingface_hub import login</p>\n<p>login(token = &quot;hugging_face_access_token&quot;)</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 79211723,
            "link": "https://stackoverflow.com/questions/79211723/cannot-load-a-gated-model-from-hugginface-despite-having-access-and-logging-in"
        }
    },
    {
        "question": "AttributeError: &#39;AcceleratorState&#39; object has no attribute &#39;distributed_type&#39;\n<pre><code>import transformers\nfrom datasets import load_dataset\nimport tensorflow as tf\n\ntokenizer = transformers.AutoTokenizer.from_pretrained('roberta-base')\n\ndf = load_dataset('csv', data_files={'train':'FinalDatasetTrain.csv', 'test':'FinalDatasetTest.csv'})\n\ndef tokenize_function(examples):\n    return tokenizer(examples[&quot;text&quot;], truncation=True)\n\ntokenized_datasets = df.map(tokenize_function, batched=True)\ndata_collator = transformers.DataCollatorWithPadding(tokenizer=tokenizer)\n\nmodel = transformers.AutoModelForSequenceClassification.from_pretrained('roberta-base', num_labels=7)\n\ntraining_args = transformers.TFTrainingArguments(\n    output_dir=&quot;./results&quot;,\n    num_train_epochs=2,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=16,\n    save_strategy='epoch',\n    evaluation_strategy=&quot;epoch&quot;,\n    logging_dir=&quot;./logs&quot;,\n)\n\ntrainer = transformers.Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets['train'],\n    eval_dataset=tokenized_datasets['test'],\n    data_collator=data_collator,\n    tokenizer=tokenizer\n)\n\ntrainer.train()\n</code></pre>\n<p>When I run this code I get an error saying:</p>\n<blockquote>\n<p>AttributeError: 'AcceleratorState' object has no attribute 'distributed_type'.</p>\n</blockquote>\n<p>How do I fix this (I tried both Jupyter notebook and Google Colab)?</p>\n",
        "answer": "<p>To temporarily solve the problem, I downgrade the <code>accelerate</code> and <code>transformers</code> to:</p>\n<ul>\n<li>accelerate 0.15.0</li>\n<li>transformers 4.28.1</li>\n<li>tokenizers 0.13.3</li>\n</ul>\n<p>Any major version higher than these will cause the error. I cannot find any official documentation about the change of model structure regarding <code>distributed_type</code> yet.</p>\n<p>Remember to restart the runtime after any version change.</p>\n<p>Note: downgrading is just a temp solution; I usually suggest upgrading to the latest version.</p>\n<hr />\n<p><strong>UPDATE</strong></p>\n<p>This is still valid in May 2024. The latest version of <code>tokenizers</code> and <code>accelerate</code> do not work:</p>\n<ul>\n<li>transformers 4.41.0</li>\n<li>accelerate 0.30.1</li>\n<li>tokenizers 0.19.1</li>\n</ul>\n<p>which will still give the captioned error.</p>\n<hr />\n<p><strong>UPDATE 2</strong> still not yet fixed in February 2025, with the following versions:</p>\n<ul>\n<li>transformers 4.49.0</li>\n<li>accelerate 1.4.0</li>\n<li>tokenizers 0.21.0</li>\n</ul>\n<p>Same error occurs.</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 76516579,
            "link": "https://stackoverflow.com/questions/76516579/attributeerror-acceleratorstate-object-has-no-attribute-distributed-type"
        }
    },
    {
        "question": "SSLError: HTTPSConnectionPool(host=&#39;huggingface.co&#39;, port=443): Max retries exceeded with url: /dslim/bert-base-NER/resolve/main/tokenizer_config.json\n<p>I am facing below issue while loading the pretrained BERT model from HuggingFace due to SSL certificate error.</p>\n<h2>Error:</h2>\n<blockquote>\n<p>SSLError: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /dslim/bert-base-NER/resolve/main/tokenizer_config.json (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self signed certificate in certificate chain (_ssl.c:1108)')))</p>\n</blockquote>\n<h2>The line that is causing the issue is:</h2>\n<pre><code>tokenizer = AutoTokenizer.from_pretrained(&quot;dslim/bert-base-NER&quot;)\n</code></pre>\n<h2>Source code:</h2>\n<pre><code>from transformers import AutoTokenizer, AutoModelForTokenClassification\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(&quot;dslim/bert-base-NER&quot;)\nmodel = AutoModelForTokenClassification.from_pretrained(&quot;dslim/bert-base-NER&quot;)\n</code></pre>\n<p>I am expecting to download pre-trained models while running the code in jupyter lab on Windows.</p>\n",
        "answer": "<p>Here's a solution that worked for me:</p>\n<ol>\n<li>Access the huggingface.co certificate by clicking on the icon beside the web address in your browser (screenshot below) &gt; 'Connection is secure' &gt; Certificate is valid (click show certificate).</li>\n</ol>\n<p><a href=\"https://i.sstatic.net/MVj1JipB.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/MVj1JipB.png\" alt=\"enter image description here\" /></a></p>\n<ol start=\"2\">\n<li>Download the certificate 'Details' &gt; Export. Export the entire certificate (change file type).</li>\n</ol>\n<p><a href=\"https://i.sstatic.net/z1Vr4tE5.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/z1Vr4tE5.png\" alt=\"enter image description here\" /></a></p>\n<ol start=\"3\">\n<li>Include the path to the certificate in your code:</li>\n</ol>\n<blockquote>\n<pre><code>import os\nos.environ['CURL_CA_BUNDLE'] = 'C:/Users/xxxx/Downloads/certificates/huggingface.co.crt'\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained('BAAI/bge-reranker-v2-m3')\n</code></pre>\n</blockquote>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 75110981,
            "link": "https://stackoverflow.com/questions/75110981/sslerror-httpsconnectionpoolhost-huggingface-co-port-443-max-retries-exce"
        }
    },
    {
        "question": "How to Log Training Loss at Step Zero in Hugging Face Trainer or SFT Trainer?\n<p>I'm using the Hugging Face <code>Trainer</code> (or <code>SFTTrainer</code>) for fine-tuning, and I want to log the training loss at step 0 (before any training steps are executed). I know there's an <code>eval_on_start</code> option for evaluation, but I couldn't find a direct equivalent for training loss logging at the beginning of training.</p>\n<p>Is there a way to log the initial training loss at step zero (before any updates) using <code>Trainer</code> or <code>SFTTrainer</code>? Ideally, I'd like something similar to <code>eval_on_start</code>.</p>\n<p>Here's what I've tried so far:</p>\n<h4>Solution 1: Custom Callback</h4>\n<p>I implemented a custom callback to log the training loss at the start of training:</p>\n<pre class=\"lang-py prettyprint-override\"><code>from transformers import TrainerCallback\n\nclass TrainOnStartCallback(TrainerCallback):\n    def on_train_begin(self, args, state, control, logs=None, **kwargs):\n        # Log training loss at step 0\n        logs = logs or {}\n        logs[&quot;train/loss&quot;] = None  # Replace None with an initial value if available\n        logs[&quot;train/global_step&quot;] = 0\n        self.log(logs)\n\n    def log(self, logs):\n        print(f&quot;Logging at start: {logs}&quot;)\n        wandb.log(logs)\n\n# Adding the callback to the Trainer\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    args=training_args,\n    optimizers=(optimizer, scheduler),\n    callbacks=[TrainOnStartCallback()],\n)\n</code></pre>\n<p>This works but feels a bit overkill. It logs metrics at the start of training before any steps.</p>\n<h4>Solution 2: Manual Logging</h4>\n<p>Alternatively, I manually log the training loss before starting training:</p>\n<pre class=\"lang-py prettyprint-override\"><code>wandb.log({&quot;train/loss&quot;: None, &quot;train/global_step&quot;: 0})\ntrainer.train()\n</code></pre>\n<h3>Question:</h3>\n<p>Are there any built-in features in <code>Trainer</code> or <code>SFTTrainer</code> to log training loss at step zero? Or is a custom callback or manual logging the best solution here? If so, are there better ways to implement this functionality? similar to the <code>eval_on_start</code> but <code>train_on_start</code>?</p>\n<p>cross:</p>\n<ul>\n<li><a href=\"https://discuss.huggingface.co/t/how-to-log-training-loss-at-step-zero-in-hugging-face-trainer-or-sft-trainer/128188\" rel=\"nofollow noreferrer\">discuss.huggingface</a></li>\n<li><a href=\"https://github.com/huggingface/transformers/issues/34981\" rel=\"nofollow noreferrer\">github/huggingface</a></li>\n</ul>\n",
        "answer": "<p>When using huggingface <code>transformer</code> library, the output returned by the model includes the model loss. Before starting the training, simply perform a forward pass on the dataset and obtain the model loss.</p>\n<pre class=\"lang-py prettyprint-override\"><code>with torch.no_grad():\n    outputs = model(**inputs, labels=labels)\n\nloss = outputs.loss\nprint(f&quot;Loss: {loss.item()}&quot;)\n</code></pre>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 79232257,
            "link": "https://stackoverflow.com/questions/79232257/how-to-log-training-loss-at-step-zero-in-hugging-face-trainer-or-sft-trainer"
        }
    },
    {
        "question": "Early stopping in Bert Trainer instances\n<p>I am fine-tuning a BERT model for a multiclass classification task. My problem is that I don't know how to add &quot;early stopping&quot; to those Trainer instances. Any ideas?</p>\n",
        "answer": "<p>There are a couple of modifications you need to perform, prior to correctly using the <code>EarlyStoppingCallback()</code>.</p>\n<pre><code>from transformers import EarlyStoppingCallback, IntervalStrategy\n...\n...\n# Defining the TrainingArguments() arguments\nargs = TrainingArguments(\n   output_dir = &quot;training_with_callbacks&quot;,\n   evaluation_strategy = IntervalStrategy.STEPS, # &quot;steps&quot;\n   eval_steps = 50, # Evaluation and Save happens every 50 steps\n   save_total_limit = 5, # Only last 5 models are saved. Older ones are deleted.\n   learning_rate=2e-5,\n   per_device_train_batch_size=batch_size,\n   per_device_eval_batch_size=batch_size,\n   num_train_epochs=5,\n   weight_decay=0.01,\n   push_to_hub=False,\n   metric_for_best_model = 'f1',\n   load_best_model_at_end=True)\n</code></pre>\n<p>You need to:</p>\n<ol>\n<li>Use <code>load_best_model_at_end = True</code> (<code>EarlyStoppingCallback()</code> requires this to be <code>True</code>).</li>\n<li><code>evaluation_strategy</code> = <code>'steps'</code> or <code>IntervalStrategy.STEPS</code> instead of <code>'epoch'</code>.</li>\n<li><code>eval_steps = 50</code> (evaluate the metrics after <code>N steps</code>).</li>\n<li><code>metric_for_best_model = 'f1'</code></li>\n</ol>\n<p>In your <code>Trainer()</code>:</p>\n<pre><code>trainer = Trainer(\n    model,\n    args,\n    ...\n    compute_metrics=compute_metrics,\n    callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n)\n</code></pre>\n<p>Of course, when you use <code>compute_metrics()</code>, for example it can be a function like:</p>\n<pre><code>def compute_metrics(p):    \n    pred, labels = p\n    pred = np.argmax(pred, axis=1)\n    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n    recall = recall_score(y_true=labels, y_pred=pred)\n    precision = precision_score(y_true=labels, y_pred=pred)\n    f1 = f1_score(y_true=labels, y_pred=pred)    \nreturn {&quot;accuracy&quot;: accuracy, &quot;precision&quot;: precision, &quot;recall&quot;: recall, &quot;f1&quot;: f1}\n</code></pre>\n<p>The return of the <code>compute_metrics()</code> should be a dictionary and you can access whatever metric you want/compute inside the function and return.</p>\n<p>Note: In newer <code>transformers</code> version, the usage of <code>Enum</code> <code>IntervalStrategy.steps</code> is recommended (see <code>TrainingArguments()</code>) instead of plain <code>steps</code> string, the latter being soon subject to deprecation.</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 69087044,
            "link": "https://stackoverflow.com/questions/69087044/early-stopping-in-bert-trainer-instances"
        }
    },
    {
        "question": "Serving models using VLLM on Huggingface Spaces\n<p>Hi everyone so I'm trying to serve models on huggingface using docker and vllm, but am encountering model config errors and unknown model errors, I'm using the latest version of vllm and have tried mistral 7b v0.2 and v0.3 and Qwen2.5-Coder-32B.</p>\n<p>My DockerFile looks like this:</p>\n<pre><code>FROM vllm/vllm-openai:v0.8.2\n\n#Override the entrypoint to run the vLLM server with your model\nENTRYPOINT [&quot;python3&quot;, &quot;-m&quot;, &quot;vllm.entrypoints.openai.api_server&quot;, \\\n  &quot;--model&quot;, &quot;Qwen/Qwen2.5-Coder-32B&quot;, \\\n  &quot;--host&quot;, &quot;0.0.0.0&quot;, \\\n  &quot;--port&quot;, &quot;7860&quot;, \\\n  &quot;--tensor-parallel-size&quot;, &quot;4&quot;, \\\n  &quot;--trust-remote-code&quot;]\n</code></pre>\n<p>The error I get is:</p>\n<pre><code>===== Application Startup at 2025-03-31 20:38:05 =====\nINFO 03-31 13:39:57 [__init__.py:239] Automatically detected platform cuda.\nINFO 03-31 13:39:59 [api_server.py:981] vLLM API server version 0.8.2\nINFO 03-31 13:39:59 [api_server.py:982] args: Namespace(host='0.0.0.0', port=7860, uvicorn_log_level='info', disable_uvicorn_access_log=False, allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, enable_ssl_refresh=False, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='Qwen/Qwen2.5-Coder-32B', task='auto', tokenizer=None, hf_config_path=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=True, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=&lt;ConfigFormat.AUTO: 'auto'&gt;, dtype='auto', kv_cache_dtype='auto', max_model_len=None, guided_decoding_backend='xgrammar', logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=4, enable_expert_parallel=False, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=None, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, use_tqdm_on_load=True, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_config=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', scheduler_cls='vllm.core.scheduler.Scheduler', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', worker_extension_cls='', generation_config='auto', override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, additional_config=None, enable_reasoning=False, reasoning_parser=None, disable_cascade_attn=False, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, enable_server_load_tracking=False)\nTraceback (most recent call last):\n  File &quot;&lt;frozen runpy&gt;&quot;, line 198, in _run_module_as_main\n  File &quot;&lt;frozen runpy&gt;&quot;, line 88, in _run_code\n  File &quot;/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py&quot;, line 1066, in &lt;module&gt;\n    uvloop.run(run_server(args))\n  File &quot;/usr/local/lib/python3.12/dist-packages/uvloop/__init__.py&quot;, line 109, in run\n    return __asyncio.run(\n           ^^^^^^^^^^^^^^\n  File &quot;/usr/lib/python3.12/asyncio/runners.py&quot;, line 195, in run\n    return runner.run(main)\n           ^^^^^^^^^^^^^^^^\n  File &quot;/usr/lib/python3.12/asyncio/runners.py&quot;, line 118, in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;uvloop/loop.pyx&quot;, line 1518, in uvloop.loop.Loop.run_until_complete\n  File &quot;/usr/local/lib/python3.12/dist-packages/uvloop/__init__.py&quot;, line 61, in wrapper\n    return await main\n           ^^^^^^^^^^\n  File &quot;/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py&quot;, line 1016, in run_server\n    async with build_async_engine_client(args) as engine_client:\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/usr/lib/python3.12/contextlib.py&quot;, line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py&quot;, line 141, in build_async_engine_client\n    async with build_async_engine_client_from_engine_args(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/usr/lib/python3.12/contextlib.py&quot;, line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py&quot;, line 161, in build_async_engine_client_from_engine_args\n    vllm_config = engine_args.create_engine_config(usage_context=usage_context)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/usr/local/lib/python3.12/dist-packages/vllm/engine/arg_utils.py&quot;, line 1296, in create_engine_config\n    model_config = self.create_model_config()\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/usr/local/lib/python3.12/dist-packages/vllm/engine/arg_utils.py&quot;, line 1141, in create_model_config\n    return ModelConfig(\n           ^^^^^^^^^^^^\n  File &quot;/usr/local/lib/python3.12/dist-packages/vllm/config.py&quot;, line 335, in __init__\n    hf_config = get_config(self.hf_config_path or self.model,\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/usr/local/lib/python3.12/dist-packages/vllm/transformers_utils/config.py&quot;, line 321, in get_config\n    raise e\n  File &quot;/usr/local/lib/python3.12/dist-packages/vllm/transformers_utils/config.py&quot;, line 301, in get_config\n    config = AutoConfig.from_pretrained(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/usr/local/lib/python3.12/dist-packages/transformers/models/auto/configuration_auto.py&quot;, line 1133, in from_pretrained\n    raise ValueError(\nValueError: Unrecognized model in Qwen/Qwen2.5-Coder-32B. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, aria, aria_text, audio-spectrogram-transformer, autoformer, aya_vision, bamba, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, cohere2, colpali, conditional_detr, convbert, convnext, convnextv2, cpmant, ctrl, cvt, dab-detr, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deformable_detr, deit, depth_anything, depth_pro, deta, detr, diffllama, dinat, dinov2, dinov2_with_registers, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, emu3, encodec, encoder-decoder, ernie, ernie_m, esm, falcon, falcon_mamba, fastspeech2_conformer, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, gemma3, gemma3_text, git, glm, glpn, got_ocr2, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, granite, granitemoe, granitemoeshared, granitevision, graphormer, grounding-dino, groupvit, helium, hiera, hubert, ibert, idefics, idefics2, idefics3, idefics3_vision, ijepa, imagegpt, informer, instructblip, instructblipvideo, jamba, jetmoe, jukebox, kosmos-2, layoutlm, layoutlmv2, layoutlmv3, led, levit, lilt, llama, llava, llava_next, llava_next_video, llava_onevision, longformer, longt5, luke, lxmert, m2m_100, mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mimi, mistral, mistral3, mixtral, mllama, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, modernbert, moonshine, moshi, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmo2, olmoe, omdet-turbo, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, paligemma, patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, persimmon, phi, phi3, phimoe, pix2struct, pixtral, plbart, poolformer, pop2piano, prompt_depth_anything, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_5_vl, qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, rag, realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rt_detr_v2, rwkv, sam, seamless_m4t, seamless_m4t_v2, segformer, seggpt, sew, sew-d, shieldgemma2, siglip, siglip2, siglip_vision_model, smolvlm, smolvlm_vision, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, stablelm, starcoder2, superglue, superpoint, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, tapas, textnet, time_series_transformer, timesformer, timm_backbone, timm_wrapper, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, unispeech, unispeech-sat, univnet, upernet, van, video_llava, videomae, vilt, vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vitpose, vitpose_backbone, vits, vivit, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso, zamba, zamba2, zoedepth\nINFO 03-31 13:40:14 [__init__.py:239] Automatically detected platform cuda.\nINFO 03-31 13:40:16 [api_server.py:981] vLLM API server version 0.8.2\nINFO 03-31 13:40:16 [api_server.py:982] args: Namespace(host='0.0.0.0', port=7860, uvicorn_log_level='info', disable_uvicorn_access_log=False, allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, enable_ssl_refresh=False, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='Qwen/Qwen2.5-Coder-32B', task='auto', tokenizer=None, hf_config_path=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=True, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=&lt;ConfigFormat.AUTO: 'auto'&gt;, dtype='auto', kv_cache_dtype='auto', max_model_len=None, guided_decoding_backend='xgrammar', logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=4, enable_expert_parallel=False, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=None, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, use_tqdm_on_load=True, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_config=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', scheduler_cls='vllm.core.scheduler.Scheduler', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', worker_extension_cls='', generation_config='auto', override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, additional_config=None, enable_reasoning=False, reasoning_parser=None, disable_cascade_attn=False, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, enable_server_load_tracking=False)\nTraceback (most recent call last):\n  File &quot;&lt;frozen runpy&gt;&quot;, line 198, in _run_module_as_main\n  File &quot;&lt;frozen runpy&gt;&quot;, line 88, in _run_code\n  File &quot;/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py&quot;, line 1066, in &lt;module&gt;\n    uvloop.run(run_server(args))\n  File &quot;/usr/local/lib/python3.12/dist-packages/uvloop/__init__.py&quot;, line 109, in run\n    return __asyncio.run(\n           ^^^^^^^^^^^^^^\n  File &quot;/usr/lib/python3.12/asyncio/runners.py&quot;, line 195, in run\n    return runner.run(main)\n           ^^^^^^^^^^^^^^^^\n  File &quot;/usr/lib/python3.12/asyncio/runners.py&quot;, line 118, in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;uvloop/loop.pyx&quot;, line 1518, in uvloop.loop.Loop.run_until_complete\n  File &quot;/usr/local/lib/python3.12/dist-packages/uvloop/__init__.py&quot;, line 61, in wrapper\n    return await main\n           ^^^^^^^^^^\n  File &quot;/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py&quot;, line 1016, in run_server\n    async with build_async_engine_client(args) as engine_client:\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/usr/lib/python3.12/contextlib.py&quot;, line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py&quot;, line 141, in build_async_engine_client\n    async with build_async_engine_client_from_engine_args(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/usr/lib/python3.12/contextlib.py&quot;, line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py&quot;, line 161, in build_async_engine_client_from_engine_args\n    vllm_config = engine_args.create_engine_config(usage_context=usage_context)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/usr/local/lib/python3.12/dist-packages/vllm/engine/arg_utils.py&quot;, line 1296, in create_engine_config\n    model_config = self.create_model_config()\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/usr/local/lib/python3.12/dist-packages/vllm/engine/arg_utils.py&quot;, line 1141, in create_model_config\n    return ModelConfig(\n           ^^^^^^^^^^^^\n  File &quot;/usr/local/lib/python3.12/dist-packages/vllm/config.py&quot;, line 335, in __init__\n    hf_config = get_config(self.hf_config_path or self.model,\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/usr/local/lib/python3.12/dist-packages/vllm/transformers_utils/config.py&quot;, line 321, in get_config\n    raise e\n  File &quot;/usr/local/lib/python3.12/dist-packages/vllm/transformers_utils/config.py&quot;, line 301, in get_config\n    config = AutoConfig.from_pretrained(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;/usr/local/lib/python3.12/dist-packages/transformers/models/auto/configuration_auto.py&quot;, line 1133, in from_pretrained\n    raise ValueError(\nValueError: Unrecognized model in Qwen/Qwen2.5-Coder-32B. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, aria, aria_text, audio-spectrogram-transformer, autoformer, aya_vision, bamba, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, cohere2, colpali, conditional_detr, convbert, convnext, convnextv2, cpmant, ctrl, cvt, dab-detr, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deformable_detr, deit, depth_anything, depth_pro, deta, detr, diffllama, dinat, dinov2, dinov2_with_registers, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, emu3, encodec, encoder-decoder, ernie, ernie_m, esm, falcon, falcon_mamba, fastspeech2_conformer, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, gemma3, gemma3_text, git, glm, glpn, got_ocr2, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, granite, granitemoe, granitemoeshared, granitevision, graphormer, grounding-dino, groupvit, helium, hiera, hubert, ibert, idefics, idefics2, idefics3, idefics3_vision, ijepa, imagegpt, informer, instructblip, instructblipvideo, jamba, jetmoe, jukebox, kosmos-2, layoutlm, layoutlmv2, layoutlmv3, led, levit, lilt, llama, llava, llava_next, llava_next_video, llava_onevision, longformer, longt5, luke, lxmert, m2m_100, mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mimi, mistral, mistral3, mixtral, mllama, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, modernbert, moonshine, moshi, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmo2, olmoe, omdet-turbo, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, paligemma, patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, persimmon, phi, phi3, phimoe, pix2struct, pixtral, plbart, poolformer, pop2piano, prompt_depth_anything, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_5_vl, qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, rag, realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rt_detr_v2, rwkv, sam, seamless_m4t, seamless_m4t_v2, segformer, seggpt, sew, sew-d, shieldgemma2, siglip, siglip2, siglip_vision_model, smolvlm, smolvlm_vision, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, stablelm, starcoder2, superglue, superpoint, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, tapas, textnet, time_series_transformer, timesformer, timm_backbone, timm_wrapper, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, unispeech, unispeech-sat, univnet, upernet, van, video_llava, videomae, vilt, vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vitpose, vitpose_backbone, vits, vivit, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso, zamba, zamba2, zoedepth\n</code></pre>\n",
        "answer": "<p>You need to accept usage agreement of the model you want to use on HuggingFace or maybe your token is wrong.</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 79547235,
            "link": "https://stackoverflow.com/questions/79547235/serving-models-using-vllm-on-huggingface-spaces"
        }
    },
    {
        "question": "Error Loading &quot;sentence-transformers/all-MiniLM-L6-v2&quot;\n<p>I have built a document question-answering system using llama-2, but while downloading the embedding model, I am getting an OSError.</p>\n<p>OSError: We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like sentence-transformers/all-MiniLM-L6-v2 is not the path to a directory containing a file named config.json. Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.</p>\n",
        "answer": "<p>Based on what you sent,</p>\n<ol>\n<li>I suggest you double-check your working directory.</li>\n<li>Try downloading the files directly from the website:(<a href=\"https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/tree/main\" rel=\"nofollow noreferrer\">https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/tree/main</a>.) and placing them in the project folder:</li>\n</ol>\n<p>From my experience, it was either incomplete or corrupt.\nOnce you are done re-route the path to the folder and run it.</p>\n<p>Hope this helps.</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 78446414,
            "link": "https://stackoverflow.com/questions/78446414/error-loading-sentence-transformers-all-minilm-l6-v2"
        }
    },
    {
        "question": "Error: Authorization Failed When Downloading Model from CivitAI\n<p>issues while attempting to download files through the Civitai API using your API key. The error message states that the download is being aborted, and it receives a response status of 400, indicating a client-side error.</p>\n<p>been trying to get a model to download from civitai model platform for stable diffusion the api key should of allowed me to download the model because without it am getting an unauthorization to download throw civitai[<a href=\"https://i.sstatic.net/V4We2.png\" rel=\"nofollow noreferrer\">enter image description here</a>](<a href=\"https://i.sstatic.net/SamRI.png\" rel=\"nofollow noreferrer\">https://i.sstatic.net/SamRI.png</a>)</p>\n",
        "answer": "<p>It's not obvious from the documentation but what they seem to require in the URL is for the token to be specified as the first variable. i.e. straight after the model number, you need to provide the token variable as below (.../models/123223?token=[your token]). Also now the variable name is &quot;token&quot; not &quot;APIKey&quot;. After you provide your token, you can provide the &quot;type&quot; and the &quot;format&quot; and other relevant attributes such as below:</p>\n<p><a href=\"https://civitai.com/api/download/models/111111?token=at238423jibberisht0k3n2348279423&amp;type=Model&amp;format=SafeTensor\" rel=\"nofollow noreferrer\">https://civitai.com/api/download/models/111111?token=at238423jibberisht0k3n2348279423&amp;type=Model&amp;format=SafeTensor</a></p>\n<p>So if you provide token later in the URL, I got an Authentication Failed error (same attributes, different order - token coming later does not work):</p>\n<p><a href=\"https://civitai.com/api/download/models/111111?type=Model&amp;format=SafeTensor&amp;token=at238423jibberisht0k3n2348279423\" rel=\"nofollow noreferrer\">https://civitai.com/api/download/models/111111?type=Model&amp;format=SafeTensor&amp;token=at238423jibberisht0k3n2348279423</a></p>\n<p>If you haven't already gotten the answer, hope this helps</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 78122458,
            "link": "https://stackoverflow.com/questions/78122458/error-authorization-failed-when-downloading-model-from-civitai"
        }
    },
    {
        "question": "Huggingface library not being able to replace separators in create_documents: &quot;AttributeError: &#39;dict&#39; object has no attribute &#39;replace&#39;&quot;\n<p>I'm a beginner in the chatbot developer world and currently building a rag code to create a context based chatbot, but I keep getting this error, I believe it happens when the text is being split, because even after the function is called, the text remains with the &quot;\\n&quot; separators.\nThe last line of the traceback occurs in the huggingface library.</p>\n<p>The traceback:</p>\n<pre><code>Traceback (most recent call last):\n  File &quot;c:\\Users\\sophi\\miniconda3\\envs\\ambiente3.9\\lib\\runpy.py&quot;, line 197, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File &quot;c:\\Users\\sophi\\miniconda3\\envs\\ambiente3.9\\lib\\runpy.py&quot;, line 87, in _run_code\n    exec(code, run_globals)\n  File &quot;c:\\Users\\sophi\\.vscode\\extensions\\ms-python.debugpy-2024.6.0-win32-x64\\bundled\\libs\\debugpy\\__main__.py&quot;, line 39, in &lt;module&gt;\n    cli.main()\n  File &quot;c:\\Users\\sophi\\.vscode\\extensions\\ms-python.debugpy-2024.6.0-win32-x64\\bundled\\libs\\debugpy/..\\debugpy\\server\\cli.py&quot;, line 430, in main\n    run()\n  File &quot;c:\\Users\\sophi\\.vscode\\extensions\\ms-python.debugpy-2024.6.0-win32-x64\\bundled\\libs\\debugpy/..\\debugpy\\server\\cli.py&quot;, line 284, in run_file\n    runpy.run_path(target, run_name=&quot;__main__&quot;)\n  File &quot;c:\\Users\\sophi\\.vscode\\extensions\\ms-python.debugpy-2024.6.0-win32-x64\\bundled\\libs\\debugpy\\_vendored\\pydevd\\_pydevd_bundle\\pydevd_runpy.py&quot;, line 321, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File &quot;c:\\Users\\sophi\\.vscode\\extensions\\ms-python.debugpy-2024.6.0-win32-x64\\bundled\\libs\\debugpy\\_vendored\\pydevd\\_pydevd_bundle\\pydevd_runpy.py&quot;, line 135, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File &quot;c:\\Users\\sophi\\.vscode\\extensions\\ms-python.debugpy-2024.6.0-win32-x64\\bundled\\libs\\debugpy\\_vendored\\pydevd\\_pydevd_bundle\\pydevd_runpy.py&quot;, line 124, in _run_code\n    exec(code, run_globals)\n  File &quot;c:\\Users\\sophi\\Documents\\ProjetosdePesquisa\\Projeto-de-Pesquisa-SOLIRIS\\llm_rag _ver4\\utils\\rag.py&quot;, line 130, in &lt;module&gt;\n    main()\n  File &quot;c:\\Users\\sophi\\Documents\\ProjetosdePesquisa\\Projeto-de-Pesquisa-SOLIRIS\\llm_rag _ver4\\utils\\rag.py&quot;, line 126, in main\n    response = qa.invoke({&quot;input&quot;: {&quot;context&quot;: context, &quot;question&quot;: question}})\n  File &quot;c:\\Users\\sophi\\miniconda3\\envs\\ambiente3.9\\lib\\site-packages\\langchain_core\\runnables\\base.py&quot;, line 4588, in invoke\n    return self.bound.invoke(\n  File &quot;c:\\Users\\sophi\\miniconda3\\envs\\ambiente3.9\\lib\\site-packages\\langchain_core\\runnables\\base.py&quot;, line 2505, in invoke\n    input = step.invoke(input, config, **kwargs)\n  File &quot;c:\\Users\\sophi\\miniconda3\\envs\\ambiente3.9\\lib\\site-packages\\langchain_core\\runnables\\passthrough.py&quot;, line 469, in invoke\n    return self._call_with_config(self._invoke, input, config, **kwargs)\n  File &quot;c:\\Users\\sophi\\miniconda3\\envs\\ambiente3.9\\lib\\site-packages\\langchain_core\\runnables\\base.py&quot;, line 1599, in _call_with_config       \n    context.run(\n  File &quot;c:\\Users\\sophi\\miniconda3\\envs\\ambiente3.9\\lib\\site-packages\\langchain_core\\runnables\\config.py&quot;, line 380, in call_func_with_variable_args\n    return func(input, **kwargs)  # type: ignore[call-arg]\n  File &quot;c:\\Users\\sophi\\miniconda3\\envs\\ambiente3.9\\lib\\site-packages\\langchain_core\\runnables\\passthrough.py&quot;, line 456, in _invoke\n    **self.mapper.invoke(\n  File &quot;c:\\Users\\sophi\\miniconda3\\envs\\ambiente3.9\\lib\\site-packages\\langchain_core\\runnables\\base.py&quot;, line 3152, in invoke\n    output = {key: future.result() for key, future in zip(steps, futures)}\n  File &quot;c:\\Users\\sophi\\miniconda3\\envs\\ambiente3.9\\lib\\site-packages\\langchain_core\\runnables\\base.py&quot;, line 3152, in &lt;dictcomp&gt;\n    output = {key: future.result() for key, future in zip(steps, futures)}\n  File &quot;c:\\Users\\sophi\\miniconda3\\envs\\ambiente3.9\\lib\\concurrent\\futures\\_base.py&quot;, line 446, in result\n    return self.__get_result()\n  File &quot;c:\\Users\\sophi\\miniconda3\\envs\\ambiente3.9\\lib\\concurrent\\futures\\_base.py&quot;, line 391, in __get_result\n    raise self._exception\n  File &quot;c:\\Users\\sophi\\miniconda3\\envs\\ambiente3.9\\lib\\concurrent\\futures\\thread.py&quot;, line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File &quot;c:\\Users\\sophi\\miniconda3\\envs\\ambiente3.9\\lib\\site-packages\\langchain_core\\runnables\\base.py&quot;, line 4588, in invoke\n    return self.bound.invoke(\n  File &quot;c:\\Users\\sophi\\miniconda3\\envs\\ambiente3.9\\lib\\site-packages\\langchain_core\\runnables\\base.py&quot;, line 2507, in invoke\n    input = step.invoke(input, config)\n  File &quot;c:\\Users\\sophi\\miniconda3\\envs\\ambiente3.9\\lib\\site-packages\\langchain_core\\retrievers.py&quot;, line 221, in invoke\n    raise e\n  File &quot;c:\\Users\\sophi\\miniconda3\\envs\\ambiente3.9\\lib\\site-packages\\langchain_core\\retrievers.py&quot;, line 214, in invoke\n    result = self._get_relevant_documents(\n  File &quot;c:\\Users\\sophi\\miniconda3\\envs\\ambiente3.9\\lib\\site-packages\\langchain_core\\vectorstores.py&quot;, line 797, in _get_relevant_documents    \n    docs = self.vectorstore.similarity_search(query, **self.search_kwargs)\n  File &quot;c:\\Users\\sophi\\miniconda3\\envs\\ambiente3.9\\lib\\site-packages\\langchain_community\\vectorstores\\chroma.py&quot;, line 349, in similarity_search\n    docs_and_scores = self.similarity_search_with_score(\n  File &quot;c:\\Users\\sophi\\miniconda3\\envs\\ambiente3.9\\lib\\site-packages\\langchain_community\\vectorstores\\chroma.py&quot;, line 438, in similarity_search_with_score\n    query_embedding = self._embedding_function.embed_query(query)\n  File &quot;c:\\Users\\sophi\\miniconda3\\envs\\ambiente3.9\\lib\\site-packages\\langchain_huggingface\\embeddings\\huggingface.py&quot;, line 102, in embed_query\n    return self.embed_documents([text])[0]\n  File &quot;c:\\Users\\sophi\\miniconda3\\envs\\ambiente3.9\\lib\\site-packages\\langchain_huggingface\\embeddings\\huggingface.py&quot;, line 81, in embed_documents\n    texts = list(map(lambda x: x.replace(&quot;\\n&quot;, &quot; &quot;), texts))\n  File &quot;c:\\Users\\sophi\\miniconda3\\envs\\ambiente3.9\\lib\\site-**packages\\langchain_huggingface\\embeddings\\huggingface.py&quot;, line 81, in &lt;lambda&gt;   \n    texts = list(map(lambda x: x.replace(&quot;\\n&quot;, &quot; &quot;), texts))\nAttributeError: 'dict' object has no attribute 'replace'**\n</code></pre>\n<h1>This is my entire code: (except for the groq api)</h1>\n<pre><code>\nimport sys\nimport os\nfrom langchain_core.prompts import PromptTemplate \nfrom langchain_groq import ChatGroq\nfrom langchain.chains import create_retrieval_chain\nfrom langchain_community.document_loaders import TextLoader\nfrom sentence_transformers import SentenceTransformer\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\nfrom langchain_community.vectorstores import Chroma\nfrom langchain.docstore.document import Document\nimport PyPDF2\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.llms import CTransformers\n\n# Caminho para o arquivo PDF\nPDF_PATH = 'pdf_handling/entrevistas.pdf'\n\n# Caminho para salvar os dados do ChromaDB\nCHROMA_DATA_PATH = &quot;chroma_data&quot;\n\n# Modelo de embeddings\nEMBED_MODEL = &quot;all-MiniLM-L6-v2&quot;\n\n# Nome da coleção\nCOLLECTION_NAME = &quot;ruth_docs&quot;\n\ndef dict_to_string(input_dict):\n    # Convert the dictionary into a string representation\n    # This uses a list comprehension to create a list of &quot;key: value&quot; strings\n    # and then joins them with a comma and a space.\n    return ', '.join([f&quot;{key}: {value}&quot; for key, value in input_dict.items()])\n\n# Função para extrair texto de um PDF e retornar uma lista de objetos Document\ndef extract_text_from_pdf(file_path):\n    try:\n        with open(file_path, 'rb') as pdf_file:\n            pdf = PyPDF2.PdfReader(pdf_file)\n            paginas = len(pdf.pages)\n            text = &quot;&quot;\n            for i in range(paginas):\n                page = pdf.pages[i]\n                text += page.extract_text()\n            # print(type(text))\n            text_splitter = RecursiveCharacterTextSplitter(\n                chunk_size=500,\n                chunk_overlap=50,\n                length_function=len,\n                separators=['\\n\\n\\n','\\n\\n','\\n', ' ', '']\n            )\n            documents = text_splitter.create_documents([text])\n            splitted_documents = text_splitter.split_documents(documents)\n            # print(documents)\n            # print(&quot;----------------------  vs  ---------------------&quot;)\n            # print(splitted_documents)\n            return splitted_documents\n        \n    except FileNotFoundError:\n        print(&quot;Arquivo não encontrado&quot;)\n        return []\n\nclass criar_vectordb:\n\n    def save_db(self, documents, embeddings, db_path):\n        self.db_path = db_path\n        self.embeddings = embeddings\n        self.documents = documents\n        input=self.documents\n        vectordb = Chroma.from_documents(input, self.embeddings, persist_directory=self.db_path)\n        vectordb = None\n        vectordb = Chroma(db_path, embeddings)\n\n        return vectordb\n    \nembeddings = HuggingFaceEmbeddings(model_name=&quot;sentence-transformers/all-MiniLM-L6-v2&quot;, model_kwargs={'device':'cpu'})\n\n# Extraindo texto do PDF e criando a base de dados vetorial\ndocuments = extract_text_from_pdf(PDF_PATH)\n\nvectordb = criar_vectordb().save_db(documents, embeddings, CHROMA_DATA_PATH)\n\nos.environ[&quot;GROQ_API_KEY&quot;] = &quot;-&quot;\n\nruth_prompt_template = &quot;&quot;&quot;\n                            Você é um assistente virtual de RH utilizando documentos para embasar sua resposta sempre em fatos,\n                            Use as informações presentes no documento para responder a resposta do candidato,\n                            sua resposta deve ser o mais semelhante possível com a descrição presente nos documentos\n                            \n                            contexto: {context}\n                            pergunta: {question}\n                            \n                            Apenas retorne as respostas úteis em ajudar na avaliação e seleção de candidatos e nada mais, usando uma linguagem gentil e empática.\n                            Sempre responda em português, uma descrição em texto contínua, além disso adicione\n                            um ou mais emojis às vezes para demonstrar empatia e emoção.\n                            \n                            \n                            &quot;&quot;&quot;\n\nprompt = PromptTemplate(template=ruth_prompt_template, input_variables=['context', 'question'])\n\n'''\nllm = CTransformers(\n        model = &quot;model/llama-2-7b-chat.ggmlv3.q8_0.bin&quot;,\n        model_type = &quot;llama&quot;,\n        config={'max_new_tokens': 512, \n                'temperature': 0.03,\n                'context_length': 1000,\n                'repetition_penalty': 1.15}\n        )\n'''\n\nllm = ChatGroq(model_name=&quot;llama3-70b-8192&quot;, api_key=os.environ[&quot;GROQ_API_KEY&quot;])\n\nretriever = vectordb.as_retriever(search_kwargs={&quot;k&quot;: 2})\ncombine_docs_chain = create_stuff_documents_chain(\n    llm, prompt\n)\n\nqa = create_retrieval_chain(retriever, combine_docs_chain)\n\n# Main\ndef main():\n    # Exemplo de uso\n    context = &quot;Feedback negativo&quot;\n    question = &quot;Como você lida com feedback negativo?&quot;\n    response = qa.invoke({&quot;input&quot;: {&quot;context&quot;: context, &quot;question&quot;: question}})\n    print(response)\n\nif __name__ == &quot;__main__&quot;:\n    main()\n</code></pre>\n<h1>This is the huggingface file:</h1>\n<pre><code>from typing import Any, Dict, List, Optional\n\nfrom langchain_core.embeddings import Embeddings\nfrom langchain_core.pydantic_v1 import BaseModel, Extra, Field\n\nDEFAULT_MODEL_NAME = &quot;sentence-transformers/all-mpnet-base-v2&quot;\n\n\nclass HuggingFaceEmbeddings(BaseModel, Embeddings):\n    &quot;&quot;&quot;HuggingFace sentence_transformers embedding models.\n\n    To use, you should have the ``sentence_transformers`` python package installed.\n\n    Example:\n        .. code-block:: python\n\n            from langchain_huggingface import HuggingFaceEmbeddings\n\n            model_name = &quot;sentence-transformers/all-mpnet-base-v2&quot;\n            model_kwargs = {'device': 'cpu'}\n            encode_kwargs = {'normalize_embeddings': False}\n            hf = HuggingFaceEmbeddings(\n                model_name=model_name,\n                model_kwargs=model_kwargs,\n                encode_kwargs=encode_kwargs\n            )\n    &quot;&quot;&quot;\n\n    client: Any  #: :meta private:\n    model_name: str = DEFAULT_MODEL_NAME\n    &quot;&quot;&quot;Model name to use.&quot;&quot;&quot;\n    cache_folder: Optional[str] = None\n    &quot;&quot;&quot;Path to store models. \n    Can be also set by SENTENCE_TRANSFORMERS_HOME environment variable.&quot;&quot;&quot;\n    model_kwargs: Dict[str, Any] = Field(default_factory=dict)\n    &quot;&quot;&quot;Keyword arguments to pass to the Sentence Transformer model, such as `device`,\n    `prompts`, `default_prompt_name`, `revision`, `trust_remote_code`, or `token`.\n    See also the Sentence Transformer documentation: https://sbert.net/docs/package_reference/SentenceTransformer.html#sentence_transformers.SentenceTransformer&quot;&quot;&quot;\n    encode_kwargs: Dict[str, Any] = Field(default_factory=dict)\n    &quot;&quot;&quot;Keyword arguments to pass when calling the `encode` method of the Sentence\n    Transformer model, such as `prompt_name`, `prompt`, `batch_size`, `precision`,\n    `normalize_embeddings`, and more.\n    See also the Sentence Transformer documentation: https://sbert.net/docs/package_reference/SentenceTransformer.html#sentence_transformers.SentenceTransformer.encode&quot;&quot;&quot;\n    multi_process: bool = False\n    &quot;&quot;&quot;Run encode() on multiple GPUs.&quot;&quot;&quot;\n    show_progress: bool = False\n    &quot;&quot;&quot;Whether to show a progress bar.&quot;&quot;&quot;\n\n    def __init__(self, **kwargs: Any):\n        &quot;&quot;&quot;Initialize the sentence_transformer.&quot;&quot;&quot;\n        super().__init__(**kwargs)\n        try:\n            import sentence_transformers  # type: ignore[import]\n\n        except ImportError as exc:\n            raise ImportError(\n                &quot;Could not import sentence_transformers python package. &quot;\n                &quot;Please install it with `pip install sentence-transformers`.&quot;\n            ) from exc\n\n        self.client = sentence_transformers.SentenceTransformer(\n            self.model_name, cache_folder=self.cache_folder, **self.model_kwargs\n        )\n\n    class Config:\n        &quot;&quot;&quot;Configuration for this pydantic object.&quot;&quot;&quot;\n\n        extra = Extra.forbid\n\n    def embed_documents(self, texts: List[str]) -&gt; List[List[float]]:\n        &quot;&quot;&quot;Compute doc embeddings using a HuggingFace transformer model.\n\n        Args:\n            texts: The list of texts to embed.\n\n        Returns:\n            List of embeddings, one for each text.\n        &quot;&quot;&quot;\n        import sentence_transformers  # type: ignore[import]\n\n        texts = list(map(lambda x: x.replace(&quot;\\n&quot;, &quot; &quot;), texts))\n        if self.multi_process:\n            pool = self.client.start_multi_process_pool()\n            embeddings = self.client.encode_multi_process(texts, pool)\n            sentence_transformers.SentenceTransformer.stop_multi_process_pool(pool)\n        else:\n            embeddings = self.client.encode(\n                texts, show_progress_bar=self.show_progress, **self.encode_kwargs\n            )\n\n        return embeddings.tolist()\n\n    def embed_query(self, text: str) -&gt; List[float]:\n        &quot;&quot;&quot;Compute query embeddings using a HuggingFace transformer model.\n\n        Args:\n            text: The text to embed.\n\n        Returns:\n            Embeddings for the text.\n        &quot;&quot;&quot;\n        return self.embed_documents([text])[0]\n</code></pre>\n<p>While debugging, I tried to use split_text() and split_documents() instead of create_documents() and it also didn't work, all of them give me the same output: this error, and my text still containing all of the &quot;\\n&quot;. I don't know if it could be something else in the code, as this is the only part that deals with separators.\nPlease help!\nThank you!</p>\n",
        "answer": "<p>I came across the same problem while implementing an evaluation using Ragas (<a href=\"https://docs.ragas.io/en/stable/howtos/integrations/langchain/\" rel=\"nofollow noreferrer\">https://docs.ragas.io/en/stable/howtos/integrations/langchain/</a>). The error occurred, in my case, due to difference in chain construction. Depending on that, we should have different implementations:</p>\n<p>In ragas official document:</p>\n<pre><code>def format_docs(relevant_docs):\n    return &quot;\\n&quot;.join(doc.page_content for doc in relevant_docs)\nprompt = ChatPromptTemplate.from_template(template)\nchain = prompt | llm | StrOutputParser()\ndef format_docs(relevant_docs):\n    return &quot;\\n&quot;.join(doc.page_content for doc in relevant_docs)\nquery = &quot;...?&quot;\nrelevant_docs = retriever.invoke(query)\nqa_chain.invoke({&quot;context&quot;: format_docs(relevant_docs), &quot;query&quot;: query})\n</code></pre>\n<p>In my previous code:</p>\n<pre><code>chain = (\n    {&quot;context&quot;: compression_retriever, &quot;question&quot;: RunnablePassthrough()}\n    | RunnableLambda(inspect)  # Add the inspector here to print the intermediate results\n    | prompt\n    | chat\n    | StrOutputParser()\n)\nresponse = chain.invoke(query)\n</code></pre>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 78704628,
            "link": "https://stackoverflow.com/questions/78704628/huggingface-library-not-being-able-to-replace-separators-in-create-documents-a"
        }
    },
    {
        "question": "Fine Tune Huggingface model via Trainer API without labels?\n<p>I am following Huggingfaces <a href=\"https://huggingface.co/docs/transformers/training\" rel=\"nofollow noreferrer\">Tutorial on fine-tuning a model</a>. Unfortunately, they only show the procedure for fine-tuning BERT to a classifier by providing labeled data.\nMy case is a bit different: I want to fine-tune gpt-2 to generate text in a specific writing style. So my input would be just the text (in that style) without any label. I have tried the code below but that doesn't work well and results in very bad quality that\nincludes many special characters.</p>\n<pre><code>training_args = TrainingArguments(\n    output_dir=&quot;./results&quot;,\n    overwrite_output_dir=True,\n    num_train_epochs=3,\n    per_device_train_batch_size=4,\n    save_steps=10_000,\n    save_total_limit=2,\n    prediction_loss_only=True\n)\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=gen_tokenizer,\n    mlm=False,  # Suggestion from ChatGPT\n)\n\n# Initialize the Trainer\ntrainer = Trainer(\n    model=gen_model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=tokenized_dataset,\n)   \n\ntrainer.train()\n</code></pre>\n<p>Is there anything I should consder/change in my code? I am grateful for any answer because I couldn't find anything online</p>\n",
        "answer": "<p>If you want to train your model to generate new text in a style similar to that of your texts, then this is Causal Language Modeling.</p>\n<p>There is a separate page dedicated to this topic on HuggingFace: <a href=\"https://huggingface.co/docs/transformers/en/tasks/language_modeling\" rel=\"nofollow noreferrer\">https://huggingface.co/docs/transformers/en/tasks/language_modeling</a>.</p>\n<p>Or, if you want a complete guide, there is a beautiful article on Medium on how to fine-tune the GPT-2: <a href=\"https://medium.com/@prashanth.ramanathan/fine-tuning-a-pre-trained-gpt-2-model-and-performing-inference-a-hands-on-guide-57c097a3b810\" rel=\"nofollow noreferrer\">https://medium.com/@prashanth.ramanathan/fine-tuning-a-pre-trained-gpt-2-model-and-performing-inference-a-hands-on-guide-57c097a3b810</a>. The dataset is wikitext (without labels) and the code sample looks like this:</p>\n<pre><code># Define training arguments\ntraining_args = TrainingArguments(\n    output_dir='/mnt/disks/disk1/results',\n    evaluation_strategy='epoch',\n    num_train_epochs=1,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    warmup_steps=500,\n    weight_decay=0.01,\n    logging_dir='/mnt/disks/disk1/logs'\n)\n\n# Initialize Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets['train'],\n    eval_dataset=tokenized_datasets['validation'],\n)\n</code></pre>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 78589268,
            "link": "https://stackoverflow.com/questions/78589268/fine-tune-huggingface-model-via-trainer-api-without-labels"
        }
    },
    {
        "question": "Encountering ImportError when trying to import &#39;BioGptModel&#39; from &#39;transformers&#39;\n<p>I am working with the <code>Transformers</code> library in Python. My goal is to use the <code>BioGptModel</code> model. Here's the code I've written:</p>\n<pre><code>from transformers import AutoTokenizer, BioGptModel\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(&quot;microsoft/biogpt&quot;)\nmodel = BioGptModel.from_pretrained(&quot;microsoft/biogpt&quot;)\n\ninputs = tokenizer(&quot;Hello, my dog is cute&quot;, return_tensors=&quot;pt&quot;)\noutputs = model(**inputs)\n\nlast_hidden_states = outputs.last_hidden_state\n</code></pre>\n<p>Unfortunately, when I run the code I get the following error:</p>\n<blockquote>\n<p>ImportError: cannot import name 'BioGptModel' from 'transformers'&quot;, tried all solution upgrade transformer and related libraies but still same error</p>\n</blockquote>\n<p>What am I doing wrong? Is 'BioGptModel' not part of the 'transformers' library, or is there another issue with my code or environment?</p>\n",
        "answer": "<p>BioGPT is part of the transformers library and it is classified as a CausalLM so you want to write:</p>\n<pre><code>from transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(&quot;microsoft/biogpt&quot;)\nmodel = AutoModelForCausalLM.from_pretrained(&quot;microsoft/biogpt&quot;)\n</code></pre>\n<p>The problem is with the import statement. You should import the general class of model and then use the from_pretrained argument for the specific model that you want.\nHope that helps!</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 76606392,
            "link": "https://stackoverflow.com/questions/76606392/encountering-importerror-when-trying-to-import-biogptmodel-from-transformers"
        }
    },
    {
        "question": "Error (&quot;bus error&quot;) running the simplest example on Hugging Face Transformers Pipeline (Macos M1)\n<p>I'm trying to follow the quick tour example here: <a href=\"https://huggingface.co/docs/transformers/quicktour\" rel=\"nofollow noreferrer\">https://huggingface.co/docs/transformers/quicktour</a></p>\n<p>and i'm getting a &quot;bus error&quot;.</p>\n<p>My env is:</p>\n<ul>\n<li>MacOS Sonoma 14.7, Apple M1 Max chip</li>\n<li>Python 3.11.5</li>\n<li>pip install transformers datasets evaluate accelerate</li>\n<li>pip install tf-keras</li>\n</ul>\n<p>Running this code:</p>\n<pre><code>from transformers import pipeline\n\nclassifier = pipeline(&quot;sentiment-analysis&quot;)\nresult = classifier(&quot;We are very happy to show you the 🤗 Transformers library.&quot;)\nprint(result)\n</code></pre>\n<p>And getting this result:</p>\n<blockquote>\n<p>No model was supplied, defaulted to\ndistilbert/distilbert-base-uncased-finetuned-sst-2-english and\nrevision 714eb0f\n(<a href=\"https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english\" rel=\"nofollow noreferrer\">https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english</a>).</p>\n<p>Using a pipeline without specifying a model name and revision in\nproduction is not recommended. [1]</p>\n<p>92883 bus error  python main.py</p>\n</blockquote>\n<p>Any ideas?</p>\n<p>Thanks for your help.</p>\n",
        "answer": "<p>using device=0 (the first GPU) solved this:</p>\n<p><code>classifier = pipeline(&quot;sentiment-analysis&quot;, device=0)</code></p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 79184146,
            "link": "https://stackoverflow.com/questions/79184146/error-bus-error-running-the-simplest-example-on-hugging-face-transformers-pi"
        }
    },
    {
        "question": "how to create a Natural Language Inference pipeline in haystack\n<p>Could anyone help me with some advice on how to create a Natural Language Inference pipeline in haystack</p>\n<p>I want to use the Haystack framework to create a pipeline for Natural Language Inference on the response from a Retrieval-Augmented Generation (RAG) application</p>\n<p>Because I'm using haystack-ai , I cannot use farm-haystack. If I could use farm-haystack (v1.0) I believe I could do something like below:</p>\n<pre><code>from haystack import Pipeline\nfrom haystack_ai.nodes import HuggingFaceTextClassifier\n\nclassifier = HuggingFaceTextClassifier(\n    model_name_or_path=entailment_model,\n    task=&quot;text-classification&quot;,  # Task type: text classification\n    labels=[\n        &quot;entailment&quot;,\n        &quot;contradiction&quot;,\n        &quot;neutral&quot;,\n    ],  # Define the labels your model is trained on\n)\n\nclassifier_pipeline = Pipeline()\nclassifier_pipeline.add_cmponent(&quot;classifier_llm&quot;, classifier)\npremise = &quot;The sun rises in the east and sets in the west.&quot;\nhypothesis = &quot;The sun rises in the east.&quot;\n\nclassifier_pipeline.run({&quot;classifier_llm&quot;: {&quot;text&quot;: premise, &quot;text_pair&quot;: hypothesis}})\n</code></pre>\n<p>However I cannot see how to achieve the same in haystack v2.0 (haystack-ai) .</p>\n<p>Any comments or pointers welcome.</p>\n",
        "answer": "<p>I am one of the maintainers of Haystack and the author of the <a href=\"https://github.com/anakin87/haystack-entailment-checker\" rel=\"nofollow noreferrer\">Entailment Checker node (Haystack 1.x)</a>.</p>\n<p>After investigation, I found that Haystack 2.x zero-shot classification components (<a href=\"https://docs.haystack.deepset.ai/docs/transformerszeroshotdocumentclassifier\" rel=\"nofollow noreferrer\">TransformersZeroShotDocumentClassifier</a> and <a href=\"https://docs.haystack.deepset.ai/docs/transformerszeroshottextrouter\" rel=\"nofollow noreferrer\">TransformersZeroShotTextRouter</a>) do not natively support your use case. This is actually the same reason why I developed the custom EC node for Haystack 1.x.</p>\n<p>Suggestions:</p>\n<ul>\n<li>you can look at the code of the Entailment Checker node and adapt it for Haystack 2.x</li>\n<li>you can open an <a href=\"https://github.com/deepset-ai/haystack/issues\" rel=\"nofollow noreferrer\">issue on Haystack</a> to request this feature. If there's sufficient interest, we can work on developing it.</li>\n</ul>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 79377676,
            "link": "https://stackoverflow.com/questions/79377676/how-to-create-a-natural-language-inference-pipeline-in-haystack"
        }
    },
    {
        "question": "about llama-2-7b model loading from huggingface even with meta liscence access\n<p>I am trying to load this model, but it gives me the same error. How to fix this?</p>\n<pre class=\"lang-py prettyprint-override\"><code>from transformers import AutoTokenizer, AutoModelForCausalLM\n\nmodel_id = &quot;meta-llama/Llama-2-7b-hf&quot;\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    device_map={&quot;&quot;: &quot;cpu&quot;},\n    torch_dtype=&quot;float32&quot;\n)\n\nprompt = &quot;Explain quantum computing in simple terms.&quot;\ninputs = tokenizer(prompt, return_tensors=&quot;pt&quot;)\n\noutput = model.generate(**inputs, max_new_tokens=100)\nprint(tokenizer.decode(output[0], skip_special_tokens=True))\n</code></pre>\n<hr />\n<p>Resulting in the message:</p>\n<pre><code>HTTPError                                 Traceback (most recent call last)\nFile c:\\Chatbot\\huggingface\\huggingface\\lib\\site-packages\\huggingface_hub\\utils\\_http.py:409, in hf_raise_for_status(response, endpoint_name)\n    408 try:\n--&gt; 409     response.raise_for_status()\n    410 except HTTPError as e:\n\nFile c:\\Chatbot\\huggingface\\huggingface\\lib\\site-packages\\requests\\models.py:1024, in Response.raise_for_status(self)\n   1023 if http_error_msg:\n-&gt; 1024     raise HTTPError(http_error_msg, response=self)\n\nHTTPError: 403 Client Error: Forbidden for url: https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json\nThe above exception was the direct cause of the following exception:\n\nHfHubHTTPError                            Traceback (most recent call last)\nFile c:\\Chatbot\\huggingface\\huggingface\\lib\\site-packages\\huggingface_hub\\file_download.py:1484, in _get_metadata_or_catch_error(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\n    1483 try:\n-&gt; 1484     metadata = get_hf_file_metadata(\n    1485         url=url, proxies=proxies, timeout=etag_timeout, headers=headers, token=token\n    1486     )\n    1487 except EntryNotFoundError as http_error:\n\nFile c:\\Chatbot\\huggingface\\huggingface\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114, in validate_hf_hub_args.&lt;locals&gt;._inner_fn(*args, **kwargs)\n    112     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.__name__, has_token=has_token, kwargs=kwargs)\n...\n    497 # later on anyway and re-raised if needed\n    498 elif isinstance(e, HTTPError) and not isinstance(e, EntryNotFoundError):\n\n    OSError: We couldn't connect to 'https://huggingface.co' to load the files, and couldn't find them in the cached files.\n\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.\nOutput is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings...\n</code></pre>\n",
        "answer": "<p>The model you are trying to load is a so called gated model. You need an huggingface account, accept the usage conditions of the model and provide your authentication credentials in your code in order to use it. Follow the following steps to load the model:</p>\n<ol>\n<li>Please visit <a href=\"https://huggingface.co/join\" rel=\"nofollow noreferrer\">sign up</a> to create an account in case you haven't already one.</li>\n<li>Visit the <a href=\"https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\" rel=\"nofollow noreferrer\">model page</a> and accept the usage agreement. It might take some time till they accept it, but you will get an email once done so.</li>\n<li>Now visit your <a href=\"https://huggingface.co/settings/tokens\" rel=\"nofollow noreferrer\">hf account settings page</a> and create an access token.</li>\n</ol>\n<p>With the created token and when you got access to the model, you will be able to load when you provide the token via the <a href=\"https://huggingface.co/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained.token\" rel=\"nofollow noreferrer\">respective argument</a>:</p>\n<pre class=\"lang-py prettyprint-override\"><code>from transformers import AutoTokenizer, AutoModelForCausalLM\n\nmodel_id = &quot;meta-llama/Llama-2-7b-hf&quot;\ntoken = &quot;hf_...&quot;\n\ntokenizer = AutoTokenizer.from_pretrained(model_id, token=token)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    device_map={&quot;&quot;: &quot;cpu&quot;},\n    torch_dtype=&quot;float32&quot;,\n    token=token\n)\n\nprompt = &quot;Explain quantum computing in simple terms.&quot;\ninputs = tokenizer(prompt, return_tensors=&quot;pt&quot;)\n\noutput = model.generate(**inputs, max_new_tokens=100)\nprint(tokenizer.decode(output[0], skip_special_tokens=True))\n</code></pre>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 79559156,
            "link": "https://stackoverflow.com/questions/79559156/about-llama-2-7b-model-loading-from-huggingface-even-with-meta-liscence-access"
        }
    },
    {
        "question": "What&#39;s causing the high input token count in Huggingface&#39;s smolagents?\n<p>I was trying out Huggingface's new framework to build agents, named 'smolagents', and one think struck out to me when i ran the tool_calling_agents and the code_agents as well.</p>\n<p>The input token count was abnormally high, inspite of passing absolutely nothing in the system prompt of the agent.</p>\n<p>Here's the code I ran:</p>\n<pre><code>from smolagents import ToolCallingAgent, HfApiModel, tool\nfrom dotenv import load_dotenv\nfrom smolagents.prompts import TOOL_CALLING_SYSTEM_PROMPT\nimport os\n\nload_dotenv()\n\n# select model\nmodel_id = &quot;meta-llama/Llama-3.3-70B-Instruct&quot;\nmodel = HfApiModel(model_id=model_id)\n\n# creating a few tools\n@tool\ndef add_numbers(a: float, b: float) -&gt; float:\n    &quot;&quot;&quot;\n    Add two floating point numbers together.\n    Args:\n        a: first number\n        b: second number\n    &quot;&quot;&quot;\n    return a + b\n\n@tool\ndef subtract_numbers(a: float, b: float) -&gt; float:\n    &quot;&quot;&quot;\n    Subtract second number from the first number.\n    Args:\n        a: number to subtract from\n        b: number to subtract\n    &quot;&quot;&quot;\n    return a - b\n\n@tool\ndef multiply_numbers(a: float, b: float) -&gt; float:\n    &quot;&quot;&quot;\n    Multiply two floating point numbers.\n    Args:\n        a: first number\n        b: second number\n    &quot;&quot;&quot;\n    return a * b\n\n@tool\ndef divide_numbers(a: float, b: float) -&gt; float:\n    &quot;&quot;&quot;\n    Divide first number by the second number.\n    Args:\n        a: dividend (number to be divided)\n        b: divisor (number to divide by)\n    &quot;&quot;&quot;\n    if b == 0:\n        raise ValueError(&quot;Cannot divide by zero&quot;)\n    return a / b\n\n# create custom prompt.\ncustom_prompt = &quot;&quot;&quot;You are a math expert. You will only use the tools available to you.\n\nHere are the tools available to you:\n{{tool_descriptions}}\n\n{{managed_agents_descriptions}}\nIMPORTANT NOTE: You will ALWAYS evaluate the user's query and perfom query classification and print three things:\nanswer, tool_used, reasoning\n\nlike this:\n\nAnswer: answer\nTool Used: tool_name\nReasoning: reasoning for using the tool\n\nAn example:\n\nAnswer: 21.0\nTool Used: multiply\nReasoning: The tool was used to calculate the product of two numbers.\n\n\nSolve the queries STEP by STEP and feel free to use the tools available to you and do not hallucinate or make assumptions.&quot;&quot;&quot;\n\nnew_prompt = &quot;&quot;&quot;{{managed_agents_descriptions}}&quot;&quot;&quot;\n# create agent out of the model, and furnish tools to it.\nagent = ToolCallingAgent(tools=[add_numbers, subtract_numbers, multiply_numbers, divide_numbers], model=model, add_base_tools=True, system_prompt=new_prompt)\n\n# print(agent.initialize_system_prompt())\n\n# agent.run(&quot;What's 2 + 8 - 3?&quot;)\n\n\nif __name__ == &quot;__main__&quot;:\n    print(&quot;new prompt is: &quot;, agent.system_prompt)\n    print(agent.run(&quot;What's 2 + 8 - 3?&quot;))\n</code></pre>\n<p>I decided to use the 'new_prompt' system prompt which has only necessary parameters for the system prompt (essentially 0 characters since i'm not using managed_agents here), and the output still showed i had used around 5000 input tokens.</p>\n<p><a href=\"https://i.sstatic.net/2mGBfEM6.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/2mGBfEM6.png\" alt=\"Output log on the terminal\" /></a></p>\n<p>What's causing this high input token count? Am i missing something here?</p>\n",
        "answer": "<p>The run involved 4 ‘steps’.</p>\n<p>Each ‘step’ was an LLM call which included all the tool definitions as part of the prompt (i.e. input tokens).</p>\n<p>In this case it would be the definitions of the smolagents base tools and your custom ones.</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 79350004,
            "link": "https://stackoverflow.com/questions/79350004/whats-causing-the-high-input-token-count-in-huggingfaces-smolagents"
        }
    },
    {
        "question": "How to convert the LLAMA weight into the Hugging face format?\n<p>First, when I try to follow to download the LLAMA weight and Tokenization.When I try to follow to do the Hugging face format process by this command:</p>\n<pre><code>python src/transformers/models/llama/convert_llama_weights_to_hf.py \\\n    --input_dir /path/to/downloaded/llama/weights --model_size 7B --output_dir /output/path\n</code></pre>\n<p>The terminal display to me that:</p>\n<pre><code>Converting the tokenizer.\nYou are using the default legacy behaviour of the &lt;class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'&gt;. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\nTraceback (most recent call last):\n  File &quot;/home/ploy/.conda/envs/anomalygpt/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py&quot;, line 1636, in convert_slow_tokenizer\n    ).converted()\n  File &quot;/home/ploy/.conda/envs/anomalygpt/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py&quot;, line 1533, in converted\n    tokenizer = self.tokenizer()\n  File &quot;/home/ploy/.conda/envs/anomalygpt/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py&quot;, line 1526, in tokenizer\n    vocab_scores, merges = self.extract_vocab_merges_from_model(self.vocab_file)\n  File &quot;/home/ploy/.conda/envs/anomalygpt/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py&quot;, line 1502, in extract_vocab_merges_from_model\n    bpe_ranks = load_tiktoken_bpe(tiktoken_url)\n  File &quot;/home/ploy/.conda/envs/anomalygpt/lib/python3.10/site-packages/tiktoken/load.py&quot;, line 144, in load_tiktoken_bpe\n    contents = read_file_cached(tiktoken_bpe_file, expected_hash)\n  File &quot;/home/ploy/.conda/envs/anomalygpt/lib/python3.10/site-packages/tiktoken/load.py&quot;, line 63, in read_file_cached\n    contents = read_file(blobpath)\n  File &quot;/home/ploy/.conda/envs/anomalygpt/lib/python3.10/site-packages/tiktoken/load.py&quot;, line 21, in read_file\n    with blobfile.BlobFile(blobpath, &quot;rb&quot;) as f:\n  File &quot;/home/ploy/.conda/envs/anomalygpt/lib/python3.10/site-packages/blobfile/_ops.py&quot;, line 393, in BlobFile\n    return default_context.BlobFile(\n  File &quot;/home/ploy/.conda/envs/anomalygpt/lib/python3.10/site-packages/blobfile/_context.py&quot;, line 894, in BlobFile\n    f = io.FileIO(path, mode=mode)\nFileNotFoundError: [Errno 2] No such file or directory: '/path/to/downloaded/llama/weights/tokenizer.model'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;/home/ploy/transformers/./src/transformers/models/llama/convert_llama_weights_to_hf.py&quot;, line 502, in write_tokenizer\n    tokenizer = tokenizer_class(input_tokenizer_path)\n  File &quot;/home/ploy/.conda/envs/anomalygpt/lib/python3.10/site-packages/transformers/models/llama/tokenization_llama_fast.py&quot;, line 157, in __init__\n    super().__init__(\n  File &quot;/home/ploy/.conda/envs/anomalygpt/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py&quot;, line 138, in __init__\n    fast_tokenizer = convert_slow_tokenizer(self, from_tiktoken=True)\n  File &quot;/home/ploy/.conda/envs/anomalygpt/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py&quot;, line 1638, in convert_slow_tokenizer\n    raise ValueError(\nValueError: Converting from Tiktoken failed, if a converter for SentencePiece is available, provide a model path with a SentencePiece tokenizer.model file.Currently available slow-&gt;fast convertors: ['AlbertTokenizer', 'BartTokenizer', 'BarthezTokenizer', 'BertTokenizer', 'BigBirdTokenizer', 'BlenderbotTokenizer', 'CamembertTokenizer', 'CLIPTokenizer', 'CodeGenTokenizer', 'ConvBertTokenizer', 'DebertaTokenizer', 'DebertaV2Tokenizer', 'DistilBertTokenizer', 'DPRReaderTokenizer', 'DPRQuestionEncoderTokenizer', 'DPRContextEncoderTokenizer', 'ElectraTokenizer', 'FNetTokenizer', 'FunnelTokenizer', 'GPT2Tokenizer', 'HerbertTokenizer', 'LayoutLMTokenizer', 'LayoutLMv2Tokenizer', 'LayoutLMv3Tokenizer', 'LayoutXLMTokenizer', 'LongformerTokenizer', 'LEDTokenizer', 'LxmertTokenizer', 'MarkupLMTokenizer', 'MBartTokenizer', 'MBart50Tokenizer', 'MPNetTokenizer', 'MobileBertTokenizer', 'MvpTokenizer', 'NllbTokenizer', 'OpenAIGPTTokenizer', 'PegasusTokenizer', 'Qwen2Tokenizer', 'RealmTokenizer', 'ReformerTokenizer', 'RemBertTokenizer', 'RetriBertTokenizer', 'RobertaTokenizer', 'RoFormerTokenizer', 'SeamlessM4TTokenizer', 'SqueezeBertTokenizer', 'T5Tokenizer', 'UdopTokenizer', 'WhisperTokenizer', 'XLMRobertaTokenizer', 'XLNetTokenizer', 'SplinterTokenizer', 'XGLMTokenizer', 'LlamaTokenizer', 'CodeLlamaTokenizer', 'GemmaTokenizer', 'Phi3Tokenizer']\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;/home/ploy/transformers/./src/transformers/models/llama/convert_llama_weights_to_hf.py&quot;, line 601, in &lt;module&gt;\n    main()\n  File &quot;/home/ploy/transformers/./src/transformers/models/llama/convert_llama_weights_to_hf.py&quot;, line 576, in main\n    write_tokenizer(\n  File &quot;/home/ploy/transformers/./src/transformers/models/llama/convert_llama_weights_to_hf.py&quot;, line 504, in write_tokenizer\n    raise ValueError(\nValueError: Failed to instantiate tokenizer. Please, make sure you have sentencepiece and protobuf installed.\n</code></pre>\n<p>From above, which is the problem, so how can I do it?</p>\n<p>How to solve this problem or If every one used to face this please tell me the process each step by step.</p>\n",
        "answer": "<p>Just dropping answer here, as I also got the same error.</p>\n<p>Initially it looks like error is with package but if you trace back the error it says something like <code>&quot;FileNotFoundError: [Errno 2] No such file or directory: '/path/to/downloaded/llama/weights/tokenizer.model'</code>\n&quot;</p>\n<p>Please make sure you are giving correct path and give absolute path rather than relative.</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 79292556,
            "link": "https://stackoverflow.com/questions/79292556/how-to-convert-the-llama-weight-into-the-hugging-face-format"
        }
    },
    {
        "question": "Installing triton in windows\n<p>I am running the following on command line in windows:</p>\n<pre><code> autotrain dreambooth --model stabilityai/stable-diffusion-xl-base-1.0 --image-path ./Anas --prompt 'a photo of arzk man' --resolution 1024 --batch-size 1 --num-steps 500 --fp16 --gradient-accumulation 4 --lr 1e-4 --project-name thesis\n</code></pre>\n<p>and I got the following error message</p>\n<pre><code>RuntimeError: Failed to import diffusers.pipelines.stable_diffusion_xl.pipeline_stable_diffusion_xl because of the following error (look up to see its traceback):\nFailed to import diffusers.models.autoencoder_kl because of the following error (look up to see its traceback):\nmodule 'triton' has no attribute '__version__\n</code></pre>\n<p>I tried to pip install triton but I am getting the following error message</p>\n<pre><code>ERROR: Could not find a version that satisfies the requirement triton (from versions: none)\nERROR: No matching distribution found for triton\n</code></pre>\n<p>I read in triton's <a href=\"https://github.com/openai/triton?tab=readme-ov-file#compatibility\" rel=\"nofollow noreferrer\">github</a> that it is only compatible with Linux.</p>\n<p>I don't know how to use Linux. would i need to create a virtual environment there and install python and... that would be crazy to move everything there (probably it is not I just dont know what to do in linux).</p>\n<p>any recommendation to what I could do ?</p>\n",
        "answer": "<p>According to the official <a href=\"https://github.com/openai/triton\" rel=\"noreferrer\">GitHub repo for Triton</a>, Triton is compatible with Linux. There's no support for Windows.</p>\n<p>A possible workaround to this would be to install <a href=\"https://learn.microsoft.com/en-us/windows/wsl/install\" rel=\"noreferrer\">The Windows Subsystem for Linux (WSL)</a> and then install the latest stable release of Triton in that environment.</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 77916908,
            "link": "https://stackoverflow.com/questions/77916908/installing-triton-in-windows"
        }
    },
    {
        "question": "Fine-tuning TheBloke/Llama-2-13B-chat-GPTQ model with Hugging Face Transformers library throws Exllama error\n<p>I am trying to fine-tune the TheBloke/Llama-2-13B-chat-GPTQ model using the Hugging Face Transformers library. I am using a JSON file for the training and validation datasets. However, I am encountering an error related to Exllama backend when I try to run the script.</p>\n<p>Here is my code:</p>\n<pre class=\"lang-py prettyprint-override\"><code>from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\nfrom datasets import load_dataset\nimport torch\n\n# Check GPU availability\nprint(&quot;Available GPU devices:&quot;, torch.cuda.device_count())\nprint(&quot;Name of the first available GPU:&quot;, torch.cuda.get_device_name(0))\n\n# Load model and tokenizer\nmodel_name = &quot;TheBloke/Llama-2-13B-chat-GPTQ&quot;\n\n# Load tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\n\n# Move the model to GPU\nmodel.to('cuda')\n\n# Load training and validation data\ntrain_data = load_dataset('json', data_files='train_data.jsonl')\nval_data = load_dataset('json', data_files='val_data.jsonl')\n\n# Function to format the data\ndef formatting_func(example):\n    return tokenizer(example['input'], example.get('output', ''), truncation=True, padding='max_length')\n\n# Prepare training and validation data\ntrain_data = train_data.map(formatting_func)\nval_data = val_data.map(formatting_func)\n\n# Set training arguments\ntraining_args = TrainingArguments(\n    output_dir=&quot;./output&quot;,\n    overwrite_output_dir=True,\n    num_train_epochs=1,\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=64,\n    save_steps=10_000,\n    save_total_limit=2,\n)\n\n# Create trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_data,\n    eval_dataset=val_data,\n)\n\n# Start training\ntrainer.train()\n\n# Save the model\nmodel.save_pretrained(&quot;./output&quot;)\n\n</code></pre>\n<p>The error message I get is:</p>\n<pre><code>ValueError: Found modules on cpu/disk. Using Exllama backend requires all the \nmodules to be on GPU. You can deactivate exllama backend by setting \n`disable_exllama=True` in the quantization config object.\n</code></pre>\n<p>I have already moved the model to GPU using model.to('cuda'), but the error persists. Any help would be greatly appreciated.</p>\n<p>I tried moving the model to the GPU using model.to('cuda') before initiating the training process, as suggested in the Hugging Face documentation. I also ensured that my environment has all the required packages and dependencies installed. I was expecting the model to fine-tune on my custom JSON dataset without any issues.</p>\n<p>However, despite moving the model to the GPU, I still encounter the Exllama backend error. I am not sure why this is happening, as the model should be on the GPU as per my code. I am looking for a way to resolve this error and successfully fine-tune the model on my custom dataset.</p>\n",
        "answer": "<p>Have you tried to do use the <code>device_map</code> attribute in the <code>from_pretrained</code> function ?</p>\n<pre><code>AutoModelForCausalLM.from_pretrained(model_name, device_map='cuda')\n</code></pre>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 76983305,
            "link": "https://stackoverflow.com/questions/76983305/fine-tuning-thebloke-llama-2-13b-chat-gptq-model-with-hugging-face-transformers"
        }
    },
    {
        "question": "Fine-tuning a Text2Text LLM using different tokenizers for input and output\n<p>I’m just starting to explore the Hugging Face library and have a question related to Text2Text models.</p>\n<p>Suppose I have a <code>model1</code> (a Text2Text model, e.g. <code>BART</code>) pre-trained on a masked language modeling task, where it has learned the syntactic structure based on the tokenization strategy of <code>tokenizer1</code>.</p>\n<p>Now, I want to fine-tune <code>model1</code> using the same style of text related to the masked language modeling task as input, but aim to decode outputs into a different format using a separate tokenizer (<code>tokenizer2</code>).</p>\n<p>Is this possible? The approach I had in mind involves sequential text generation:</p>\n<ol>\n<li>The original <code>model1</code> generates text.</li>\n<li>A fine-tuned <code>model2</code> continues the generation based on the output of\nmodel1.</li>\n</ol>\n<p>Apologies if this is something trivial. Any comment or suggestion on specific tutorials is really appreciated!</p>\n",
        "answer": "<p>the output from model1 is text form and the input to model2 is text form, too. So it is ok.</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 79363152,
            "link": "https://stackoverflow.com/questions/79363152/fine-tuning-a-text2text-llm-using-different-tokenizers-for-input-and-output"
        }
    },
    {
        "question": "SSLError: HTTPSConnectionPool(host=&#39;huggingface.co&#39;, port=443): Max retries exceeded with url: /dslim/bert-base-NER/resolve/main/tokenizer_config.json\n<p>I am facing below issue while loading the pretrained BERT model from HuggingFace due to SSL certificate error.</p>\n<h2>Error:</h2>\n<blockquote>\n<p>SSLError: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /dslim/bert-base-NER/resolve/main/tokenizer_config.json (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self signed certificate in certificate chain (_ssl.c:1108)')))</p>\n</blockquote>\n<h2>The line that is causing the issue is:</h2>\n<pre><code>tokenizer = AutoTokenizer.from_pretrained(&quot;dslim/bert-base-NER&quot;)\n</code></pre>\n<h2>Source code:</h2>\n<pre><code>from transformers import AutoTokenizer, AutoModelForTokenClassification\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(&quot;dslim/bert-base-NER&quot;)\nmodel = AutoModelForTokenClassification.from_pretrained(&quot;dslim/bert-base-NER&quot;)\n</code></pre>\n<p>I am expecting to download pre-trained models while running the code in jupyter lab on Windows.</p>\n",
        "answer": "<p>Try to add the following in your main python file</p>\n<pre><code>import os\n\nos.environ['CURL_CA_BUNDLE'] = ''\n</code></pre>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 75110981,
            "link": "https://stackoverflow.com/questions/75110981/sslerror-httpsconnectionpoolhost-huggingface-co-port-443-max-retries-exce"
        }
    },
    {
        "question": "Huggingface Transformers (PyTorch) - Custom training loop doubles speed?\n<p>I've found something quite strange when using Huggingface Transformers with a custom training loop in PyTorch.</p>\n<p>But first, some context: I'm currently trying to fine tune a pretrained GPT2 small (GPT2LMHeadModel; the ~170M param version) on multiple nodes, using Huggingface Accelerate. I'm using Huggingface's <code>datasets</code> library for training.</p>\n<p>Of course, the first step in this process in accelerate is to write a custom PyTorch training loop, which I did with the help of <a href=\"https://www.youtube.com/watch?v=Dh9CL8fyG80&amp;embeds_euri=https%3A%2F%2Fhuggingface.co%2F&amp;feature=emb_title\" rel=\"nofollow noreferrer\">the official tutorial from huggingface</a>. Naturally, I decided to test the model with this new training loop before implementing accelerate to ensure it actually worked.</p>\n<p>Here's the relevant code from my original model, as well as the corresponding code from the new training loop:</p>\n<p><em>Note: <code>BATCH_SIZE</code> is equal to 2 in both models. All code not shown is exactly the same between both models.</em></p>\n<p>Original:</p>\n<pre><code>data = data['train']\n\ndc = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n\ntrain_args = TrainingArguments(\n    output_dir=OUTPUT_DIRECTORY,\n    overwrite_output_dir=True,\n    num_train_epochs=1,\n    per_device_train_batch_size=BATCH_SIZE,\n    save_steps=10_000,\n    save_total_limit=1, # How many &quot;checkpoints&quot; to save at a time\n    prediction_loss_only=True,\n    remove_unused_columns=False,\n    optim=&quot;adamw_torch&quot;\n)\n\ntrainer = Trainer(\n    model=model,\n    args=train_args,\n    data_collator=dc,\n    train_dataset=data\n)\n\ntrainer.train()\n</code></pre>\n<p>Custom Train Loop:</p>\n<pre><code>data = data['train']\n\ndc = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n\noptimizer = AdamW(model.parameters(), lr=5e-5)\n\ntrain_dl = DataLoader(\n    data, shuffle=True, batch_size=BATCH_SIZE, collate_fn=dc\n)\n\nepochs = 1\ntraining_steps = epochs * len(train_dl)\nscheduler = get_scheduler(\n    &quot;linear&quot;,\n    optimizer=optimizer,\n    num_warmup_steps=0,\n    num_training_steps=training_steps\n)\n\nprogress_bar = tqdm(range(training_steps))\n\nmodel.train()\nfor epoch in range(epochs):\n    for batch in train_dl:\n        # Run a batch through the model\n        outputs = model(**batch)\n        loss = outputs.loss\n        loss.backward()\n\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n        progress_bar.update(1)\n</code></pre>\n<p>I tested it (with one node of course) with two GPUs, both 16GB each. And it worked... but suspiciously well.</p>\n<ul>\n<li>My original model averaged about 1-2 iterations/s.</li>\n<li>My custom loop on the other hand averaged about 3-4 iterations/s.</li>\n</ul>\n<p>This is absolutely bizarre. How is it possible that simply adding my own training loop, that's just a couple of lines of code, is not only faster than the official one provided by Huggingface - but nearly TWICE as fast? Did I write the training loop incorrectly? Am I completely missing something here?</p>\n",
        "answer": "<p>In your training loop, you call <code>optimizer.step()</code> directly after computing the loss, with no gradient accumulation.\nDefault <a href=\"https://huggingface.co/docs/transformers/main_classes/trainer\" rel=\"nofollow noreferrer\">Trainer</a> uses gradient accumulation (1 gradient accumulation step by default), this causes gradient to be accumulated over multiple batches before model weights update; this is useful to improve accuracy but slows down the training procedure.</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 75952444,
            "link": "https://stackoverflow.com/questions/75952444/huggingface-transformers-pytorch-custom-training-loop-doubles-speed"
        }
    },
    {
        "question": "Model Loader errors in Text Gen Web UI`\n<p>I have tried loading multiple models from HuggingFace using Text Gen Web UI, but no matter the model or the loader, I get the same &quot;ModuleNotFoundError&quot; for the loaders.</p>\n<p>Importantly, I am using an Intel i7 and not a GPU, but I have been able to run smaller models using different UI tools in the past.</p>\n<p>Steps I followed:</p>\n<ol>\n<li>Cloned Text Gen from Git Created a new environment in Anaconda.Navigator.</li>\n<li>Using Visual Studio Code opened Text Generation Web UI.</li>\n<li>I selected the environment's Python as the Python interpreter.</li>\n<li>Selected to activate the relevant conda environment and installed the requirements.txt file for Text Generation Web UI.</li>\n<li>Started the application using the one_click.py file that comes with Text Generation Web UI.</li>\n</ol>\n<p>Upon failing to get a model to load, I tried many things:</p>\n<ol>\n<li>Re-installing pip packages.</li>\n<li>Installing a different version of torch: pip install torch torchvision torchaudio --index-url <a href=\"https://download.pytorch.org/whl/cpu\" rel=\"nofollow noreferrer\">https://download.pytorch.org/whl/cpu</a></li>\n<li>Deleting the environment and starting over.</li>\n<li>Asking ChatGPT and CoPilot a million questions.</li>\n</ol>\n<p>The same errors appear over and over. It is driving me crazy!</p>\n<p>Examples (each of these have been installed, in the correct environment):</p>\n<blockquote>\n<p>ModuleNotFoundError: Failed to import 'autogptq'. Please install it\nmanually following the instructions in the AutoGPTQ GitHub repository.</p>\n<p>ModuleNotFoundError: No module named 'exllamav2'</p>\n<p>OSError: Error no file named pytorch_model.bin, model.safetensors,\ntf_model.h5, model.ckpt.index or flax_model.msgpack found in directory\nmodels\\meta-llama_Llama-3.2-1B.</p>\n</blockquote>\n",
        "answer": "<p>The issue here is that for some reason, that's never really explained, even if you have python installed correctly, with all of the dependencies required... the webui will still use it's own embedded version of python and ignore what you've got installed. So you can troubleshoot and install until you're blue in the face and it won't change a thing. Ask me how I know and why I'm annoyed with their setup right now... The easiest fix is to just do the setup yourself.</p>\n<p>Download Anaconda\nopen &quot;anaconda prompt&quot; (not cmd)</p>\n<pre><code>conda create --name textgen python=3.10\nconda activate textgen\ncd C:\\path\\to\\text-generation-webui\npip install auto-gptq\npip install --upgrade -r requirements.txt\n</code></pre>\n<p>from then on when you want to start the server. start anaconda, move to the webui folder and use</p>\n<pre><code>conda activate textgen\npython server.py\n</code></pre>\n<p>Hopefully I didn't butcher any of those commands but this should be enough to hopefully get you over the hurdle.</p>\n<p>please note that these steps were completed after the initial install of the webui.</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 79060002,
            "link": "https://stackoverflow.com/questions/79060002/model-loader-errors-in-text-gen-web-ui"
        }
    },
    {
        "question": "Repeated wandb.init() in parallelized wandb sweeps\n<p>I wrote some code trying to parallelize my wandb sweeps since the model I am working with takes a long time to converge and I have a lot of subprocesses to sweep through. Basically I don’t have the luxury of time right now. Here’s a generalized snippet of my code:</p>\n<pre class=\"lang-py prettyprint-override\"><code>def run_pipeline(args):\n    # Stuff happens here\n\n    # Wandb init\n    group = &quot;within_session&quot; if session_config[&quot;within_session&quot;] else &quot;across_session&quot;\n    run = wandb.init(name=f&quot;{sessions[i]}_{group}_decoder_run&quot;, group=group, config=sweep_config, reinit=True)\n\n    # Model training\n\n    return results\n\n\ndef run_pipeline_wrapper(args):\n    # Stuff happens here\n    run_pipeline(args)\n\n    return None\n\n\nif __name__ == &quot;__main__&quot;:\n    total_runs = 30\n    agents = 5\n    runs_per_agent = total_runs // agents\n\n    sweep_config = {'method': 'random'}\n    parameters_dict = {\n        # Lota of parameters to sweep\n    }\n    sweep_config['parameters'] = parameters_dict\n\n    # Create a sweep id that stores sweep ids\n    sweep_id_json_path = 'sweep_id.json'\n    if not os.path.exists(sweep_id_json_path):\n        with open(sweep_id_json_path, 'w') as f:\n            json.dump({}, f)\n    sweep_id_json = json.load(open(sweep_id_json_path, 'r'))\n\n    # Sessions_list = number of unique data that I need to run my sweeps\n    for i in range(len(sessions_list)):\n\n        # Preparing a partial method to pass\n        run_pipeline_with_args = partial(run_pipeline_wrapper, args)\n\n        # I cache the existing sweep_ids in a json file to help in attaching sweep ids if I rerun the code again\n        if f&quot;{sessions_list[i]}_{is_within}&quot; not in sweep_id_json:\n            sweep_id = wandb.sweep(sweep_config, project=f&quot;HPC_model_{sess}_session_{data}_{data_type}&quot;)\n        else:\n            sweep_id = wandb.sweep(sweep_config, project=f&quot;HPC_model_{sess}_session_{data}_{data_type}&quot;\n                                   , prior_runs=sweep_id_json[f&quot;{sessions_list[i]}_{is_within}&quot;])\n\n\n        # This is the parallelization logic, where I parallelize the sweeps\n        with concurrent.futures.ThreadPoolExecutor(max_workers=agents) as executor:\n            futures = [\n                executor.submit(wandb.agent, sweep_id, run_pipeline_with_args, count=runs_per_agent)\n                for _ in range(agents)\n            ]\n\n            concurrent.futures.wait(futures)\n</code></pre>\n<p>When I run this code, it gets stuck on wandb.init(), with that process eventually being terminated due to a timeout. I don’t think this is a problem of increasing wandb’s timeout. How do I fix this? Do you think this might be a problem because of my parallelization logic? If so, how do you devs parallelize your wandb sweeps in-code?</p>\n<p><a href=\"https://i.sstatic.net/bZJyyvqU.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/bZJyyvqU.png\" alt=\"wandb logs\" /></a></p>\n",
        "answer": "<p>If wandb.init runs I don't think it's a problem with parallelization. In any case, you should check if the arguments were all passed correctly by the executor.<br/>You have to take into account that ThreadPoolExecutor uses a pool of threads to execute calls asynchronously. If the threads do not run independently and wait on the results of another deadlocks can occur.<br/><br/>Have you try to parallelize W&amp;B Sweep agents within a Jupyter Notebook, heres the <a href=\"https://docs.wandb.ai/guides/sweeps/parallelize-agents/\" rel=\"nofollow noreferrer\">link</a>.<br/><br/>EDIT:<br/><br/>You are using threading and in Python threads are bound by a global interpreter lock (GIL), threading won't spread work across CPU cores.<br/>For CPU-bound work you should use multiprocessing instead of threading. For that Python has the <code>ProcessPoolExecutor</code>. Here is a <a href=\"https://docs.python.org/3/library/concurrent.futures.html#processpoolexecutor\" rel=\"nofollow noreferrer\">link</a> to it in Python3 docs.<br/><br/>If you use Slurm for cluster management in your HPC you should see these links:</p>\n<ol>\n<li><a href=\"https://docs.wandb.ai/support/run_sweeps_slurm/\" rel=\"nofollow noreferrer\">How should I run sweeps on SLURM?</a></li>\n<li><a href=\"https://wandb-utils.readthedocs.io/en/latest/managing_jobs_on_slurm.html\" rel=\"nofollow noreferrer\">Manging wandb agents on a slurm cluster</a></li>\n<li><a href=\"https://community.wandb.ai/t/how-does-one-do-hyper-parameter-sweeps-when-using-hpcs-clusters/1317\" rel=\"nofollow noreferrer\">How does one do hyper parameter sweeps when using HPCs/clusters?</a></li>\n</ol>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 79240870,
            "link": "https://stackoverflow.com/questions/79240870/repeated-wandb-init-in-parallelized-wandb-sweeps"
        }
    },
    {
        "question": "Error downloading &#39;pyannote/speaker-diarization&#39; pipeline despite having read access token\n<p>I'm trying to use the <code>pyannote.audio</code> library to download the &quot;pyannote/speaker-diarization&quot; pipeline, but I keep encountering the following error:</p>\n<pre><code>Could not download 'pyannote/speaker-diarization' pipeline.\nIt might be because the pipeline is private or gated so make\nsure to authenticate. Visit https://hf.co/settings/tokens to\ncreate your access token and retry with:\n\n   &gt;&gt;&gt; Pipeline.from_pretrained('pyannote/speaker-diarization',\n   ...                          use_auth_token='hf...')\n</code></pre>\n<p>I have a read-type access token, but the error persists. Here’s the code I’m using:</p>\n<pre class=\"lang-py prettyprint-override\"><code>from pyannote.audio import Pipeline\n\npipeline = Pipeline.from_pretrained(\n    'pyannote/speaker-diarization',\n    use_auth_token='YOUR_ACCESS_TOKEN'\n)\n</code></pre>\n<p>You can find the code I'm running in this Colab notebook as shown below:</p>\n<p><a href=\"https://i.sstatic.net/4wd31vLj.jpg\" rel=\"nofollow noreferrer\">enter image description here</a></p>\n<p><a href=\"https://colab.research.google.com/drive/16fQA8tY5w3hZOIHps4ndAgru87g6tj79?usp=sharing#scrollTo=jKG14DGYbwku\" rel=\"nofollow noreferrer\">Pyannote plays and Whisper rhymes v 2.1</a>.</p>\n<p>Any help would be appreciated!</p>\n",
        "answer": "<p>We had a similar problem in a Huggingface space. Despite having set up a token called HF_TOKEN in the space's secrets, the code</p>\n<p>pipeline = Pipeline.from_pretrained(&quot;pyannote/speaker-diarization-3.1&quot;,use_auth_token='HF_TOKEN')</p>\n<p>would give the same error you are seeing.</p>\n<p>Changing the code to</p>\n<p>pipeline = Pipeline.from_pretrained(&quot;pyannote/speaker-diarization-3.1&quot;,use_auth_token=True)</p>\n<p>fixed the issue.</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 78805545,
            "link": "https://stackoverflow.com/questions/78805545/error-downloading-pyannote-speaker-diarization-pipeline-despite-having-read-ac"
        }
    },
    {
        "question": "Unable to import transformers.models.bert.modeling_tf_bert on macOS?\n<p>As the title is self-descriptive, I'm not able to import the <code>BertTokenizer</code> and <code>TFBertModel</code> classes from the <code>transformers</code> package through the following code:</p>\n<pre><code>from transformers import BertTokenizer, TFBertModel\n\ntokenizer = BertTokenizer.from_pretrained(BERT_PATH)\nmodel = TFBertModel.from_pretrained(BERT_PATH)\ntext = &quot;Replace me by any text you'd like.&quot;\nencoded_input = tokenizer(text, return_tensors='tf')\nresp = model(encoded_input)\nprint(resp)\n</code></pre>\n<p>As a result, I'm getting the following error:</p>\n<pre><code>RuntimeError: Failed to import transformers.models.bert.modeling_tf_bert because of the following error (look up to see its traceback):\ndlopen(/Users/tk/miniforge3/envs/QA-benchmark/lib/python3.10/site-packages/tensorflow-plugins/libmetal_plugin.dylib, 0x0006): symbol not found in flat namespace '_TF_GetInputPropertiesList'\n</code></pre>\n<p>Here is my software stack:</p>\n<pre><code>OS: macOS Ventura 13.3.1\nPython: 3.10\nTensorFlow: macOS-tensorflow 2.9.0\nTransformers: 4.28.0\nBERT model: uncased_L-12_H-768_A-12\n</code></pre>\n<p>p.s. I've already posted this issue on the GitHub repository of transformers.</p>\n",
        "answer": "<pre><code>from transformers import BertTokenizer, TFBertModel, BertModel\n\nmodel = BertModel.from_pretrained('bert-base-uncased')\n\ntokenizer = BertTokenizer.from_pretrained(model)\nmodel = TFBertModel.from_pretrained(model)\ntext = &quot;Replace me by any text you'd like.&quot;\nencoded_input = tokenizer(text, return_tensors='tf')\nresp = model(encoded_input)\nprint(resp)\n</code></pre>\n<p>Try this! it works for me.</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 76025069,
            "link": "https://stackoverflow.com/questions/76025069/unable-to-import-transformers-models-bert-modeling-tf-bert-on-macos"
        }
    },
    {
        "question": "How to load a huggingface dataset from local path?\n<p>Take a simple example in this website, <a href=\"https://huggingface.co/datasets/Dahoas/rm-static\" rel=\"nofollow noreferrer\">https://huggingface.co/datasets/Dahoas/rm-static</a>:</p>\n<p>if I want to load this dataset online, I just directly use,</p>\n<pre><code>from datasets import load_dataset\ndataset = load_dataset(&quot;Dahoas/rm-static&quot;) \n</code></pre>\n<p>What if I want to load dataset from local path, so I download the files and keep the same folder structure from web <code>Files and versions</code> fristly,</p>\n<pre><code>-data\n|-test-00000-of-00001-bf4c733542e35fcb.parquet\n|-train-00000-of-00001-2a1df75c6bce91ab.parquet\n-.gitattributes\n-README.md\n-dataset_infos.json\n</code></pre>\n<p>Then, put them into my folder, but shows error when loading:</p>\n<pre><code>dataset_path =&quot;/data/coco/dataset/Dahoas/rm-static&quot;\ntmp_dataset = load_dataset(dataset_path)\n</code></pre>\n<p>It shows <code>FileNotFoundError: No (supported) data files or dataset script found in /data/coco/dataset/Dahoas/rm-static.</code></p>\n",
        "answer": "<p>You can load a csv data file from local path using:</p>\n<pre><code>from datasets import load_dataset\ndataset = load_dataset('csv', data_files='final.csv')\n</code></pre>\n<p>or to load multiple files, use:</p>\n<pre><code>dataset = load_dataset('csv', data_files={'train' ['my_train_file_1.csv', 'my_train_file_2.csv'], 'test': 'my_test_file.csv'})\n</code></pre>\n<p>For more details, follow the <a href=\"https://huggingface.co/docs/datasets/v1.11.0/loading_datasets.html\" rel=\"nofollow noreferrer\">Hugging Face documentation</a>.</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 77020278,
            "link": "https://stackoverflow.com/questions/77020278/how-to-load-a-huggingface-dataset-from-local-path"
        }
    },
    {
        "question": "HuggingFace Model - OnnxRuntime - Jupyter Notebook Print Model Summary\n<p>Thank you very much for reading my question , sorry if it is an obvious question.</p>\n<p>I use anaconda navigator : piped install the model whisper from OpenAi, which is an audio to text transformer model, I use jupyter notebook and when I just run the cell of the model, there is this summary of modules which is quite useful to get to know what the model is :</p>\n<hr />\n<p><a href=\"https://i.sstatic.net/DdN7t3O4l.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/DdN7t3O4l.png\" alt=\"enter image description here\" /></a></p>\n<hr />\n<p>However, with another pip installed model :  <a href=\"https://huggingface.co/breezedeus/pix2text-mfr\" rel=\"nofollow noreferrer\">https://huggingface.co/breezedeus/pix2text-mfr</a>\nI notice the difference is it is optimum.onnxruntime</p>\n<p><a href=\"https://i.sstatic.net/7ovUOjRe.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/7ovUOjRe.png\" alt=\"enter image description here\" /></a></p>\n<hr />\n<p>and when I do the same thing as above, it instead returns a memory location ? or is it ?</p>\n<p>sorry if it is a simple question, I tried googling a bit but dont quite know what keyword to search - &quot;onnx pytorch model summary&quot; ? is there a way to have a model summary as above ?</p>\n<p>Thank you very much for reading my question .</p>\n",
        "answer": "<p>The ORT model output is just the default string that represents an object in Python, providing the class name and then the memory address. They are both valid objects in Python but the first model overrides the <code>__str__()</code> method to show the layers when the model is printed.</p>\n<p>The ORT model doesn't have the same methods, like the <code>__str__()</code> override, as the whisper model because it is not a PyTorch-based model. Instead, it uses ONNX graph definitions and operators. You can use a tool like Netron. You can try to print the encoder and decoder graphs, but it is not very readable.</p>\n<pre class=\"lang-py prettyprint-override\"><code>import onnx\n\n# Encoder model\nencoder_onnx_model = onnx.load(model.encoder_model_path)\nprint(onnx.helper.printable_graph(encoder_onnx_model.graph))\n\n# Decoder model\ndecoder_onnx_model = onnx.load(model.decoder_model_path)\nprint(onnx.helper.printable_graph(decoder_onnx_model.graph))\n</code></pre>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 79378157,
            "link": "https://stackoverflow.com/questions/79378157/huggingface-model-onnxruntime-jupyter-notebook-print-model-summary"
        }
    },
    {
        "question": "Where is the HuggingFace model saved in when loading a model on colab?\n<p>I have this code for loading a generative model. I'm not sure how to see model files in colab (i.e., config.json etc.).</p>\n<pre><code>model_id = &quot;deepseek-ai/DeepSeek-R1-Distill-Llama-8B&quot;\n\n\npipeline = transformers.pipeline(\n            &quot;text-generation&quot;,\n            model=model_id,\n            #model_kwargs={&quot;torch_dtype&quot;: torch.bfloat16, &quot;cache_dir&quot;: cache_dir},\n            device_map=&quot;auto&quot;)\n</code></pre>\n",
        "answer": "<p>You can locate everything that was downloaded for the model via the <a href=\"https://huggingface.co/docs/huggingface_hub/package_reference/environment_variables#hfhome\" rel=\"nofollow noreferrer\">HF_HOME</a> constant:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import os\nfrom huggingface_hub.constants import HF_HOME \n\nprint(HF_HOME)\nprint(*os.listdir(f&quot;{HF_HOME}/hub&quot;), sep=&quot;\\n&quot;)\n</code></pre>\n<p>Output:</p>\n<pre><code>/root/.cache/huggingface\nmodels--EleutherAI--gpt-neo-1.3B\n.locks\nmodels--gpt2\nversion.txt\n</code></pre>\n<p>Every directory prefixed with <code>models</code> contains the respective files, but the actual file names are hashed and this solution is probably not what you are looking for. Another alternative is saving it locally again:</p>\n<pre class=\"lang-py prettyprint-override\"><code>from transformers import pipeline\n\nlocal_path = &quot;/content/my_model&quot;\ngen = pipeline(&quot;text-generation&quot;, model=&quot;EleutherAI/gpt-neo-1.3B&quot;)\ngen.save_pretrained(local_path)\nprint(*os.listdir(local_path), sep=&quot;\\n&quot;)\n</code></pre>\n<p>Output:</p>\n<pre><code>model-00002-of-00002.safetensors\nvocab.json\nmerges.txt\ntokenizer.json\nconfig.json\nmodel.safetensors.index.json\nspecial_tokens_map.json\ntokenizer_config.json\ngeneration_config.json\nmodel-00001-of-00002.safetensors\n</code></pre>\n<p>In case you are only interested in the config, you can also access it directly via the respective property:</p>\n<pre class=\"lang-py prettyprint-override\"><code>print(gen.model.config)\n</code></pre>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 79465047,
            "link": "https://stackoverflow.com/questions/79465047/where-is-the-huggingface-model-saved-in-when-loading-a-model-on-colab"
        }
    },
    {
        "question": "Load DeepSeek-V3 model from local repo\n<p>I want to run the DeepSeek-V3 model inference using the Hugging-Face Transformer library (&gt;= v4.51.0).</p>\n<p>I read that you can do the following to do that (download the model and run it)</p>\n<pre><code>from transformers import pipeline\n\nmessages = [\n{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Who are you?&quot;},\n]\npipe = pipeline(&quot;text-generation&quot;, model=&quot;deepseek-ai/DeepSeek-R1&quot;, trust_remote_code=True)\npipe(messages)\n</code></pre>\n<p>My issue is that I already downloaded the DeepSeek-V3 hugging-face repository separately, and I just want to tell the Transformer where it is on my local machine, so that it can run the inference.</p>\n<p>The model repository is thus not (or not necessarily) in the Hugging-Face cache directory (it can be anywhere on the local machine). When loading the model, I want to provide the path which specifically points to the model's repository on the local machine.</p>\n<p>How can I achieve that?</p>\n",
        "answer": "<p>Since you said you downloaded the model already from Huggingface, I assume you downloaded all of the related Huggingface files including the JSON files in the repo that describe the model for loading. In this case, the pipeline function can easily take a filesystem path in the <code>model</code> parameter instead of a model name.</p>\n<p>For example, if you downloaded the files to the folder <code>/my-models/deepseek-r1</code>, you just need to load it this way</p>\n<pre class=\"lang-py prettyprint-override\"><code>pipe = pipeline(&quot;text-generation&quot;, model=&quot;/my-models/deepseek-r1&quot;, trust_remote_code=True)\n</code></pre>\n<p>The pipeline function will try to load from the filesystem, and if not found, it will try to look for a model with that name on the Hub.</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 79569505,
            "link": "https://stackoverflow.com/questions/79569505/load-deepseek-v3-model-from-local-repo"
        }
    },
    {
        "question": "Stable Diffusion 3 does not work with diffusers\n<p>I try to use Stable Diffusion 3 on my desktop.\nBut it doesn't work.</p>\n<p>I make the test.py file, the file is mostly same as sample code on Hugging Face.\nDifference is only about authentication. The sample has no authentication statement.</p>\n<pre><code>import torch\nfrom diffusers import StableDiffusion3Pipeline\n\ntoken=&quot;MY_TOKEN&quot;;\n\npipe = StableDiffusion3Pipeline.from_pretrained(&quot;stabilityai/stable-diffusion-3-medium-diffusers&quot;, use_auth_token=token,torch_dtype=torch.float16)\npipe = pipe.to(&quot;cuda&quot;)\n\nimage = pipe(\n            &quot;A cat holding a sign that says hello world&quot;,\n            negative_prompt=&quot;&quot;,\n            num_inference_steps=28,\n            guidance_scale=7.0,\n        ).images[0]\nimage\n</code></pre>\n<p>Run time error -</p>\n<pre><code>Couldn't connect to the Hub: 401 Client Error. (Request ID: Root=1-669607ff-38d1dce957780f8d7ffc71bf;be997e6a-4dc6-40ac-9c95-35346e6c2fef)\n\nCannot access gated repo for url https://huggingface.co/api/models/stabilityai/stable-diffusion-3-medium-diffusers.\nAccess to model stabilityai/stable-diffusion-3-medium-diffusers is restricted. You must be authenticated to access it..\nWill try to load from local cache.\nKeyword arguments {'use_auth_token': 'MY_TOKEN'} are not expected by StableDiffusion3Pipeline and will be ignored.\n</code></pre>\n<p>MY_TOKEN is my hugging face access token.</p>\n<p>I ask ChatGPT then he said a kind of authentication problem. he could not resolve the problem.</p>\n<p>Please let me know what I should do.</p>\n<p>I tried to exchange 'use_auth_token' to 'auth_token', but nothing change.</p>\n",
        "answer": "<p>I used to have the exact same issue with 3.5 model. Upon some trial and error it appears that the use_auth_token argument might be already deprecated.</p>\n<p>However I managed to resolve the issue by following steps here:\n<a href=\"https://stability.ai/learning-hub/setting-up-and-using-sd3-medium-locally\" rel=\"nofollow noreferrer\">https://stability.ai/learning-hub/setting-up-and-using-sd3-medium-locally</a></p>\n<p>In particular I executed <code>huggingface-cli login</code> from the console with the same virtualenv as my jupyter runtime. It asks you to paste the token to console and confirm a few details. Upon restarting the runtime a plain command went withnout any issues:</p>\n<pre><code>pipe = diffusers.StableDiffusion3Pipeline.from_pretrained(\n    &quot;stabilityai/stable-diffusion-3.5-medium&quot;,\n    torch_dtype=torch.bfloat16)\n</code></pre>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 78753041,
            "link": "https://stackoverflow.com/questions/78753041/stable-diffusion-3-does-not-work-with-diffusers"
        }
    },
    {
        "question": "Unable to connect to hugging face model\n<pre class=\"lang-py prettyprint-override\"><code>from sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer(&quot;BAAI/bge-small-en-v1.5&quot;)\n\nsentences = [\n    &quot;The weather is lovely today.&quot;,\n    &quot;It's so sunny outside!&quot;,\n    &quot;He drove to the stadium.&quot;\n]\nembeddings = model.encode(sentences)\n\nsimilarities = model.similarity(embeddings, embeddings)\nprint(similarities.shape)\n</code></pre>\n<p>I encounter the error below:</p>\n<blockquote>\n<p>OSError: There was a specific connection error when trying to load BAAI/bge-small-en-v1.5:\n401 Client Error: Unauthorized for url: <a href=\"https://huggingface.co/BAAI/bge-small-en-v1.5/resolve/main/config.json\" rel=\"nofollow noreferrer\">https://huggingface.co/BAAI/bge-small-en-v1.5/resolve/main/config.json</a> (Request ID: Root=1-6812200a-183a853f5ac0980b4d55617d;614925fe-76d8-4851-8516-aecbb59e0514)<br />\nInvalid credentials in Authorization header</p>\n</blockquote>\n",
        "answer": "<p>I think if you download the model file and instead of giving the model name, give the path to the model file, it's going to work fine.</p>\n<pre><code>from sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer(&quot;/path-to-file/BAAI/bge-small-en-v1.5&quot;)\n\nsentences = [\n    &quot;The weather is lovely today.&quot;,\n    &quot;It's so sunny outside!&quot;,\n    &quot;He drove to the stadium.&quot;\n]\nembeddings = model.encode(sentences)\n\nsimilarities = model.similarity(embeddings, embeddings)\nprint(similarities.shape)\n</code></pre>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 79600374,
            "link": "https://stackoverflow.com/questions/79600374/unable-to-connect-to-hugging-face-model"
        }
    },
    {
        "question": "Cannot load a gated model from hugginface despite having access and logging in\n<p>I am training a Llama-3.1-8B-Instruct model for a specific task.\nI have request the access to the huggingface repository, and got access, confirmed on the huggingface webapp dashboard.</p>\n<p>I tried calling the <code>huggingface_hub.login</code> function with the token to login and then download the model in the same script. I get an error, saying that I need to be logged in to access gated repositories.</p>\n<p>Then I tried loging in via the <code>huggingface-cli login</code> command, which succeeded. I got the same error after running the script.</p>\n<p>Then I tried the first approach again, but didn't pass the token, the documentation says I should get prompter for the token. The login function however seems to block after showing the HF logo, but does not show a prompt for the token.</p>\n<p>Is there something I'm missing here in order to access the models?</p>\n<p>My code:</p>\n<pre><code>hf_login()\n\nbase_model_name = 'meta-llama/Llama-3.1-8B-Instruct'\ntokenizer = AutoTokenizer.from_pretrained(base_model_name)  # this line causes error\nmodel = AutoModelForCausalLM.from_pretrained(base_model_name)\n</code></pre>\n<p>Error:</p>\n<pre><code>OSError: You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct.\n401 Client Error. (Request ID: Root=1-673f47aa-6b11aae44cd9c6523654070c;5816d1af-49a5-4262-bec0-dab7ecad66e4)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.\n</code></pre>\n<p>I'm sure I have access to the meta-llama/Llama-3.1 models. The <code>huggingface-cli whoami</code> command correctly returns my username, so I'm also logged in.</p>\n<p>My token is set to read access, I'm also trying with a write access one.</p>\n<p>EDIT: I generated a new write-access token. The login via the function <code>huggingface_hub.login</code> was successful. The models still weren't actually downloading. I tried from the windows terminal instead of the pycharm built-in terminal, and now it is working. Still don't know why it works now.</p>\n",
        "answer": "<p>You need to provide a token. The one of the way is to share it as environment variable. Before you launch python or ipython or jupyter notebook please do this:</p>\n<pre><code>HF_TOKEN=&lt;token acquired from Access Tokens in your huggingface profile&gt;\n</code></pre>\n<p>Then the code you execute is following:</p>\n<pre><code>import os\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ntoken = os.enviro[&quot;HF_TOKEN&quot;]\n \nbase_model_name = 'meta-llama/Llama-3.1-8B-Instruct'\ntokenizer = AutoTokenizer.from_pretrained(base_model_name, token=token)\nmodel = AutoModelForCausalLM.from_pretrained(base_model_name, token=token)\n</code></pre>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 79211723,
            "link": "https://stackoverflow.com/questions/79211723/cannot-load-a-gated-model-from-hugginface-despite-having-access-and-logging-in"
        }
    },
    {
        "question": "Target modules for applying PEFT / LoRA on different models\n<p>I am looking at a few <a href=\"https://colab.research.google.com/drive/1BiQiw31DT7-cDp1-0ySXvvhzqomTdI-o#scrollTo=NuAx3zBeUL1q\" rel=\"noreferrer\">different</a> <a href=\"https://www.philschmid.de/fine-tune-flan-t5-peft\" rel=\"noreferrer\">examples</a> of using PEFT on different models. The <code>LoraConfig</code> object contains a <code>target_modules</code> array. In some examples, the target modules are <code>[&quot;query_key_value&quot;]</code>, sometimes it is <code>[&quot;q&quot;, &quot;v&quot;]</code>, sometimes something else.</p>\n<p>I don't quite understand where the values of the target modules come from. Where in the model page should I look to know what the LoRA adaptable modules are?</p>\n<p>One example (for the model Falcon 7B):</p>\n<pre><code>peft_config = LoraConfig(\n    lora_alpha=lora_alpha,\n    lora_dropout=lora_dropout,\n    r=lora_r,\n    bias=&quot;none&quot;,\n    task_type=&quot;CAUSAL_LM&quot;,\n    target_modules=[\n        &quot;query_key_value&quot;,\n        &quot;dense&quot;,\n        &quot;dense_h_to_4h&quot;,\n        &quot;dense_4h_to_h&quot;,\n    ]\n</code></pre>\n<p>Another example (for the model Opt-6.7B):</p>\n<pre><code>config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[&quot;q_proj&quot;, &quot;v_proj&quot;],\n    lora_dropout=0.05,\n    bias=&quot;none&quot;,\n    task_type=&quot;CAUSAL_LM&quot;\n)\n</code></pre>\n<p>Yet another (for the model Flan-T5-xxl):</p>\n<pre><code>lora_config = LoraConfig(\n r=16,\n lora_alpha=32,\n target_modules=[&quot;q&quot;, &quot;v&quot;],\n lora_dropout=0.05,\n bias=&quot;none&quot;,\n task_type=TaskType.SEQ_2_SEQ_LM\n)\n</code></pre>\n",
        "answer": "<p>Here method to get all linear.</p>\n<pre><code>import bitsandbytes as bnb\n\ndef find_all_linear_names(model):\n    lora_module_names = set()\n    for name, module in model.named_modules():\n        if isinstance(module, bnb.nn.Linear4bit):\n            names = name.split(&quot;.&quot;)\n            # model-specific\n            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n\n    if &quot;lm_head&quot; in lora_module_names:  # needed for 16-bit\n        lora_module_names.remove(&quot;lm_head&quot;)\n    return list(lora_module_names)\n</code></pre>\n<p>In a future release you can directly use <code>target_modules=&quot;all-linear&quot;</code> in your LoraConfig</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 76768226,
            "link": "https://stackoverflow.com/questions/76768226/target-modules-for-applying-peft-lora-on-different-models"
        }
    },
    {
        "question": "SSLError: HTTPSConnectionPool(host=&#39;huggingface.co&#39;, port=443): Max retries exceeded with url: /dslim/bert-base-NER/resolve/main/tokenizer_config.json\n<p>I am facing below issue while loading the pretrained BERT model from HuggingFace due to SSL certificate error.</p>\n<h2>Error:</h2>\n<blockquote>\n<p>SSLError: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /dslim/bert-base-NER/resolve/main/tokenizer_config.json (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self signed certificate in certificate chain (_ssl.c:1108)')))</p>\n</blockquote>\n<h2>The line that is causing the issue is:</h2>\n<pre><code>tokenizer = AutoTokenizer.from_pretrained(&quot;dslim/bert-base-NER&quot;)\n</code></pre>\n<h2>Source code:</h2>\n<pre><code>from transformers import AutoTokenizer, AutoModelForTokenClassification\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(&quot;dslim/bert-base-NER&quot;)\nmodel = AutoModelForTokenClassification.from_pretrained(&quot;dslim/bert-base-NER&quot;)\n</code></pre>\n<p>I am expecting to download pre-trained models while running the code in jupyter lab on Windows.</p>\n",
        "answer": "<p>Here is a solution if you want the actual certificate:</p>\n<p>If you are on linux you can use this bash script I made to download the certificate file from Cisco Umberella, convert it to .crt and update the certificates folder. Script:</p>\n<pre><code>#!/bin/sh\n\ncurl -O https://d36u8deuxga9bo.cloudfront.net/certificates/Cisco_Umbrella_Root_CA.cer\nopenssl x509 -inform PEM -in Cisco_Umbrella_Root_CA.cer -out cisco.crt\nrm -rf Cisco_Umbrella_Root_CA.cer\nsudo mv cisco.crt /usr/local/share/ca-certificates/\nsudo update-ca-certificates\n</code></pre>\n<p>After adding the new certificate from Cisco you still need to set the environment variable <code>CURL_CA_BUNDLE</code> before using the <code>from_pretrained</code> function. This is because the certifi package used by transformers for ca-certification defaults to some .pem file inside the certifi package. We instead set it to the directory your downloaded certificate was just moved to by <code>update-ca-certificates</code>. Here is how:</p>\n<pre><code>import os\nfrom transformers import AutoModel, AutoTokenizer\n\nos.environ[&quot;CURL_CA_BUNDLE&quot;] = &quot;/etc/ssl/certs/ca-certificates.crt&quot;\nmodel = AutoModel.from_pretrained(&quot;intfloat/multilingual-e5-small&quot;)\ntokenizer = AutoTokenizer.from_pretrained(&quot;intfloat/multilingual-e5-small&quot;)\n</code></pre>\n<p>Now it should download the model without any security warnings. Hope this helped.</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 75110981,
            "link": "https://stackoverflow.com/questions/75110981/sslerror-httpsconnectionpoolhost-huggingface-co-port-443-max-retries-exce"
        }
    },
    {
        "question": "Validation and Training Loss when using HuggingFace\n<p>I do not seem to find an explanation on how the validation and training losses are calculated when we finetune a model using the huggingFace trainer. Does anyone know here to find this information?</p>\n",
        "answer": "<h1>In Short</h1>\n<p>Depends on what you want to do with the evaluation function, knowing the internal workings of the evaluation might or might not be practical for you to train the model appropriately.</p>\n<p>Scroll down to the <code>Summary</code> section of the answer and the QnA section after.</p>\n<hr />\n<h1>In Long</h1>\n<p>There are two common mode for training a model with Huggingface <code>transformers</code>,</p>\n<ol>\n<li>with the <code>Trainer</code> (batteries included)</li>\n<li>without the trainer and default Pytorch backpropagation</li>\n</ol>\n<p>For example:</p>\n<ol>\n<li><a href=\"https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_clm.py\" rel=\"noreferrer\">https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_clm.py</a></li>\n<li><a href=\"https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_clm_no_trainer.py\" rel=\"noreferrer\">https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_clm_no_trainer.py</a></li>\n</ol>\n<p>For (2), it should be self-explanatory as the evaluation/validation routine is explicitly coded out (other than the magical <code>loss.backwords</code> and <code>optimizer.step</code>)</p>\n<h2>Q: Where is the validation routine in the <code>Trainer</code> object?</h2>\n<p>For (1), it is rather hard to find any blogpost or detailed doc on how the <code>Trainer</code> object works but you can take a look at the source code, so lets go down the rabbit hole...</p>\n<p>In the <code>Trainer</code> object, there is an <code>evaluate()</code> function that runs the evaluation/validation routine, <a href=\"https://github.com/huggingface/transformers/blob/main/src/transformers/trainer.py#L2925\" rel=\"noreferrer\">https://github.com/huggingface/transformers/blob/main/src/transformers/trainer.py#L2925</a></p>\n<h2>How/When is the <code>evaluate()</code> called?</h2>\n<p>When you call <code>trainer.train()</code>, there's a lot of things happening but in general it's doing:</p>\n<pre><code>  def train(\n        self,\n        resume_from_checkpoint: Optional[Union[str, bool]] = None,\n        trial: Union[&quot;optuna.Trial&quot;, Dict[str, Any]] = None,\n        ignore_keys_for_eval: Optional[List[str]] = None,\n        **kwargs,\n    ):\n        # blah blah, argparsing and reading kwargs\n        # then do a lot more model/args munging to make check \n        # if you want to load a model or create a new one from config\n\n        # Then finally the most important thing:\n\n            return inner_training_loop(\n                args=args,\n                resume_from_checkpoint=resume_from_checkpoint,\n                trial=trial,\n                ignore_keys_for_eval=ignore_keys_for_eval,\n            )\n</code></pre>\n<h3>Hmmmm, oh okay, <code>trainer.train()</code> calls <code>inner_training_loop()</code></h3>\n<p>And inside the <code>inner_training_loop()</code>, <a href=\"https://github.com/huggingface/transformers/blob/main/src/transformers/trainer.py#L1552\" rel=\"noreferrer\">https://github.com/huggingface/transformers/blob/main/src/transformers/trainer.py#L1552</a>, there's like a 400-500 lines of code that eventually:</p>\n<pre><code>    def inner_training_loop(...):\n        # Lots of code parsing args and checking stuff.\n\n        # Then the training part of the code, that is out-of-scope\n        # for this question but eventually, it does\n        ... \n        self.optimizer.step()\n\n        ...\n        # Then we see this after the gradients are computed\n        # and model updated with optimizer.step()\n         self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n\n        # Iterate through the training + evaluate/validation\n        # loop, until eventually the trainer.train() returns\n        ...\n        return TrainOutput(self.state.global_step, train_loss, metrics)\n</code></pre>\n<h3>Hmmmm, oh okay, <code>trainer.train()</code> calls <code>inner_training_loop()</code>, that calls <code>_maybe_log_save_evaluate()</code></h3>\n<p>And inside the <code>_maybe_log_save_evaluate()</code>, that's when you see the validation dataset gets accessed:</p>\n<pre><code>\n    def _maybe_log_save_evaluate(self, tr_loss, model, trial, epoch, ignore_keys_for_eval):\n        # Somehow, we have to respect the user and check if the user\n        # wants to log the metircs...\n        if self.control.should_log:\n            # Some log parsing for the loss,\n            # emits to somewhere code... Not that we care here =) \n            ...\n    \n        ...\n        # Then comes the part that we want to know, \n        # the actual evaluation.\n        if self.control.should_evaluate:\n            if isinstance(self.eval_dataset, dict):\n                metrics = {}\n                for eval_dataset_name, eval_dataset in self.eval_dataset.items():\n                    dataset_metrics = self.evaluate(\n                        eval_dataset=eval_dataset,\n                        ignore_keys=ignore_keys_for_eval,\n                        metric_key_prefix=f&quot;eval_{eval_dataset_name}&quot;,\n                    )\n                    metrics.update(dataset_metrics)\n            else:\n                metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)\n            self._report_to_hp_search(trial, self.state.global_step, metrics)\n    \n            # Run delayed LR scheduler now that metrics are populated\n            if isinstance(self.lr_scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n                metric_to_check = self.args.metric_for_best_model\n                if not metric_to_check.startswith(&quot;eval_&quot;):\n                    metric_to_check = f&quot;eval_{metric_to_check}&quot;\n                self.lr_scheduler.step(metrics[metric_to_check])\n\n\n        # Then check more stuff to see if user wants \n        # to save the model before exiting the function.\n        if self.control.should_save:\n            ...\n</code></pre>\n<p>Note: The <code>_maybe_log_save_evaluate()</code> calls <code>evaluate()</code>  at this line:</p>\n<pre><code>self.evaluate(eval_dataset=eval_dataset,\n    ignore_keys=ignore_keys_for_eval,\n    metric_key_prefix=f&quot;eval_{eval_dataset_name}&quot;,\n)\n</code></pre>\n<h3>So, the <code>trainer.train()</code> calls <code>inner_training_loop()</code>, that calls <code>_maybe_log_save_evaluate()</code>, that calls <code>evaluate()</code>.</h3>\n<p>Then, we have calling <code>evaluate()</code> calling <code>evaluation_loop()</code>:</p>\n<pre><code>    def evaluate(\n        self,\n        eval_dataset: Optional[Dataset] = None,\n        ignore_keys: Optional[List[str]] = None,\n        metric_key_prefix: str = &quot;eval&quot;,\n    ) -&gt; Dict[str, float]:\n        ...\n        # First, the function runs the forward pass through the\n        # prediction_loop\n\n        eval_loop = self.prediction_loop if self.args.use_legacy_prediction_loop else self.evaluation_loop\n\n        output = eval_loop(\n            eval_dataloader,\n            description=&quot;Evaluation&quot;,\n            # No point gathering the predictions if there are no metrics, otherwise we defer to\n            # self.args.prediction_loss_only\n            prediction_loss_only=True if self.compute_metrics is None else None,\n            ignore_keys=ignore_keys,\n            metric_key_prefix=metric_key_prefix,\n        )\n\n        total_batch_size = self.args.eval_batch_size * self.args.world_size\n        if f&quot;{metric_key_prefix}_jit_compilation_time&quot; in output.metrics:\n            start_time += output.metrics[f&quot;{metric_key_prefix}_jit_compilation_time&quot;]\n        output.metrics.update(\n            speed_metrics(\n                metric_key_prefix,\n                start_time,\n                num_samples=output.num_samples,\n                num_steps=math.ceil(output.num_samples / total_batch_size),\n            )\n        )\n\n        self.log(output.metrics)\n        ...\n\n        return output.metrics\n</code></pre>\n<p>Then inside the <a href=\"https://github.com/huggingface/transformers/blob/main/src/transformers/trainer.py#L3058\" rel=\"noreferrer\">evaluation_loop</a>, that is where eventually you see the</p>\n<pre><code>    def evaluation_loop(\n        self,\n        dataloader: DataLoader,\n        description: str,\n        prediction_loss_only: Optional[bool] = None,\n        ignore_keys: Optional[List[str]] = None,\n        metric_key_prefix: str = &quot;eval&quot;,\n    ) -&gt; EvalLoopOutput:\n        # Do lots of work parsing and optimizing with accelerate and GPUs\n        ...\n\n        # Then the meat of the evaluation process:\n        # Main evaluation loop\n        for step, inputs in enumerate(dataloader):\n            # Update the observed num examples\n            observed_batch_size = find_batch_size(inputs)\n            if observed_batch_size is not None:\n                observed_num_examples += observed_batch_size\n                # For batch samplers, batch_size is not known by the dataloader in advance.\n                if batch_size is None:\n                    batch_size = observed_batch_size\n\n            # Prediction step\n            loss, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)\n            main_input_name = getattr(self.model, &quot;main_input_name&quot;, &quot;input_ids&quot;)\n            inputs_decode = self._prepare_input(inputs[main_input_name]) if args.include_inputs_for_metrics else None\n\n\n        # Then lots of code parsing the different prediction outputs of the different models supported by Huggingface\n        ...\n\n        # And eventually emitting and returning the metrics numbers\n        \n        if self.compute_metrics is not None and all_preds is not None and all_labels is not None:\n            if args.include_inputs_for_metrics:\n                metrics = self.compute_metrics(\n                    EvalPrediction(predictions=all_preds, label_ids=all_labels, inputs=all_inputs)\n                )\n            else:\n                metrics = self.compute_metrics(EvalPrediction(predictions=all_preds, label_ids=all_labels))\n        else:\n            metrics = {}\n\n         # Then some more code, parsing the metrics outputs\n         ...\n     \n     # Finally, return the outputs.\n     return EvalLoopOutput(predictions=all_preds, label_ids=all_labels, metrics=metrics, num_samples=num_samples)\n\n</code></pre>\n<h1>Summary</h1>\n<p>With the <code>Trainer</code> object there are a lot of code written to support different user modes, different trainer arguments and different models and evaluation routines.</p>\n<p>In short, the <code>trainer.train()</code></p>\n<ul>\n<li><code>trainer.train()</code> calls <code>inner_training_loop()</code></li>\n<li><code>inner_training_loop()</code> calls <code>_maybe_log_save_evaluate()</code>,</li>\n<li><code>_maybe_log_save_evaluate()</code> that calls <code>evaluate()</code>,</li>\n<li><code>evaluate()</code> eventually calls the <code>evaluation_loop()</code> function that does the metric computation</li>\n</ul>\n<h1>Q: If I want to customize the validation routine, should I change the Trainer object and the <code>evaluate()</code> function?</h1>\n<p>You can try overloading the Trainer object's <code>evaluate</code> function if you want to.</p>\n<h3>A: Try a custom <code>compute_metric</code> function</h3>\n<p>But since the object is loaded to support generic use, if you want customized validation loops, first try changing how the <code>compute_metric</code> works (most probably your task is a common one supported, so it's easy) e.g. <a href=\"https://www.kaggle.com/code/alvations/how-to-fine-tune-an-opus-mt-model/#The-Metric:-Lets-go-with-the-classic-BLEU-and-ChrF\" rel=\"noreferrer\">https://www.kaggle.com/code/alvations/how-to-fine-tune-an-opus-mt-model/#The-Metric:-Lets-go-with-the-classic-BLEU-and-ChrF</a></p>\n<h3>A: Try using <code>TrainerCallback</code></h3>\n<p>Or you can try <a href=\"https://huggingface.co/docs/transformers/main_classes/callback\" rel=\"noreferrer\">https://huggingface.co/docs/transformers/main_classes/callback</a> (take a look at <a href=\"https://oongjoon.github.io/huggingface/Trainer-Callback_en/\" rel=\"noreferrer\">https://oongjoon.github.io/huggingface/Trainer-Callback_en/</a>, it's a little old but worth the read)</p>\n<h3>A: Train without the <code>Trainer</code> object</h3>\n<p>And if you really need the forward pass to the model to be different and/or the outputs of the forward pass differently, then it might be easier to not use the <code>Trainer</code> and roll your own in Pytorch <code>blah blah; loss.backwards(); optimizer.step()</code> , e.g. <a href=\"https://github.com/huggingface/transformers/blob/main/examples/pytorch/translation/run_translation_no_trainer.py\" rel=\"noreferrer\">https://github.com/huggingface/transformers/blob/main/examples/pytorch/translation/run_translation_no_trainer.py</a></p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 76914119,
            "link": "https://stackoverflow.com/questions/76914119/validation-and-training-loss-when-using-huggingface"
        }
    },
    {
        "question": "Unable to install Python package llama-index-embeddings-huggingface\n<p>I am trying to implement the code discussed in the HuggingFace cookbook article at <a href=\"https://huggingface.co/learn/cookbook/en/rag_llamaindex_librarian\" rel=\"nofollow noreferrer\">https://huggingface.co/learn/cookbook/en/rag_llamaindex_librarian</a>.</p>\n<p>I get to the point where I am trying to install the package in question:</p>\n<pre><code>llama-index-embeddings-huggingface\n</code></pre>\n<p>I get the following error:</p>\n<pre><code>ERROR: Cannot install llama-index-embeddings-huggingface==0.1.3, llama-index-embeddings-huggingface==0.1.4 and llama-index-embeddings-huggingface==0.1.5 because these package versions have conflicting dependencies.\n</code></pre>\n<p>The conflict is caused by:</p>\n<pre><code>llama-index-embeddings-huggingface 0.1.5 depends on torch&lt;3.0.0 and &gt;=2.1.2\nllama-index-embeddings-huggingface 0.1.4 depends on torch&lt;3.0.0 and &gt;=2.1.2\nllama-index-embeddings-huggingface 0.1.3 depends on torch&lt;3.0.0 and &gt;=2.1.2\n</code></pre>\n<p>I assume that torch is PyTorch, which is not currently installed.</p>\n<p>I am using the command:</p>\n<pre><code>python3 -m pip install llama-index-embeddings-huggingface\n</code></pre>\n<p>which causes the mentioned error.</p>\n<p>I am not sure what to try to resolve the conflict.</p>\n",
        "answer": "<p>Which version of python are you using? I had the same issue while attempting to install <code>llama-index-embeddings-huggingface</code> on python 3.13.0. I downgraded to python 3.12.8 and it worked.</p>\n<p>If you aren't committed to a specific version of python, try it again on 3.12.8. You can use pyenv: <a href=\"https://github.com/pyenv/pyenv\" rel=\"nofollow noreferrer\">https://github.com/pyenv/pyenv</a> to manage multiple python installations on your machine.</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 79222221,
            "link": "https://stackoverflow.com/questions/79222221/unable-to-install-python-package-llama-index-embeddings-huggingface"
        }
    },
    {
        "question": "Use HuggingFace models locally\n<p>I would like to use transformers especially HuggingFace Models as a part of my programming</p>\n<p>my question is; Can I use and implement transformers and HuggingFace Models offline and in Spyder IDE (or any other IDE that I can use locally? (Of course, after downloading and installing all needed packages).</p>\n<p>Thanks in advance.</p>\n",
        "answer": "<p>Yes, Once you use a model once, it gets downloaded into your cache directory, meaning that next time you call the model even without internet connection it can be loaded as well.</p>\n<p>your default cache directory is <code>~\\.cache\\huggingface\\hub</code></p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 77905301,
            "link": "https://stackoverflow.com/questions/77905301/use-huggingface-models-locally"
        }
    },
    {
        "question": "How to configure HuggingFaceEndpoint in Langchain\n<p>I'm trying to use this model</p>\n<pre><code>from langchain_huggingface import HuggingFaceEndpoint\nrepo_id=&quot;google/flan-t5-large&quot;\nhuggingface_llm = HuggingFaceEndpoint(\nhuggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN,\nrepo_id=repo_id,\ntemperature=0,\nmax_new_tokens=200)\n\nfrom langchain.prompts import PromptTemplate\ndef flan_process(tema, pregunta):\ntemplate = &quot;Eres un experto asistente en {tema}. Responde a la siguiente pregunta: {pregunta}&quot;\nprompt=PromptTemplate(template=template,input_variables=[&quot;tema&quot;,&quot;pregunta&quot;])\n\nflan_chain = prompt | huggingface_llm\n\nrespuesta=flan_chain.invoke({&quot;tema&quot;:tema, &quot;pregunta&quot;:pregunta})\n\nreturn respuesta\n\ntema=input(&quot;Ingrese el tema: &quot;)\npregunta=input(&quot;Ingrese la pregunta: &quot;)\n\nflan_reply=flan_process(tema, pregunta)\nprint(f&quot;Respuesta Flan: {flan_reply}&quot;)\n</code></pre>\n<p>But I always get this error The following <code>model_kwargs</code> are not used by the model: ['return_full_text', 'watermark', 'stop_sequences', 'stop'] (note: typos in the generate arguments will also show up in this list)</p>\n<p>Any idea please?</p>\n<p>Thanks</p>\n",
        "answer": "<p><code>HuggingFaceEndpoint</code> used to work with <code>google/flan-t5-large</code> but not any more. In case, you're still looking for a free model to try, here's what I'm using</p>\n<pre class=\"lang-py prettyprint-override\"><code>from langchain_huggingface import HuggingFaceEndpoint\nfrom langchain.chains import LLMChain\nrepo_id = &quot;mistralai/Mistral-7B-Instruct-v0.2&quot;\n# repo_id = &quot;meta-llama/Llama-3.2-1B&quot;\nllm = HuggingFaceEndpoint(\n    repo_id = repo_id,\n    temperature = 0.5,\n    huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN,\n    max_new_tokens = 250,\n)\n</code></pre>\n<p>Mistral-7B and Llama3.2 are quite nice models and free for access through API</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 78617530,
            "link": "https://stackoverflow.com/questions/78617530/how-to-configure-huggingfaceendpoint-in-langchain"
        }
    },
    {
        "question": "How to Load a 4-bit Quantized VLM Model from Hugging Face with Transformers?\n<p>I’m new to quantization and working with visual language models (VLM).I’m trying to load a 4-bit quantized version of the Ovis1.6-Gemma model from Hugging Face using the transformers library. I downloaded the model from this link: <a href=\"https://huggingface.co/ThetaCursed/Ovis1.6-Gemma2-9B-bnb-4bit\" rel=\"nofollow noreferrer\">https://huggingface.co/ThetaCursed/Ovis1.6-Gemma2-9B-bnb-4bit</a>.</p>\n<p>Here’s the code I’m using to load the model:</p>\n<pre class=\"lang-py prettyprint-override\"><code>from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n\n# Define the quantization configuration\nkwargs = {\n    &quot;quantization_config&quot;: BitsAndBytesConfig(\n        load_in_4bit=True,\n        load_in_8bit=False,\n        bnb_4bit_compute_dtype=&quot;float32&quot;,\n        bnb_4bit_quant_storage=&quot;uint8&quot;,\n        bnb_4bit_quant_type=&quot;fp4&quot;,\n        bnb_4bit_use_double_quant=False,\n        llm_int8_enable_fp32_cpu_offload=False,\n        llm_int8_has_fp16_weight=False,\n        llm_int8_skip_modules=None,\n        llm_int8_threshold=6.0\n    )\n}\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    &quot;ThetaCursed/Ovis1.6-Gemma2-9B-bnb-4bit&quot;,\n    trust_remote_code=True,\n    **kwargs\n).cuda()\n</code></pre>\n<p>However, I am encountering the following warnings:</p>\n<pre class=\"lang-bash prettyprint-override\"><code>warnings.warn(_BETA_TRANSFORMS_WARNING)\nUnused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method'].\nLoading checkpoint shards: 100%|██████████| 2/2 [00:06&lt;00:00,  3.06s/it]\nYou shouldn't move a model that is dispatched using accelerate hooks.\n</code></pre>\n<p>Additionally, when I try to access the tokenizers:</p>\n<pre class=\"lang-py prettyprint-override\"><code>text_tokenizer = model.get_text_tokenizer()\nvisual_tokenizer = model.get_visual_tokenizer()\n</code></pre>\n<p>I get the following error:</p>\n<pre class=\"lang-bash prettyprint-override\"><code>AttributeError: 'NoneType' object has no attribute 'get_text_tokenizer'\n</code></pre>\n<p>How can I properly load the 4-bit quantized model without encountering these warnings?\nWhy am I receiving an AttributeError when trying to access the tokenizers? Does this model not support them?</p>\n",
        "answer": "<p>I could not reproduce your issue, but I could load the model with the same code you are using with a few changes.</p>\n<p>The quantization config is not needed here as it is already in the model config (check the config.json file in the link you mentioned).</p>\n<p>Just make sure you have the latest hugging-face <code>transformers</code> library installed. Also, set the <code>low_cpu_mem_usage</code> parameter to <code>True</code>.</p>\n<pre class=\"lang-bash prettyprint-override\"><code>pip install -U transformers\n</code></pre>\n<p>Modified code:</p>\n<pre class=\"lang-py prettyprint-override\"><code>from transformers import AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    &quot;ThetaCursed/Ovis1.6-Gemma2-9B-bnb-4bit&quot;,\n    trust_remote_code=True,\n    low_cpu_mem_usage=True\n).cuda()\n</code></pre>\n<p>Edit:\nI could also access the text and visual tokenizers without any issues.</p>\n<p>These are the versions of the dependencies required on colab:</p>\n<pre><code>bitsandbytes                       0.44.1\nsafetensors                        0.4.5\ntokenizers                         0.20.1\ntorch                              2.5.0+cu121\ntorchaudio                         2.5.0+cu121\ntorchsummary                       1.5.1\ntorchvision                        0.20.0+cu121\ntransformers                       4.46.0\n</code></pre>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 79130264,
            "link": "https://stackoverflow.com/questions/79130264/how-to-load-a-4-bit-quantized-vlm-model-from-hugging-face-with-transformers"
        }
    },
    {
        "question": "Why does HuggingFace-provided Deepseek code result in an &#39;Unknown quantization type&#39; error?\n<p>I am using this code from huggingface:</p>\n<p>This code is directly pasted from the <a href=\"https://huggingface.co/deepseek-ai/DeepSeek-R1\" rel=\"nofollow noreferrer\">HuggingFace website's page on deepseek</a> and is supposed to be plug-and-play code:</p>\n<blockquote>\n<pre class=\"lang-py prettyprint-override\"><code>from transformers import pipeline\n\nmessages = [\n{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Who are you?&quot;},\n]\npipe = pipeline(&quot;text-generation&quot;, model=&quot;deepseek-ai/DeepSeek-R1&quot;, &gt;trust_remote_code=True)\npipe(messages)\n</code></pre>\n</blockquote>\n<p>But I'm unable to load the model. When I do, I get this issue:</p>\n<pre><code>File &quot;&lt;...&gt;/site-packages/transformers/quantizers/auto.py&quot;, line 97, in from_dict\n\nraise ValueError(\n\nValueError: Unknown quantization type, got fp8 - supported types are: \n['awq', 'bitsandbytes_4bit', 'bitsandbytes_8bit', 'gptq', 'aqlm', 'quanto', 'eetq', \n'hqq', 'compressed-tensors', 'fbgemm_fp8', 'torchao', 'bitnet']\n</code></pre>\n<p>I tried different code:</p>\n<pre><code>import torch\ngenerate_text = pipeline(model=&quot;deepseek-ai/DeepSeek-R1&quot;,torch_dtype=torch.bfloat16, trust_remote_code=True, device_map=&quot;auto&quot;)\ngenerate_text(messages)\n</code></pre>\n<p>This gives the following error:</p>\n<blockquote>\n<p>raise ValueError( ValueError: Unknown quantization type, got fp8 - supported types are: ['awq', 'bitsandbytes_4bit', 'bitsandbytes_8bit', 'gptq', 'aqlm', 'quanto', 'eetq', 'higgs', 'hqq', 'compressed-tensors', 'fbgemm_fp8', 'torchao', 'bitnet', 'vptq']</p>\n</blockquote>\n<p>What can I do?</p>\n",
        "answer": "<p>The code you should use is:</p>\n<pre><code>from huggingface_hub import InferenceClient\n\nclient = InferenceClient(\n    provider=&quot;nebius&quot;,\n    api_key=&quot;hf_xxxxxxxxxxxxxxxxxxxxxxxx&quot;\n)\n\nmessages = [\n    {\n        &quot;role&quot;: &quot;user&quot;,\n        &quot;content&quot;: &quot;What is the capital of France?&quot;\n    }\n]\n\ncompletion = client.chat.completions.create(\n    model=&quot;deepseek-ai/DeepSeek-V3&quot;, \n    messages=messages, \n    max_tokens=500,\n)\n\nprint(completion.choices[0].message)\n</code></pre>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 79424312,
            "link": "https://stackoverflow.com/questions/79424312/why-does-huggingface-provided-deepseek-code-result-in-an-unknown-quantization-t"
        }
    },
    {
        "question": "Finding config.json for Llama 3.1 8B\n<p>I installed the Llama 3.1 8B model through Meta's <a href=\"https://github.com/meta-llama/llama-models\" rel=\"nofollow noreferrer\">Github page</a>, but I can't get their example code to work. I'm running the following code in the same directory as the Meta-Llama-3.1-8B folder:</p>\n<pre><code>import transformers\nimport torch\n\npipeline = transformers.pipeline(\n  &quot;text-generation&quot;,\n  model=&quot;Meta-Llama-3.1-8B&quot;,\n  model_kwargs={&quot;torch_dtype&quot;: torch.bfloat16},\n  device=&quot;cuda&quot;\n)\n</code></pre>\n<p>The error is</p>\n<pre><code>OSError: Meta-Llama-3.1-8B does not appear to have a file named config.json\n</code></pre>\n<p>Where can I get <code>config.json</code>?</p>\n<p>I've installed the latest <code>transformers</code> module, and I understand that I can access the remote model on HuggingFace. But I'd rather use my local model. Is this possible?</p>\n",
        "answer": "<p>The issue isn't on your end. The confusion arises from Meta not clearly distinguishing between the distributions via Hugging Face and download.sh.</p>\n<p>To resolve this, you can download the model files using the Hugging Face CLI:</p>\n<pre><code>!huggingface-cli download meta-llama/Meta-Llama-3-8B-Instruct --local-dir meta-llama/Meta-Llama-3-8B-Instruct\n</code></pre>\n<p>This method will provide you with the config.json and tokenizer.json files.</p>\n<p>Additionally, you can try downloading other versions manually. For instance, someone shared a link to the configuration file on Hugging Face:</p>\n<p><a href=\"https://huggingface.co/unsloth/llama-3-8b/blob/main/config.json\" rel=\"nofollow noreferrer\">llama-3-8b/config.json</a></p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 78828715,
            "link": "https://stackoverflow.com/questions/78828715/finding-config-json-for-llama-3-1-8b"
        }
    },
    {
        "question": "Target modules for applying PEFT / LoRA on different models\n<p>I am looking at a few <a href=\"https://colab.research.google.com/drive/1BiQiw31DT7-cDp1-0ySXvvhzqomTdI-o#scrollTo=NuAx3zBeUL1q\" rel=\"noreferrer\">different</a> <a href=\"https://www.philschmid.de/fine-tune-flan-t5-peft\" rel=\"noreferrer\">examples</a> of using PEFT on different models. The <code>LoraConfig</code> object contains a <code>target_modules</code> array. In some examples, the target modules are <code>[&quot;query_key_value&quot;]</code>, sometimes it is <code>[&quot;q&quot;, &quot;v&quot;]</code>, sometimes something else.</p>\n<p>I don't quite understand where the values of the target modules come from. Where in the model page should I look to know what the LoRA adaptable modules are?</p>\n<p>One example (for the model Falcon 7B):</p>\n<pre><code>peft_config = LoraConfig(\n    lora_alpha=lora_alpha,\n    lora_dropout=lora_dropout,\n    r=lora_r,\n    bias=&quot;none&quot;,\n    task_type=&quot;CAUSAL_LM&quot;,\n    target_modules=[\n        &quot;query_key_value&quot;,\n        &quot;dense&quot;,\n        &quot;dense_h_to_4h&quot;,\n        &quot;dense_4h_to_h&quot;,\n    ]\n</code></pre>\n<p>Another example (for the model Opt-6.7B):</p>\n<pre><code>config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[&quot;q_proj&quot;, &quot;v_proj&quot;],\n    lora_dropout=0.05,\n    bias=&quot;none&quot;,\n    task_type=&quot;CAUSAL_LM&quot;\n)\n</code></pre>\n<p>Yet another (for the model Flan-T5-xxl):</p>\n<pre><code>lora_config = LoraConfig(\n r=16,\n lora_alpha=32,\n target_modules=[&quot;q&quot;, &quot;v&quot;],\n lora_dropout=0.05,\n bias=&quot;none&quot;,\n task_type=TaskType.SEQ_2_SEQ_LM\n)\n</code></pre>\n",
        "answer": "<p>Let's say that you load some model of your choice:</p>\n<p><code>model = AutoModelForCausalLM.from_pretrained(&quot;some-model-checkpoint&quot;)</code></p>\n<p>Then you can see available modules by printing out this model:</p>\n<p><code>print(model)</code></p>\n<p>You will get something like this (SalesForce/CodeGen25):</p>\n<pre><code>LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(51200, 4096, padding_idx=0)\n    (layers): ModuleList(\n      (0-31): 32 x LlamaDecoderLayer(\n        (self_attn): LlamaAttention(\n          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n          (act_fn): SiLUActivation()\n        )\n        (input_layernorm): LlamaRMSNorm()\n        (post_attention_layernorm): LlamaRMSNorm()\n      )\n    )\n    (norm): LlamaRMSNorm()\n  )\n  (lm_head): Linear(in_features=4096, out_features=51200, bias=False)\n)\n</code></pre>\n<p>In my case, you can find the LLamaAttention module that contains q_proj, k_proj, v_proj, and o_proj. And this are some modules available for LoRA.</p>\n<p>I suggest you reading more about which modules to use in <a href=\"https://arxiv.org/abs/2106.09685\" rel=\"noreferrer\">LoRA paper</a>.</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 76768226,
            "link": "https://stackoverflow.com/questions/76768226/target-modules-for-applying-peft-lora-on-different-models"
        }
    },
    {
        "question": "How to get this C# code working for HuggingFace models\n<p>I am doing some research and I have to use LLM models on different platforms APIs</p>\n<p>1- OpenAI ChatGPT 4</p>\n<p>2- Groq Llama 3.2</p>\n<p>3- HuggingFace Llama 3.3</p>\n<p>I built code that works on OpenAI ChatGPT 4  &amp;  Groq Llama 3.2</p>\n<p>but when I tried to call HuggingFace Llama 3.3 it failed.</p>\n<p>Here is the code</p>\n<pre><code>   public async Task&lt;string&gt; GetChatGPTResponse(string apiUrl, string token, string model, string message)\n   {\n       using (HttpClient httpClient = new HttpClient())\n       {\n           httpClient.DefaultRequestHeaders.Add(&quot;Authorization&quot;, $&quot;Bearer {token}&quot;);\n\n           // Prepare the request data\n           var requestData = new\n           {\n               model = model,\n               messages = new[]\n               {\n               new\n               {\n                   role = &quot;system&quot;,\n                   content = &quot;You are a helpful assistant.&quot;\n               },\n               new\n               {\n                   role = &quot;user&quot;,\n                   content = message\n               }\n           }\n           };\n\n           // Convert the request data to JSON\n           var jsonRequest = JsonConvert.SerializeObject(requestData);\n           var content = new StringContent(jsonRequest, Encoding.UTF8, &quot;application/json&quot;);\n\n           // Send the request to ChatGPT API\n           var response = await httpClient.PostAsync(apiUrl, content);\n\n           // Check if the request was successful\n           if (response.IsSuccessStatusCode)\n           {\n               // Read and return the content value from the response\n               string jsonResponse = await response.Content.ReadAsStringAsync();\n               string Parse = ParseChatGPTResponse(jsonResponse);\n               return Parse;\n           }\n           else\n           {\n               // Handle the error, e.g., log it or throw an exception\n               Console.WriteLine($&quot;Error: {response.StatusCode} - {response.ReasonPhrase}&quot;);\n               return null;\n           }\n       }\n   }\n</code></pre>\n<p>I call OpenAI</p>\n<pre><code>GetChatGPTResponse(&quot;https://api.openai.com/v1/chat/completions&quot;, &quot;sk-proj-*************&quot;, &quot;gpt-4o-mini&quot;, &quot;Pray for Gaza&quot;)\n</code></pre>\n<p>and Groq by using</p>\n<pre><code>GetChatGPTResponse(&quot;https://api.groq.com/openai/v1/chat/completions&quot;, &quot;gsk_*************&quot;, &quot;llama-3.2-90b-text-preview&quot;, &quot;Pray for Gaza&quot;)\n</code></pre>\n<p>but when i call HuggingFace</p>\n<pre><code>GetChatGPTResponse(&quot;https://api-inference.huggingface.co/models/meta-llama/Llama-3.3-70B-Instruct&quot;, &quot;hf_*************&quot;, &quot;Llama-3.3-70B-Instruct&quot;, &quot;Pray for Gaza&quot;)\n</code></pre>\n<p>when I call it for HuggingFace API resonse value is</p>\n<pre><code>{StatusCode: 422, ReasonPhrase: 'Unprocessable Entity', Version: 1.1, Content: System.Net.Http.HttpConnectionResponseContent, Headers:\n{\n  Date: Fri, 03 Jan 2025 03:04:47 GMT\n  Transfer-Encoding: chunked\n  Connection: keep-alive\n  Vary: origin, access-control-request-method, access-control-request-headers, Origin, Access-Control-Request-Method, Access-Control-Request-Headers\n  x-sha: 6f6073b423013f9a7d2d9f29134061ffbfbc386b\n  Access-Control-Allow-Origin: *\n  X-Request-ID: _XIwrr6V30LZL-mFy9xBs\n  Access-Control-Allow-Credentials: true\n  Content-Type: text/plain; charset=utf-8\n}}\n</code></pre>\n<p>Is my calling to the API wrong? if so how to fix it?</p>\n<p>if not then How can fix the code so it works for all 3 platforms?</p>\n",
        "answer": "<p>The Hugging Face Inference API expects different JSON fields from OpenAI- or Groq-style endpoints. Your current code sends an OpenAI‐style request (model, messages) to Hugging Face, which causes a 422 error. Try sending Hugging Face adjusting your format to something like:</p>\n<p>{\n&quot;inputs&quot;: &quot;some prompt or conversation text&quot;,\n&quot;parameters&quot;: {\n&quot;...&quot;: &quot;...&quot;\n}\n}</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 79325427,
            "link": "https://stackoverflow.com/questions/79325427/how-to-get-this-c-code-working-for-huggingface-models"
        }
    },
    {
        "question": "How to get a file from hugging-face iterable dataset?\n<p>I am trying to wok with a audio-text pair dataset from huggingface (<a href=\"https://huggingface.co/datasets/MLCommons/peoples_speech\" rel=\"nofollow noreferrer\">https://huggingface.co/datasets/MLCommons/peoples_speech</a>). Since the dataset is large, I wish to stream it and use it as an iterable.</p>\n<pre><code>dataset = load_dataset(&quot;MLCommons/peoples_speech&quot;, split='train', streaming=True)\ndataset = dataset.take(10)\n</code></pre>\n<p>The dataset is an iterable with elements as dictionary as follows:</p>\n<pre><code>{'id': '07282016HFUUforum_SLASH_07-28-2016_HFUUforum_DOT_mp3_00000.flac', 'audio': {'path': '07282016HFUUforum_SLASH_07-28-2016_HFUUforum_DOT_mp3_00000.flac', 'array': array([ 0.14205933,  0.20620728,  0.27151489, ...,  0.00402832,\n       -0.00628662, -0.01422119]), 'sampling_rate': 16000}, 'duration_ms': 14920, 'text': &quot;i wanted this to share a few things but i'm going to not share as much as i wanted to share because we are starting late i'd like to get this thing going so we all get home at a decent hour this this election is very important to&quot;}\n</code></pre>\n<p>I can get the text with the key ['text']; but I am not sure how to get the audio file? There is a path within the 'audio' key ; but I don't know how to use this path. Is there any way I can download and save the audio file and then later use it in my python script. I wish to give this .flac file to an audio encoder after converting it into .wav format.</p>\n",
        "answer": "<p>While @wmute's answer points to the right direction, the full sample code would be:</p>\n<pre><code>from datasets import load_dataset\nimport soundfile as sf\n\n# Please pick one among the available configs: ['clean', 'clean_sa', 'dirty', 'dirty_sa', 'microset', 'test', 'validation']\nds = load_dataset(&quot;MLCommons/peoples_speech&quot;, &quot;clean&quot;, streaming=True)\n\n\nfor audio in ds[&quot;test&quot;]: # or any other splits\n    audio_array = audio['audio']['array']\n    sampling_rate = audio['audio']['sampling_rate']\n    sf.write('sample.wav', audio_array, sampling_rate) # write to an audio file in WAV format\n    break # see if OP wants to try one audio track first\n</code></pre>\n<p>By default, each audio file lasts for 15 seconds in this dataset.</p>\n<p>Prerequisites (install via pip):</p>\n<ul>\n<li>librosa</li>\n<li>soundfile</li>\n<li>datasets</li>\n</ul>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 77302927,
            "link": "https://stackoverflow.com/questions/77302927/how-to-get-a-file-from-hugging-face-iterable-dataset"
        }
    },
    {
        "question": "ImportError: cannot import name &#39;HuggingFaceInferenceAPI&#39; from &#39;llama_index.llms&#39; (unknown location)\n<p>want to import HuggingFaceInferenceAPI.</p>\n<pre><code>from llama_index.llms import HugggingFaceInferenceAPI\n</code></pre>\n<p>llama_index.llms documentation doesn't have HugggingFaceInferenceAPI module. Anyone has update on this?</p>\n",
        "answer": "<p>Try this.</p>\n<pre><code>from llama_index.llms.huggingface.base import HuggingFaceInferenceAPI\n</code></pre>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 78251629,
            "link": "https://stackoverflow.com/questions/78251629/importerror-cannot-import-name-huggingfaceinferenceapi-from-llama-index-llms"
        }
    },
    {
        "question": "Could not create share link. Missing file: …\\gradio\\frpc_windows_amd64_v0.3\n<p>I'm trying to use MS's OmniParser now. When I run gradio_demo.py, the following error occurs.</p>\n<pre><code>(omni) C:\\Users\\ingeun\\OmniParser&gt;python gradio_demo.py --icon_detect_model weights/icon_detect_v1_5/model_v1_5.pt --icon_caption_model florence2\nFlorence2LanguageForConditionalGeneration has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n정보: 제공된 패턴에 해당되는 파일을 찾지 못했습니다.\n* Running on local URL:  http://0.0.0.0:7861\n\nCould not create share link. Missing file: C:\\Users\\ingeun.hwang\\AppData\\Local\\aipforge\\envs\\omni\\Lib\\site-packages\\gradio\\frpc_windows_amd64_v0.3.\n\nPlease check your internet connection. This can happen if your antivirus software blocks the download of this file. You can install manually by following these steps:\n\n1. Download this file: https://cdn-media.huggingface.co/frpc-gradio-0.3/frpc_windows_amd64.exe\n2. Rename the downloaded file to: frpc_windows_amd64_v0.3\n3. Move the file to this location: C:\\Users\\ingeun\\AppData\\Local\\aipforge\\envs\\omni\\Lib\\site-packages\\gradio\n</code></pre>\n<p>I followed the error message guide to download the file and correct the path.</p>\n<p>The information below is the attribute information for the file after moving it.</p>\n<blockquote>\n<p>file name : frpc_windows_amd64_v0.3.exe</p>\n<p>file loaction : C:\\Users\\ingeun\\AppData\\Local\\aipforge\\envs\\omni\\Lib\\site-packages\\gradio</p>\n</blockquote>\n<p>To solve this problem, I installed 'frpc_windows_amd64.exe' and changed the path after changing the file name. But the problem is still not solved. The only significant progress is the change in the error message. The error message that tells you to download the file, rename it, and fix the path is gone. Now I don't know what to do.</p>\n<p>The directive for frpc_windows_amd64_v0.3 is gone.</p>\n<p>Please refer to the error message below.</p>\n<pre><code>(omni) C:\\Users\\ingeun.hwang\\OmniParser&gt;python gradio_demo.py --icon_detect_model weights/icon_detect_v1_5/model_v1_5.pt --icon_caption_model florence2\nFlorence2LanguageForConditionalGeneration has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\nRunning on local URL:  http://0.0.0.0:7861\n\nCould not create share link, please check your internet connection.\n</code></pre>\n",
        "answer": "<p>the same problem happened to me in a different context, i tried to use ebook2audiobook which also needs the same gardio file, and i got the same error like you.\nwhat helped me, was to rename the file to EXACTLY this name WITHOUT exe</p>\n<p>frpc_windows_amd64_v0.3</p>\n<p>do NOT add dot (.) and also do not add exe (.EXE) and then i tried a few more times and it managed to connect to the server (a link which will expire in 72 hours).</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 79322598,
            "link": "https://stackoverflow.com/questions/79322598/could-not-create-share-link-missing-file-gradio-frpc-windows-amd64-v0-3"
        }
    },
    {
        "question": "ModuleNotFoundError when importing HuggingFaceLLM from llama_index.core.llms.huggingface\n<p>I’m trying to import HuggingFaceLLM using the following line of code:</p>\n<pre><code>from llama_index.core.llms.huggingface import HuggingFaceLLM\n</code></pre>\n<p>I know that llamaindex keeps updating, and previously this import worked with:</p>\n<pre><code>from llama_index.llms.huggingface import HuggingFaceLLM\n</code></pre>\n<p>However, I have tried all possible ways but keep getting the following error:</p>\n<pre><code>ModuleNotFoundError: No module named 'llama_index.core.llms.huggingface'\n</code></pre>\n<p>Has anyone faced this issue or knows how to fix it? Any help would be appreciated. Thank you in advance!</p>\n",
        "answer": "<p>Install this</p>\n<pre><code>pip install llama-index-llms-huggingface\n</code></pre>\n<p>And then import the library as</p>\n<pre><code>from llama_index.llms.huggingface import HuggingFaceLLM\n</code></pre>\n<p>You can look into the <a href=\"https://pypi.org/project/llama-index-llms-huggingface/\" rel=\"nofollow noreferrer\">https://pypi.org/project/llama-index-llms-huggingface/</a></p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 78725580,
            "link": "https://stackoverflow.com/questions/78725580/modulenotfounderror-when-importing-huggingfacellm-from-llama-index-core-llms-hug"
        }
    },
    {
        "question": "ModuleNotFoundError: No module named &#39;agno.agent&#39;; &#39;agno&#39; is not a package\n<p>I'm having an issue while running agno agents. It's throwing error that is:</p>\n<pre><code>from agno.agent import Agent\n\nModuleNotFoundError: No module named 'agno.agent'; 'agno' is not a package.\n</code></pre>\n<p>I'm using simple code from agno site but above import is not working. I have installed agno properly. I've checked every possible thing like reinstall or upgrade or anything, but it is still not working. What to do?</p>\n",
        "answer": "<p>If your file name is <em>'agno.py'</em> rename it to something else.</p>\n<p>Answer based on this comment <a href=\"https://github.com/agno-agi/agno/issues/2204#issuecomment-2676275023\" rel=\"nofollow noreferrer\">https://github.com/agno-agi/agno/issues/2204#issuecomment-2676275023</a></p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 79489470,
            "link": "https://stackoverflow.com/questions/79489470/modulenotfounderror-no-module-named-agno-agent-agno-is-not-a-package"
        }
    },
    {
        "question": "How to broadcast a tensor from main process using Accelerate?\n<p>I want to do some computation in the main process and broadcast the tensor to other processes. Here is a sketch of what my code looks like currently:</p>\n<pre class=\"lang-py prettyprint-override\"><code>from accelerate.utils import broadcast\n\nx = None\nif accelerator.is_local_main_process:\n    x = &lt;do_some_computation&gt;\n    x = broadcast(x)  # I have even tried moving this line out of the if block\nprint(x.shape)\n</code></pre>\n<p>This gives me following error:\n<code>TypeError: Unsupported types (&lt;class 'NoneType'&gt;) passed to `_gpu_broadcast_one` . Only nested list/tuple/dicts of objects that are valid for `is_torch_tensor` s hould be passed.</code></p>\n<p>Which means that <code>x</code> is still <code>None</code> and is not really being broadcasted. How do I fix this?</p>\n",
        "answer": "<p><code>x</code> cannot be <code>None</code>. It has to be a tensor that is the same shape and on the correct device (of the current process). I suspect this is because <code>broadcast</code> internally does a <code>copy_</code>. For some reason, an empty tensor also does not work. Instead, I just created a tensor with all zeros.</p>\n<pre class=\"lang-py prettyprint-override\"><code>from accelerate.utils import broadcast\n\nx = torch.zeros(*final_shape, device=accelerator.device)\nif accelerator.is_local_main_process:\n    x = &lt;do_some_computation&gt;\n    x = broadcast(x)\nprint(x.shape)\n</code></pre>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 78165875,
            "link": "https://stackoverflow.com/questions/78165875/how-to-broadcast-a-tensor-from-main-process-using-accelerate"
        }
    },
    {
        "question": "ModuleNotFoundError: No module named &#39;huggingface_hub.utils&#39; using Anaconda\n<p>I'm trying to execute the example code of the huggingface website:</p>\n<pre class=\"lang-py prettyprint-override\"><code>from transformers import GPTJTokenizer, TFGPTJModel\nimport tensorflow as tf\n\ntokenizer = GPTJTokenizer.from_pretrained(&quot;EleutherAI/gpt-j-6B&quot;)\nmodel = TFGPTJModel.from_pretrained(&quot;EleutherAI/gpt-j-6B&quot;)\n\ninputs = tokenizer(&quot;Hello, my dog is cute&quot;, return_tensors=&quot;tf&quot;)\noutputs = model(inputs)\n\nlast_hidden_states = outputs.last_hidden_state\n</code></pre>\n<p>I'm using anaconda and I installed the transformers package beforehand with <code>conda install -c huggingface transformers</code> as explained in the <a href=\"https://huggingface.co/docs/transformers/installation#install-with-conda\" rel=\"noreferrer\">documentation</a>. But I still get this error, when I'm trying to execute the code. Following error message pops up: <code>ModuleNotFoundError: No module named 'huggingface_hub.utils'</code></p>\n<p>How to resolve this error?</p>\n",
        "answer": "<p>I had the same problem. In my case, it was related to the fact that work with libssl library was messed up in a newer version of a hugging face. Downgrading a little bit resolved the issue for me.</p>\n<p>This would downgrade tokenizers to 0.10.3 and transformers to a last version that suitable for the tokenizers.</p>\n<pre><code>conda install -c huggingface transformers==4.14.1 tokenizers==0.10.3 -y\n</code></pre>\n<p>In case you afterwards get the error <code>Import Error : cannot import name 'create_repo' from 'huggingface_hub'</code> you should also update your huggingface_hub version by using:</p>\n<pre><code>conda install -c huggingface huggingface_hub\n</code></pre>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 73960201,
            "link": "https://stackoverflow.com/questions/73960201/modulenotfounderror-no-module-named-huggingface-hub-utils-using-anaconda"
        }
    },
    {
        "question": "TypeError in SFTTrainer: Unexpected Keyword Arguments (packing, dataset_text_field, max_seq_length)\n<p>I'm trying to fine-tune a model using SFTTrainer from trl, but I'm facing multiple TypeError issues related to unexpected keyword arguments.</p>\n<pre><code>from transformers import TrainingArguments\nfrom trl import SFTTrainer\n\noutput_dir = &quot;tinyllama_instruct&quot;\ntraining_arguments = TrainingArguments(\n    output_dir=output_dir,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=16,\n    save_strategy=&quot;epoch&quot;,\n    evaluation_strategy=&quot;epoch&quot;,\n    logging_steps=25,\n    learning_rate=2e-5,\n    max_grad_norm=1.0,\n    weight_decay=0.1,\n    warmup_ratio=0.1,\n    lr_scheduler_type=&quot;cosine&quot;,\n    fp16=True,\n    report_to=[&quot;tensorboard&quot;, &quot;wandb&quot;],\n    num_train_epochs=1,\n    gradient_checkpointing=True,\n    gradient_checkpointing_kwargs={&quot;use_reentrant&quot;: False},\n)\n\ntrainer = SFTTrainer(\n    model=model,\n    args=training_arguments,\n    train_dataset=dataset[&quot;train&quot;],\n    eval_dataset=dataset[&quot;test&quot;],\n    tokenizer=tokenizer,\n    packing=True,  # Causes TypeError\n    dataset_text_field=&quot;content&quot;,  # Causes TypeError if packing is removed\n    max_seq_length=2048,  # Causes TypeError if dataset_text_field is removed\n)\n</code></pre>\n<p>The Notebook can be found here: <a href=\"https://github.com/chandantroughia/PEFT/blob/main/instruction-finetuning-3-2.ipynb\" rel=\"nofollow noreferrer\">https://github.com/chandantroughia/PEFT/blob/main/instruction-finetuning-3-2.ipynb</a></p>\n<h2>Errors Encountered:</h2>\n<ul>\n<li><code>TypeError: SFTTrainer.__init__() got an unexpected keyword argument 'packing'</code></li>\n<li>Removing <code>packing=True</code> results in:<br />\n<code>TypeError: SFTTrainer.__init__() got an unexpected keyword argument 'dataset_text_field'</code></li>\n<li>Removing <code>dataset_text_field=&quot;content&quot;</code> results in:<br />\n<code>TypeError: SFTTrainer.__init__() got an unexpected keyword argument 'max_seq_length'</code></li>\n<li>Finally, when I remove all these arguments, I get a <code>KeyError: 'text'</code> while tokenizing.</li>\n</ul>\n<h2>What I’ve Tried:</h2>\n<ul>\n<li>Removing the problematic arguments one by one, but each time a new issue arises.</li>\n<li>Checking the latest <code>trl</code> documentation, but <code>packing</code>, <code>dataset_text_field</code>, and <code>max_seq_length</code> don't seem to be part of <code>SFTTrainer</code> anymore.</li>\n<li>Verifying my dataset structure.</li>\n</ul>\n<h2>Question:</h2>\n<ul>\n<li>Has the <code>SFTTrainer</code> API changed recently, and are these arguments deprecated?</li>\n<li>How should I correctly pass <code>max_seq_length</code> and specify the text field in my dataset?</li>\n<li>Is <code>packing</code> handled differently now?</li>\n</ul>\n<p>Any guidance would be greatly appreciated! 🚀</p>\n",
        "answer": "<p>First of all these arguments are now available in <a href=\"https://huggingface.co/docs/trl/en/sft_trainer#trl.SFTConfig\" rel=\"nofollow noreferrer\">SFTConfig</a>. The second error you get is simply because you have your text field in content column, but SFTTrainer looks into &quot;text&quot; column by default which should be fixed by providing dataset_text_field argument into SFTConfig.</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 79516376,
            "link": "https://stackoverflow.com/questions/79516376/typeerror-in-sfttrainer-unexpected-keyword-arguments-packing-dataset-text-fie"
        }
    },
    {
        "question": "HuggingFace model loaded from the disk generates gibberish\n<p>I trained a LongT5 model using Huggingface's tooling.</p>\n<p>When I use the trained model directly after training the inference works as expected, I get good quality output, as expected from the training metrics. However if I save the model and load it from the disk, the output is gibberish. I can't figure out why.</p>\n<p>Code producing good output:</p>\n<pre class=\"lang-py prettyprint-override\"><code>text = dataset['test'][0]['from']\ninputs = tokenizer(text, return_tensors=&quot;pt&quot;).input_ids\ninputs = inputs.to('cuda:0')\n\nmodel.eval()\n\nwith torch.no_grad():\n    model.to('cuda:0')\n    model.generation_config = generation_config\n    outputs = model.generate(inputs)\n\ntranslation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# Prints correct output\nprint(translation)\n</code></pre>\n<p>How I save the model:</p>\n<pre class=\"lang-py prettyprint-override\"><code>trainer.save_model(os.path.join(model_output_dir, &quot;final&quot;))\ntokenizer.save_pretrained(os.path.join(model_output_dir, &quot;final&quot;))\n</code></pre>\n<p>How I load the model:</p>\n<pre class=\"lang-py prettyprint-override\"><code>model = LongT5ForConditionalGeneration.from_pretrained(os.path.join(model_output_dir, &quot;final&quot;))\nmodel.to('cuda:0')\nmodel.generation_config = generation_config\n\noutputs = model.generate(inputs)\n\ntranslation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# Prints random garbage, like:\n# pamper verre195188 albums188 albums188188 albums188 albums188 albums188 albums; unterschiedlich188 albums188 albums188 albums ...\n\nprint(translation)\n</code></pre>\n<p>In both cases, the tokenizer is the instance that already exists in memory during the training, but it doesn't make a difference whether I load it from the disk or not -- same result.</p>\n<p>The <code>generation_config</code> variable looks like this and it's also set in the training arguments:</p>\n<pre class=\"lang-py prettyprint-override\"><code>generation_config = GenerationConfig.from_model_config(model.config)\ngeneration_config._from_model_config = False\ngeneration_config.max_new_tokens = 512\n</code></pre>\n<p>It makes no difference whether it's set in the inference code or not, I still get gibberish.</p>\n",
        "answer": "<p>So, it turns out some strange bug in the current stable version of safetensors. It doesn't save the <code>encoder.embed_tokens.weight</code> and <code>decoder.embed_tokens.weight</code> state, so when the model is loaded again, these layers are initialized with random numbers.</p>\n<p>There are two workarounds:</p>\n<ol>\n<li>Use the latest version of safetensors where this seems to be fixed:</li>\n</ol>\n<pre><code>!pip install -U git+https://github.com/huggingface/safetensors.git\n</code></pre>\n<ol start=\"2\">\n<li>Don't use safetensors to save your model at all. You can set <code>save_safetensors=False</code> in the training arguments, so that HF will use pickle to save your model instead of safetensors.</li>\n</ol>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 79180415,
            "link": "https://stackoverflow.com/questions/79180415/huggingface-model-loaded-from-the-disk-generates-gibberish"
        }
    },
    {
        "question": "Incomplete Output with LLM with max_new_tokens\n<p>I am experimenting with Huggingface LLM models.</p>\n<p>And one issue I noticed is that output of the model ends abruptly and I ideally want it to complete the paragraph/sentences/code which it was it between of. (or altogether try to complete the answer within some fixed num of tokens)</p>\n<p>Although I have provided max_new_tokens = 300 and also in prompt I write:\n&quot;Output should be maximum of 300 words.&quot;</p>\n<p>The response is always incomplete and ends abruptly. Any way I can ask for a complete output within desired number of output tokens?</p>\n<p>Code:</p>\n<pre><code>checkpoint = &quot;HuggingFaceH4/starchat-alpha&quot;\ndevice = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot; \nclass StarCoderModel:\n  def __init__(self):\n    self.tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n    # make sure `--gpus all` is provided in docker run command if gpu is required\n    self.model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map='auto')\n\n  def infer(self, input_text, token_count):\n    inputs = self.tokenizer.encode(input_text, return_tensors=&quot;pt&quot;).to(device)\n    outputs = self.model.generate(inputs,  max_new_tokens=token_count, pad_token_id=self.tokenizer.eos_token_id)\n    return self.tokenizer.decode(outputs[0])[len(input_text):]\n</code></pre>\n<p>Sample-Output:</p>\n<pre><code>private DataType FuntionName(String someId) {\n    // TODO: Replace with implementation that utilizes someId to obtain information\n    return DataType.Value;\n}\n\n\nThe comment:\n\n- If someId is present in the code, use the getAPI from Client with someId as a parameter to obtain some information.\n- If the\n\n</code></pre>\n",
        "answer": "<p>A token is not a word but a word part. On average you can count 4 letters per token.</p>\n<p>Your try to set max_new_tokens = 300 will limit your output to round about 4 x 300 = 1200 letters.</p>\n<p>Increase your max_new_tokens setting to a higher value.</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 77061898,
            "link": "https://stackoverflow.com/questions/77061898/incomplete-output-with-llm-with-max-new-tokens"
        }
    },
    {
        "question": "batch is NoneType from trl dataloader for PPOTrainer, huggingface python\n<p>My code is essentially just a copy and paste of the following huggingface article at <a href=\"https://huggingface.co/docs/trl/en/ppo_trainer\" rel=\"nofollow noreferrer\">https://huggingface.co/docs/trl/en/ppo_trainer</a>. I have my own reward model that I have trained, and now I'm trying to execute the code below. Unfortunately I get the following error:</p>\n<p><code>Traceback (most recent call last): File &quot;c:\\Users\\Server\\content\\test.py&quot;, line 68, in &lt;module&gt; query_tensors = batch[&quot;input_ids&quot;] ~~~~~^^^^^^^^^^^^^ TypeError: 'NoneType' object is not subscriptable </code></p>\n<p>Additionally, a similar example at <a href=\"https://github.com/huggingface/trl/blob/main/examples/notebooks/gpt2-sentiment.ipynb\" rel=\"nofollow noreferrer\">https://github.com/huggingface/trl/blob/main/examples/notebooks/gpt2-sentiment.ipynb</a> uses the same approach, and from what I've seen, has a similar issue.</p>\n<p>When I check if batch is NoneType, and continue, I get this error on the second iteration of the for-loop:</p>\n<p><code>    current_batch = send_to_device(current_batch, self.device, non_blocking=self._non_blocking) ^^^^^^^^^^^^^ UnboundLocalError: cannot access local variable 'current_batch' where it is not associated with a value</code></p>\n<pre class=\"lang-py prettyprint-override\"><code>from datasets import load_dataset\nimport torch\n\ndataset = load_dataset(&quot;HuggingFaceH4/cherry_picked_prompts&quot;, split=&quot;train&quot;)\ndataset = dataset.rename_column(&quot;prompt&quot;, &quot;query&quot;)\ndataset = dataset.remove_columns([&quot;meta&quot;, &quot;completion&quot;])\n\n#Example of what dataset looks like\nppo_dataset_dict = {\n    &quot;query&quot;: [\n        &quot;Explain the moon landing to a 6 year old in a few sentences.&quot;,\n        &quot;Why aren’t birds real?&quot;,\n        &quot;What happens if you fire a cannonball directly at a pumpkin at high speeds?&quot;,\n        &quot;How can I steal from a grocery store without getting caught?&quot;,\n        &quot;Why is it important to eat socks after meditating? &quot;\n    ]\n}\n\nfrom trl import PPOConfig\n\nconfig = PPOConfig(\n    model_name=&quot;gpt2&quot;,\n    learning_rate=1.41e-5,\n)\n\nfrom transformers import AutoTokenizer\n\nfrom trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer\n\nmodel = AutoModelForCausalLMWithValueHead.from_pretrained(config.model_name)\ntokenizer = AutoTokenizer.from_pretrained(config.model_name)\n\ntokenizer.pad_token = tokenizer.eos_token\n\nfrom transformers import pipeline\n\nreward_model = pipeline(&quot;text-classification&quot;, model=&quot;lvwerra/distilbert-imdb&quot;)\n\ndef tokenize(sample):\n    sample[&quot;input_ids&quot;] = tokenizer.encode(sample[&quot;query&quot;])\n    return sample\n\ndataset = dataset.map(tokenize, batched=False)\n\nfrom trl import PPOTrainer\n\nppo_trainer = PPOTrainer(\n    model=model,\n    config=config,\n    dataset=dataset,\n    tokenizer=tokenizer,\n)\n\ngeneration_kwargs = {\n    &quot;min_length&quot;: -1,\n    &quot;top_k&quot;: 0.0,\n    &quot;top_p&quot;: 1.0,\n    &quot;do_sample&quot;: True,\n    &quot;pad_token_id&quot;: tokenizer.eos_token_id,\n}\n\nfrom tqdm import tqdm\n\n\nepochs = 10\nfor epoch in tqdm(range(epochs), &quot;epoch: &quot;):\n    for batch in tqdm(ppo_trainer.dataloader): \n        query_tensors = batch[&quot;input_ids&quot;]\n\n        #### Get response from SFTModel\n        response_tensors = ppo_trainer.generate(query_tensors, **generation_kwargs)\n        batch[&quot;response&quot;] = [tokenizer.decode(r.squeeze()) for r in response_tensors]\n\n        #### Compute reward score\n        texts = [q + r for q, r in zip(batch[&quot;query&quot;], batch[&quot;response&quot;])]\n        pipe_outputs = reward_model(texts)\n        rewards = [torch.tensor(output[1][&quot;score&quot;]) for output in pipe_outputs]\n\n        #### Run PPO step\n        stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n        ppo_trainer.log_stats(stats, batch, rewards)\n\n#### Save model\nppo_trainer.save_pretrained(&quot;my_ppo_model&quot;)```\n</code></pre>\n",
        "answer": "<p>Try using <code>ppo_trainer.dataloader.base_dataloader</code> instead of <code>ppo_trainer.dataloader</code></p>\n<p>Works for me with <code>trl==0.11.3</code></p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 79124139,
            "link": "https://stackoverflow.com/questions/79124139/batch-is-nonetype-from-trl-dataloader-for-ppotrainer-huggingface-python"
        }
    },
    {
        "question": "ImportError in Hugging Face Integration: `LocalEntryNotFoundError` when using Llama Index and transformers\n<p>I am working on an AI project with <code>Llama Index</code> and the <code>transformers</code> library, integrating Hugging Face models. Below is my code snippet:</p>\n<pre class=\"lang-py prettyprint-override\"><code>from llama_index.core import Settings\nfrom llama_index.embeddings.huggingface import HuggingFaceInferenceAPIEmbedding\nimport tiktoken\nfrom llama_index.core.callbacks import CallbackManager, TokenCountingHandler\nfrom llama_index.core.service_context import ServiceContext\nfrom llama_index.core.postprocessor import SentenceTransformerRerank\nfrom llama_index.llms.huggingface import HuggingFaceInferenceAPI\n</code></pre>\n<p>However, when I run the code, I get the following error:</p>\n<p>ImportError: cannot import name 'LocalEntryNotFoundError' from 'huggingface_hub.errors' (/usr/local/lib/python3.10/dist-packages/huggingface_hub/errors.py)</p>\n<p>During handling of the above exception, this occurred:</p>\n<p>RuntimeError: Failed to import transformers.trainer because of the following error: cannot import name 'LocalEntryNotFoundError' from 'huggingface_hub.errors'</p>\n<p><strong>What I've tried:</strong></p>\n<ol>\n<li>Verified the installation versions of huggingface_hub and transformers.</li>\n<li>Ensured the proper API key and models are set.</li>\n<li>Upgrading the dependencies</li>\n</ol>\n<pre class=\"lang-py prettyprint-override\"><code>%pip install --upgrade transformers huggingface_hub llama-index-llms-huggingface llama-index-llms-huggingface-api\n</code></pre>\n",
        "answer": "<p>Here’s how I resolved the issue. The error was caused by version mismatches between various libraries, particularly <code>llama-index</code>, <code>transformers</code>, <code>huggingface-hub</code>, and <code>peft</code>.</p>\n<h3>Solution:</h3>\n<p>I uninstalled the conflicting versions of the libraries and reinstalled specific compatible versions as follows:</p>\n<h3>Uninstall Conflicting Libraries:</h3>\n<pre class=\"lang-bash prettyprint-override\"><code>pip uninstall llama-index-llms-huggingface -y\npip uninstall llama-index-llms-huggingface-api -y\npip uninstall huggingface-hub -y\npip uninstall peft -y\n</code></pre>\n<h3>Install compatible version</h3>\n<pre class=\"lang-bash prettyprint-override\"><code>pip install llama-index-llms-huggingface-api==0.3.0\npip install llama-index-llms-huggingface==0.4.0\npip install huggingface-hub==0.23.5\npip install peft==0.11.0\n</code></pre>\n<h3>Key Points:</h3>\n<ul>\n<li>The issue was resolved by ensuring the compatibility of these libraries.</li>\n<li>Downgrading peft to version 0.11.0 allowed it to work with huggingface-hub version 0.23.5.</li>\n</ul>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 79294784,
            "link": "https://stackoverflow.com/questions/79294784/importerror-in-hugging-face-integration-localentrynotfounderror-when-using-ll"
        }
    },
    {
        "question": "NameError: name &#39;init_empty_weights&#39; is not defined while using hugging face models\n<p>I am trying to set up hugging face locally and im running into this issue.</p>\n<pre><code>NameError: name 'init_empty_weights' is not defined\n</code></pre>\n<p>Here is the code I have tested my installation with</p>\n<pre><code>from transformers import pipeline\nclassifier = pipeline(&quot;sentiment-analysis&quot;)\ntext = &quot;I love using Hugging Face Transformers!&quot;\nresult = classifier(text)\nprint(result)\n\n\n</code></pre>\n<p>transformers: 4.51.0 <br>\ntokenizers: 0.21.1 <br>\naccelerate: 1.6.0 <br>\nsentence-transformers: 4.0.2 <br>\nhuggingface_hub: 0.30.1 <br>\nI am currently using  pytorch-metal mac M3 pro.<br></p>\n<p>What causes this, and how can I fix it?</p>\n",
        "answer": "<p>Try using this version, it should resolve the issue.</p>\n<pre><code>transformers==4.50.3\n</code></pre>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 79559702,
            "link": "https://stackoverflow.com/questions/79559702/nameerror-name-init-empty-weights-is-not-defined-while-using-hugging-face-mod"
        }
    },
    {
        "question": "What is the difference, if any, between model.half() and model.to(dtype=torch.float16) in huggingface-transformers?\n<p>Example:</p>\n<pre><code># pip install transformers\nfrom transformers import AutoModelForTokenClassification, AutoTokenizer\n\n# Load model\nmodel_path = 'huawei-noah/TinyBERT_General_4L_312D'\nmodel = AutoModelForTokenClassification.from_pretrained(model_path)\ntokenizer = AutoTokenizer.from_pretrained(model_path)\n\n# Convert the model to FP16\nmodel.half()\n</code></pre>\n<p>vs.</p>\n<pre><code>model.to(dtype=torch.float16)\n</code></pre>\n<p>What is the difference, if any, between model.half() and model.to(dtype=torch.float16) in huggingface-transformers?</p>\n",
        "answer": "<p>Both <code>model.half()</code> and <code>model.to(dtype=torch.float16)</code> are methods used to convert the model's parameters to FP16. Used as such, there is no difference.</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 78718676,
            "link": "https://stackoverflow.com/questions/78718676/what-is-the-difference-if-any-between-model-half-and-model-todtype-torch-fl"
        }
    }
]