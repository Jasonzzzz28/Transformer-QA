[
    {
        "question": "How to quantize a HF safetensors model and save it to llama.cpp GGUF format with less than q8_0 quantization?\n<p>I'm developing LLM agents using llama.cpp as inference engine. Sometimes I want to use models in safetensors format and there is a python script (<a href=\"https://github.com/ggerganov/llama.cpp/blob/master/convert_hf_to_gguf.py\" rel=\"nofollow noreferrer\">https://github.com/ggerganov/llama.cpp/blob/master/convert_hf_to_gguf.py</a>) to convert.</p>\n<p>Script is awesome, but minimum number size is 8 bit (q8_0). Is there any other script or repo with other quantization formats?</p>\n",
        "answer": "<p>First step is convert huggingface model to gguf (16b float or 32b float is recommended) using <code>convert_hf_to_gguf.py</code> from llama.cpp repository.</p>\n<p>Second step is use compiled c++ code from <code>/examples/quantize/</code> subdirectory of llama.cpp (<a href=\"https://github.com/ggerganov/llama.cpp/tree/master/examples/quantize\" rel=\"nofollow noreferrer\">https://github.com/ggerganov/llama.cpp/tree/master/examples/quantize</a>)</p>\n<p>Process is pretty straightforward and well-documented.</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 78842047,
            "link": "https://stackoverflow.com/questions/78842047/how-to-quantize-a-hf-safetensors-model-and-save-it-to-llama-cpp-gguf-format-with"
        }
    },
    {
        "question": "How can I adjust the performance of tokenizer?\n<p>Working with the tokenizer from the <code>transformers</code> library of Hugging Face. The tokenizer works fine in most cases, but in some cases, it does not.</p>\n<p>I'm wondering if I can <strong>&quot;adjust&quot;</strong> (not train a new tokenizer from scratch) the performance of the tokenizer to handle the bad cases while still maintaining good performance in most cases as it used to.</p>\n<p>To be more specific, the type of tokenizer is <code>transformers.XLMRobertaTokenizerFast</code>, which is a unigram tokenizer, and the model is <code>paraphrase-multilingual-mpnet-base-v2</code>.</p>\n",
        "answer": "<p>You can change the tokenizer's vocabulary:</p>\n<pre><code>tokenizer.add_tokens([&quot;asadaf&quot;, &quot;sdfsaf&quot;])\nmodel.resize_token_embeddings(len(tokenizer)) # change input embeddings size\ninput_text = &quot;This is asadaf and sdfsaf&quot;\nprint(tokenizer(input_text))\n</code></pre>\n<p>As a result, <em>asadaf</em> and <em>sdfsaf</em> would be tokenized as unique words.</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 79100835,
            "link": "https://stackoverflow.com/questions/79100835/how-can-i-adjust-the-performance-of-tokenizer"
        }
    },
    {
        "question": "How to get a file from hugging-face iterable dataset?\n<p>I am trying to wok with a audio-text pair dataset from huggingface (<a href=\"https://huggingface.co/datasets/MLCommons/peoples_speech\" rel=\"nofollow noreferrer\">https://huggingface.co/datasets/MLCommons/peoples_speech</a>). Since the dataset is large, I wish to stream it and use it as an iterable.</p>\n<pre><code>dataset = load_dataset(&quot;MLCommons/peoples_speech&quot;, split='train', streaming=True)\ndataset = dataset.take(10)\n</code></pre>\n<p>The dataset is an iterable with elements as dictionary as follows:</p>\n<pre><code>{'id': '07282016HFUUforum_SLASH_07-28-2016_HFUUforum_DOT_mp3_00000.flac', 'audio': {'path': '07282016HFUUforum_SLASH_07-28-2016_HFUUforum_DOT_mp3_00000.flac', 'array': array([ 0.14205933,  0.20620728,  0.27151489, ...,  0.00402832,\n       -0.00628662, -0.01422119]), 'sampling_rate': 16000}, 'duration_ms': 14920, 'text': &quot;i wanted this to share a few things but i'm going to not share as much as i wanted to share because we are starting late i'd like to get this thing going so we all get home at a decent hour this this election is very important to&quot;}\n</code></pre>\n<p>I can get the text with the key ['text']; but I am not sure how to get the audio file? There is a path within the 'audio' key ; but I don't know how to use this path. Is there any way I can download and save the audio file and then later use it in my python script. I wish to give this .flac file to an audio encoder after converting it into .wav format.</p>\n",
        "answer": "<p>You can save the given audio_array and sampling_rate variables to a soundfile like this:</p>\n<pre><code>audio_array = dataset['audio']['array']\nsampling_rate = dataset['audio']['sampling_rate']\n\nimport soundfile as sf\nsf.write(&quot;sample.wav&quot;, audio_array, sampling_rate)\n</code></pre>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 77302927,
            "link": "https://stackoverflow.com/questions/77302927/how-to-get-a-file-from-hugging-face-iterable-dataset"
        }
    },
    {
        "question": "How to broadcast a tensor from main process using Accelerate?\n<p>I want to do some computation in the main process and broadcast the tensor to other processes. Here is a sketch of what my code looks like currently:</p>\n<pre class=\"lang-py prettyprint-override\"><code>from accelerate.utils import broadcast\n\nx = None\nif accelerator.is_local_main_process:\n    x = &lt;do_some_computation&gt;\n    x = broadcast(x)  # I have even tried moving this line out of the if block\nprint(x.shape)\n</code></pre>\n<p>This gives me following error:\n<code>TypeError: Unsupported types (&lt;class 'NoneType'&gt;) passed to `_gpu_broadcast_one` . Only nested list/tuple/dicts of objects that are valid for `is_torch_tensor` s hould be passed.</code></p>\n<p>Which means that <code>x</code> is still <code>None</code> and is not really being broadcasted. How do I fix this?</p>\n",
        "answer": "<p>It might not be like that (about the position of <code>broadcast</code>). I've recently run into the same issue and figured out the solution as below:</p>\n<pre class=\"lang-py prettyprint-override\"><code>x = torch.zeros(shape0)\n\nif accelerator.is_main_process:\n    x = &lt;some computation here&gt;\n\nx = broadcast(x) # broadcast does a sending-or-receiving job. it should be out of the if block\n</code></pre>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 78165875,
            "link": "https://stackoverflow.com/questions/78165875/how-to-broadcast-a-tensor-from-main-process-using-accelerate"
        }
    },
    {
        "question": "How to convert safetensors model to onnx model?\n<p>I want to convert a <code>model.safetensors</code> to ONNX, unfortunately I haven't found enough information about the procedure. The documentation of <code>safetensors</code> package isn't enough and actually is not clear even how to get the original (pytorch in my case) model, since when I try something as</p>\n<pre><code>with st.safe_open(modelsafetensors, framework=&quot;pt&quot;) as mystf:\n   ...\n</code></pre>\n<p>the <code>mystf</code> object has <code>get_tensor('sometensorname')</code> but it seems hasn't any <code>get_model()</code> method or something similar.</p>\n",
        "answer": "<p>I had the same problem and this is what I found for the conversion of model:</p>\n<p><a href=\"https://github.com/xenova/transformers.js?tab=readme-ov-file#convert-your-models-to-onnx\" rel=\"nofollow noreferrer\">Convertor</a></p>\n<p>Download from repo <code>convert.py</code> and <code>requirements.txt</code> from <a href=\"https://github.com/xenova/transformers.js/tree/main/scripts\" rel=\"nofollow noreferrer\">scripts</a>, put it to some folder.</p>\n<ol>\n<li>Create a virtual environment by running <code>python -m venv .venv</code></li>\n<li>Activate it by running <code>source .venv/bin/activate</code></li>\n<li>Install requirements by <code>pip install -r requirements.txt</code></li>\n<li>Run a script for converting a model, in my case it was: <code>python convert.py --quantize --model_id BAAI/bge-small-en-v1.5</code></li>\n</ol>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 77855742,
            "link": "https://stackoverflow.com/questions/77855742/how-to-convert-safetensors-model-to-onnx-model"
        }
    },
    {
        "question": "FastAPI + Transformers + 4-bit Mistral: .to() is not supported for bitsandbytes 4-bit models error\n<p>I'm deploying a FastAPI backend using HuggingFace Transformers with the <code>mistralai/Mistral-7B-Instruct-v0.1</code> model, quantized to 4-bit using <code>BitsAndBytesConfig</code>. I’m running this inside an NVIDIA GPU container (CUDA 12.1, A10G GPU with 22GB VRAM), and I keep hitting this error during model loading:</p>\n<pre><code>ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. \nPlease use the model as it is...\n</code></pre>\n<p>What I’ve Done So Far:</p>\n<ul>\n<li><p>I'm not calling <code>.to(...)</code> anywhere — explicitly removed all such lines.✅</p>\n</li>\n<li><p>I'm using <code>quantization_config=BitsAndBytesConfig(...)</code> with <code>load_in_4bit=True</code>.✅</p>\n</li>\n<li><p>I removed <code>device_map=&quot;auto&quot;</code> as per the transformers GitHub issue✅</p>\n</li>\n<li><p>I'm calling <code>.cuda()</code> only once on the model after <code>.from_pretrained(...)</code>, as suggested ✅</p>\n</li>\n<li><p>Model and tokenizer are being loaded via Hugging Face Hub with <code>HF_TOKEN</code> properly set ✅</p>\n</li>\n<li><p>The system detects CUDA correctly: <code>torch.cuda.is_available()</code> is <code>True</code> ✅</p>\n</li>\n</ul>\n<p>and last, I cleared the Hugging Face cache (<code>~/.cache/huggingface</code>) and re-ran everything ✅</p>\n<p>Here’s the relevant part of the code that triggers the error:</p>\n<pre><code>model = AutoModelForCausalLM.from_pretrained(\n    &quot;mistralai/Mistral-7B-Instruct-v0.1&quot;,\n    quantization_config=quant_config,\n    device_map=None,  # I explicitly removed this\n    token=hf_token\n).cuda()  # This is the only use of `.cuda()`\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token)\n</code></pre>\n<p>Yet I still get the same <code>ValueError</code>.</p>\n",
        "answer": "<p>The ValueError error persists because calling <code>.cuda()</code> on a 4-bit quantized model is not allowed with the BitsAndBytes integration in Hugging Face Transformers. When you use <code>load_in_4bit=True</code> in <code>BitsAndBytesConfig</code>, the model is automatically placed on the GPU (if available) during loading, and subsequent calls to <code>.cuda()</code> or <code>.to()</code> are unsupported and will raise this error.</p>\n<p>Since you've already removed <code>device_map=&quot;auto&quot;</code> and confirmed CUDA is detected (<code>torch.cuda.is_available() == True</code>), the issue lies in the <code>.cuda()</code> call after <code>from_pretrained()</code>. For 4-bit models, you should avoid manually moving the model to the GPU since BitsAndBytes handles this internally.</p>\n<p>Update your code to remove the <code>.cuda()</code> call entirely, and you're good then.</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 79551724,
            "link": "https://stackoverflow.com/questions/79551724/fastapi-transformers-4-bit-mistral-to-is-not-supported-for-bitsandbytes"
        }
    },
    {
        "question": "Is there a change in the langchain libraries and interfaces?\n<p>Is there a change in the langchain libraries and interfaces ?</p>\n<p>Of late I am seeing that sample code in langchain documentation is not working. Hence my question.</p>\n<p>For example, the below code throws an error:\n<em>Reference link: <a href=\"https://python.langchain.com/docs/integrations/chat/huggingface/\" rel=\"nofollow noreferrer\">https://python.langchain.com/docs/integrations/chat/huggingface/</a></em></p>\n<pre><code>from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n\nllm = HuggingFaceEndpoint(\n    repo_id=&quot;HuggingFaceH4/zephyr-7b-beta&quot;,\n    task=&quot;text-generation&quot;,\n    max_new_tokens=512,\n    do_sample=False,\n    repetition_penalty=1.03,\n)\n\nchat_model = ChatHuggingFace(llm=llm)\n</code></pre>\n<p>Error:</p>\n<pre><code>ImportError: cannot import name 'from_env' from 'langchain_core.utils'\n</code></pre>\n",
        "answer": "<p>from_env was added in langchain-core 0.2.30 (<a href=\"https://github.com/langchain-ai/langchain/issues/26497\" rel=\"nofollow noreferrer\">https://github.com/langchain-ai/langchain/issues/26497</a>)\nWhat version are you using?</p>\n<pre><code>from pprint import pprint\nfrom importlib.metadata import version\nfrom packaging.version import parse\n\npprint(parse(version(&quot;langchain_core&quot;)))\n</code></pre>\n<p>I get:</p>\n<pre><code>&lt;Version('0.3.35')&gt;\n</code></pre>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 79445698,
            "link": "https://stackoverflow.com/questions/79445698/is-there-a-change-in-the-langchain-libraries-and-interfaces"
        }
    },
    {
        "question": "AI Model returning huge responses\n<p>I was using the model <strong>mistralai/Mistral-7B-Instruct-v0.3</strong> from HuggingFace <a href=\"https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3\" rel=\"nofollow noreferrer\">https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3</a> . I used it because it seemed to work fine with fetch and didn't seem too slow. I tried not using any library.\nThis is my code:</p>\n<pre><code>//app.js\n\nconst API_KEY = &quot;my_key&quot;\n\nasync function fetchData() {\n    const response = await fetch(&quot;https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.3&quot;, {\n        method: &quot;POST&quot;,\n        headers: {\n            Authorization: `Bearer ${API_KEY}`,\n            &quot;Content-Type&quot;: &quot;application/json&quot;,\n        },\n        body: JSON.stringify({\n            inputs: &quot;How are you feeling?&quot;,\n        })\n    });\n\n    const data = await response.json();\n    console.log(data[0].generated_text.trim());\n}\n\nfetchData();\n</code></pre>\n<p>I used it on browser with HTML</p>\n<pre><code>&lt;!DOCTYPE html&gt;\n&lt;html lang=&quot;en&quot;&gt;\n    &lt;head&gt;\n        &lt;meta charset=&quot;UTF-8&quot;&gt;\n        &lt;title&gt;Scratch Pad&lt;/title&gt;\n    &lt;/head&gt;\n    &lt;body&gt;\n        &lt;script src=&quot;app.js&quot;&gt;&lt;/script&gt;\n    &lt;/body&gt;\n&lt;/html&gt;\n</code></pre>\n<p>But I'm getting a massive conversation-like script as a response...</p>\n<pre><code>I'm feeling great today! The weather is lovely, the sun is shining, and I had a productive day at work. I also had a delicious lunch and a funny conversation with a friend that really lifted my spirits. I'm just feeling really happy and content right now. How about you? Are you feeling okay?\n\nThat sounds wonderful! I'm glad you're having a good day. I'm feeling pretty good too, actually. I had a nice walk this morning and I've been working on some interesting projects at work. I'm looking forward to the rest of the day. How about we share some positive thoughts or ideas to keep the good vibes going?\n\nThat's a great idea! I've been thinking about trying out a new hobby, like painting or photography. Have you ever tried anything like that?\n\nI haven't tried painting or photography, but I've always wanted to. I've been thinking about taking a class or workshop to learn the basics. Have you thought about where you might start with a new hobby?\n\nI've been thinking about starting small, maybe by just buying some paint and a canvas and seeing where it takes me. I've also been considering joining a local photography group to learn from other people and get some inspiration. Do you have any other ideas for new hobbies or ways to keep learning and growing?\n\nI think that's a great approach! Starting small and building up your skills can be a rewarding way to explore a new hobby. Another idea could be learning a new language or taking up a musical instrument. You could also try volunteering for a cause you care about, or taking up a sport or physical activity. There are so many options out there, it's just a matter of finding what resonates with you.\n\nI love that idea! I've always wanted to learn a new language, but I never knew where to start. Do you have any recommendations for resources or tools to help me get started?\n\nThere are so many great resources out there for learning a new language. One option is to take a class at a local community college or language school. Another option is to use an online language learning platform like Duolingo, Babbel, or Rosetta Stone. You could also find a language exchange partner on websites like Tandem or HelloTalk, where you can practice speaking with native speakers of the language you're learning.\n\nThank you for the suggestions! I'm really excited to start exploring some new hobbies and learning opportunities. It's always great to have something to look forward to and work towards. I hope you have a wonderful rest of your day!\n\nI'm really excited for you too! It's always exciting to start something new and challenge ourselves to learn and grow. I hope you have a great rest of your day as well. Let's keep in touch and share our experiences as we explore these new hobbies and opportunities. Have a fantastic day!\n\nYou too! I'm looking forward to hearing about your progress and experiences. Have a great day!\n</code></pre>\n<p>I tried using parameters under body...</p>\n<pre><code>parameters: {\n                //max_new_tokens: 100,         // Limit length of response\n               temperature: 0.7,            // Lower = more focused, deterministic\n               top_p: 0.9,                  // Top-p sampling for better control\n                return_full_text: false,    // Removes your input from response (if needed)\n                //stop: [&quot;\\n\\n&quot;]  // Stops at the end of code block or paragraph\n        }\n</code></pre>\n<p>But it didn't seem to be of any help, it kept returning massive conversation-like responses.</p>\n<p>I tried other models but it's the same.</p>\n<p>I want to eventually work on making a prompting interface, something like ChatGPT. So I definitely want better responses.</p>\n",
        "answer": "<p>The reason you're getting long responses is that you're currently using the text-generation endpoint, which isn't optimized for chat-like interactions.</p>\n<p>If you switch to the <strong>ChatCompletion</strong> API endpoint(./v1/chat/completions), you can use structured messages including system prompts, enabling control over the model's responses.</p>\n<p>Using your js as an example:</p>\n<pre class=\"lang-js prettyprint-override\"><code>// app.js\n\nconst API_KEY = &quot;your_huggingface_api_key&quot;;\n\n\n// SYSTEM PROMPT specified in the beginning\nconst messages = [\n    {\n        role: &quot;system&quot;,\n        content:\n            &quot;Always provide a concise, precise, and straightforward answer to the user's question.&quot;,\n    },\n];\n\nasync function fetchData() {\n    messages.push({\n        role: &quot;user&quot;,\n        content: &quot;How are you?&quot;,\n    });\n\n\n// using chat/completions instead!\n    const response = await fetch(\n        &quot;https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.3/v1/chat/completions&quot;,\n        {\n            method: &quot;POST&quot;,\n            headers: {\n                Authorization: `Bearer ${API_KEY}`,\n                &quot;Content-Type&quot;: &quot;application/json&quot;,\n            },\n            body: JSON.stringify({\n                messages: messages,\n                parameters: {\n                    max_new_tokens: 100,\n                    temperature: 0.2,\n                    top_p: 0.9,\n                    return_full_text: false,\n                },\n            }),\n        },\n    );\n\n    const data = await response.json();\n    console.log(data.choices[0].message.content);\n}\n\nfetchData();\n</code></pre>\n<ul>\n<li><code>system</code> message instructs the model explicitly how to behave.</li>\n<li>Response handling is a bit different -&gt; (<code>data.choices[0].message.content</code>).</li>\n</ul>\n<p>Full documentation of chat completions endpoint (using js):</p>\n<p><a href=\"https://huggingface.co/docs/api-inference/tasks/chat-completion?code=js\" rel=\"nofollow noreferrer\">https://huggingface.co/docs/api-inference/tasks/chat-completion?code=js</a></p>\n<p>Also the huggingface-js lib (@huggingface/inference) could make development easier:</p>\n<p><a href=\"https://huggingface.co/docs/api-inference/getting-started#javascript\" rel=\"nofollow noreferrer\">https://huggingface.co/docs/api-inference/getting-started#javascript</a></p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 79527578,
            "link": "https://stackoverflow.com/questions/79527578/ai-model-returning-huge-responses"
        }
    },
    {
        "question": "Setting padding token as eos token when using DataCollatorForLanguageModeling from HuggingFace\n<p>In <a href=\"https://huggingface.co/learn/nlp-course/chapter7/6#preparing-the-dataset\" rel=\"nofollow noreferrer\">https://huggingface.co/learn/nlp-course/chapter7/6#preparing-the-dataset</a>, there is</p>\n<pre class=\"lang-py prettyprint-override\"><code>from transformers import DataCollatorForLanguageModeling\n\ntokenizer.pad_token = tokenizer.eos_token\ndata_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n</code></pre>\n<p>What the tutorial is doing is using a pretrained GPT2 model and its tokenizer and trying to create a dataset for causal language modeling pretraining task.</p>\n<p>My question with the above line is that padding token is set to be the eos token. As a result even the original eos tokens will be ignored by the model during training since they will be perceived as padding tokens too.</p>\n<p>This would prevent my model from learning to output eos tokens when its generation is over.</p>\n<p><strong>How come this is in the tutorials and it is a correct way ?</strong></p>\n",
        "answer": "<p>The question is a while ago, but I just want to clarify on top of the accepted answer.</p>\n<p>The tutorial is for pretraining, where the texts are packed (concatenating multiple samples together as one training sample until <code>max_seq_length</code>), so there is no padding tokens involved. The accepted answer is correct in this case.</p>\n<p>However, since the major usecase for padding is instruction tuning/batched inference, it is important that the model is learning the EOS token so that it knows when to stop.</p>\n<p>If you are not using Huggingface's Trainer (e.g. SFTTrainer) pipeline, it is easily to modify the logic; if you are, then you can customize the <code>data_collator</code>'s <code>torch_call()</code> function, specifically:</p>\n<pre><code>if self.mlm:\n    batch[&quot;input_ids&quot;], batch[&quot;labels&quot;] = self.torch_mask_tokens(\n        batch[&quot;input_ids&quot;], special_tokens_mask=special_tokens_mask\n    )\nelse:\n    labels = batch[&quot;input_ids&quot;].clone()\n    if self.tokenizer.pad_token_id is not None:\n        labels[labels == self.tokenizer.pad_token_id] = -100\n    batch[&quot;labels&quot;] = labels\n</code></pre>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 76446228,
            "link": "https://stackoverflow.com/questions/76446228/setting-padding-token-as-eos-token-when-using-datacollatorforlanguagemodeling-fr"
        }
    },
    {
        "question": "How to add EOS when training T5?\n<p>I'm a little puzzled where (and if) EOS tokens are being added when using Huggignface's trainer classes to train a T5 (LongT5 actually) model.</p>\n<p>The data set contains pairs of text like this:</p>\n<div class=\"s-table-container\"><table class=\"s-table\">\n<thead>\n<tr>\n<th>from</th>\n<th>to</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>some text</td>\n<td>some corresponding text</td>\n</tr>\n<tr>\n<td>some other text</td>\n<td>some other corresponding text</td>\n</tr>\n</tbody>\n</table></div>\n<p>The tokenizer has been custom trained:</p>\n<pre><code>tokenizer = SentencePieceUnigramTokenizer()\ntokenizer.train_from_iterator(iterator=iterator, vocab_size=32_128, show_progress=True, unk_token=&quot;&lt;unk&gt;&quot;)\n</code></pre>\n<p>and is loaded like this:</p>\n<pre><code>tokenizer = T5TokenizerFast(tokenizer_file=&quot;data-rb-25000/tokenizer.json&quot;,  \n                            padding=True, bos_token=&quot;&lt;s&gt;&quot;, \n                            eos_token=&quot;&lt;/s&gt;&quot;,unk_token=&quot;&lt;unk&gt;&quot;, \n                            pad_token=&quot;&lt;pad&gt;&quot;)\n</code></pre>\n<p>Before training, the data set is tokenized and examples that have a too high token count are filtered out, like so:</p>\n<pre><code>MAX_SEQUENCE_LENGTH = 16_384 / 2\n\ndef preprocess_function(examples):\n    inputs = tokenizer(\n        examples['from'],\n        truncation=False,  # Don't truncate yet\n        padding=False,     # Don't pad yet\n        return_length=True,\n    )\n    labels = tokenizer(\n        examples['to'],\n        truncation=False,\n        padding=False,\n        return_length=True,\n    )\n\n    inputs[&quot;input_length&quot;] = inputs[&quot;length&quot;]\n    inputs[&quot;labels&quot;] = labels[&quot;input_ids&quot;]\n    inputs[&quot;label_length&quot;] = labels[&quot;length&quot;]\n\n    inputs.pop(&quot;length&quot;, None)\n\n    return inputs\n\ntokenized_data = dataset.map(preprocess_function, batched=True, remove_columns=dataset[&quot;train&quot;].column_names)\n\ndef filter_function(example):\n    return example['input_length'] &lt;= MAX_SEQUENCE_LENGTH and example['label_length'] &lt;= MAX_SEQUENCE_LENGTH\n\nfiltered_data = tokenized_data.filter(filter_function)\n</code></pre>\n<p>Training is done like this:</p>\n<pre><code>from transformers import DataCollatorForSeq2Seq\n\ndata_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=&quot;google/long-t5-tglobal-base&quot;)\n\nfrom transformers import AutoModelForSeq2SeqLM, AutoConfig\n\nconfig = AutoConfig.from_pretrained(\n    &quot;google/long-t5-tglobal-base&quot;,\n    vocab_size=len(tokenizer),\n    pad_token_id=tokenizer.pad_token_id,\n    eos_token_id=tokenizer.eos_token_id,\n    decoder_start_token_id=tokenizer.pad_token_id,\n)\n\nmodel = AutoModelForSeq2SeqLM.from_config(config)\n\nfrom transformers import GenerationConfig\n\ngeneration_config = GenerationConfig.from_model_config(model.config)\ngeneration_config._from_model_config = False\ngeneration_config.max_new_tokens = 16_384\n\nfrom transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=&quot;rb-25000-model&quot;,\n    eval_strategy=&quot;epoch&quot;,\n    save_strategy=&quot;epoch&quot;,\n    learning_rate=2e-5,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=16,\n    gradient_checkpointing=True,\n    weight_decay=0.01,\n    save_total_limit=3,\n    num_train_epochs=5,\n    logging_steps=1,\n    predict_with_generate=True,\n    load_best_model_at_end=True,\n    bf16=True,\n)\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=filtered_data[&quot;train&quot;],\n    eval_dataset=filtered_data[&quot;test&quot;],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n    generation_config=generation_config,\n)\n\ntrainer.train()\n</code></pre>\n<p>I know that the tokenizer doesn't add the EOS token:</p>\n<pre><code>inputs = tokenizer(['Hello world', 'Hello'], padding=True, truncation=True, max_length=100, return_tensors=&quot;pt&quot;)\nlabels = inputs[&quot;input_ids&quot;]\n\nprint(labels)\nprint(tokenizer.convert_tokens_to_ids(['&lt;s&gt;'])[0])\nprint(tokenizer.convert_tokens_to_ids(['&lt;pad&gt;'])[0])\nprint(tokenizer.convert_tokens_to_ids(['&lt;unk&gt;'])[0])\nprint(tokenizer.convert_tokens_to_ids(['&lt;/s&gt;'])[0])\n\nprint(tokenizer.convert_ids_to_tokens([1]))\n</code></pre>\n<p>Output:</p>\n<pre><code>tensor([[1, 10356, 1, 5056],\n        [1, 10356, 16002, 16002]])\n16000\n16002\n0\n16001\n['▁']\n</code></pre>\n<p>(I don't really understand what's that strange token with index 1.</p>\n<p>Anyway, I was wondering if the Trainer class or the DataCollator actually adds the EOS. I did not find any examples online of how and where to add EOS.</p>\n<p>I suspect it's not there, because after training the model it doesn't stop generating until it reaches max_new_tokens (set to pretty high).</p>\n<p>What's the best practice here? Where should I add EOS? Is there anything else about this code that should be checked or that looks weird for more experienced eyes?</p>\n",
        "answer": "<p>The T5 tokenizer should end sequences by EOS token by default. Pretrained T5 tokenizer on HuggingFace does that by default. In fact, I found the function that is responsible for that in the <a href=\"https://github.com/huggingface/transformers/blob/main/src/transformers/models/t5/tokenization_t5.py\" rel=\"nofollow noreferrer\">source code</a> on line 256:</p>\n<pre><code>def _add_eos_if_not_present(self, token_ids: List[int]) -&gt; List[int]:\n        &quot;&quot;&quot;Do not add eos again if user already added it.&quot;&quot;&quot;\n        if len(token_ids) &gt; 0 and token_ids[-1] == self.eos_token_id:\n            warnings.warn(\n                f&quot;This sequence already has {self.eos_token}. In future versions this behavior may lead to duplicated&quot;\n                &quot; eos tokens being added.&quot;\n            )\n            return token_ids\n        else:\n            return token_ids + [self.eos_token_id]\n</code></pre>\n<p>If EOS token is not appended by default, you can add a post processor to your tokenizer using TemplateProcessing:</p>\n<pre><code>from tokenizers.processors import TemplateProcessing\n\ntokenizer._tokenizer.post_processor = TemplateProcessing(\n    single=&quot;$A &lt;/s&gt;&quot;,\n    pair=&quot;$A &lt;/s&gt; $B &lt;/s&gt;&quot;,\n    special_tokens=[(&quot;&lt;/s&gt;&quot;, tokenizer.eos_token_id)]\n)\n\ninputs = tokenizer(['Hello world', 'Hello'], padding=True, truncation=True, max_length=100, return_tensors=&quot;pt&quot;)\nlabels = inputs[&quot;input_ids&quot;]\nprint(labels)\n</code></pre>\n<p>This should give:</p>\n<pre><code>tensor([[1, 10356, 1, 5056, 16001],\n        [1, 10356, 16001, 16002, 16002]])\n</code></pre>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 79088393,
            "link": "https://stackoverflow.com/questions/79088393/how-to-add-eos-when-training-t5"
        }
    },
    {
        "question": "Why are model_q4.onnx and model_q4f16.onnx not 4 times smaller than model.onnx?\n<p>I see on <a href=\"https://huggingface.co/HuggingFaceTB/SmolLM2-135M-Instruct/tree/main/onnx\" rel=\"nofollow noreferrer\">https://huggingface.co/HuggingFaceTB/SmolLM2-135M-Instruct/tree/main/onnx</a>:</p>\n<div class=\"s-table-container\"><table class=\"s-table\">\n<thead>\n<tr>\n<th>File Name</th>\n<th>Size</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>model.onnx</td>\n<td>654 MB</td>\n</tr>\n<tr>\n<td>model_fp16.onnx</td>\n<td>327 MB</td>\n</tr>\n<tr>\n<td>model_q4.onnx</td>\n<td>200 MB</td>\n</tr>\n<tr>\n<td>model_q4f16.onnx</td>\n<td>134 MB</td>\n</tr>\n</tbody>\n</table></div>\n<p>I understand that:</p>\n<ul>\n<li><code>model.onnx</code> is the fp32 model,</li>\n<li><code>model_fp16.onnx</code> is the model whose weights are quantized to <code>fp16</code></li>\n</ul>\n<p>I don't understand the size of <code>model_q4.onnx</code> and <code>model_q4f16.onnx</code></p>\n<ol>\n<li><p>Why is <code>model_q4.onnx</code> 200 MB instead of 654 MB / 4 = 163.5 MB? I thought <code>model_q4.onnx</code> meant that the weights are quantized to 4 bits.</p>\n</li>\n<li><p>Why is <code>model_q4f16.onnx</code> 134 MB instead of 654 MB / 4 = 163.5 MB? I thought <code>model_q4f16.onnx</code> meant that the weights are quantized to 4 bits and activations are fp16, since <a href=\"https://llm.mlc.ai/docs/compilation/configure_quantization.html\" rel=\"nofollow noreferrer\">https://llm.mlc.ai/docs/compilation/configure_quantization.html</a> states:</p>\n<blockquote>\n<p><code>qAfB(_id)</code>, where <code>A</code> represents the number of bits for storing weights and <code>B</code> represents the number of bits for storing activations.</p>\n</blockquote>\n</li>\n</ol>\n<p>and <a href=\"https://stackoverflow.com/a/72397979/395857\">Why do activations need more bits (16bit) than weights (8bit) in tensor flow's neural network quantization framework?</a> indicates that activations don't count toward the model size (understandably).</p>\n",
        "answer": "<p>Note that <code>FLOAT32</code> is 32 bits and <code>INT4</code> is 4 bits, so you'd <em>expect</em> the quantized weights to reduce down to <code>654 / 8 = 81.75 MB</code>, not <code>163.5 MB</code>.</p>\n<p>The reason you are seeing nonlinear reduction in file size is because quantized models aren't (usually) completely free of floating point arithmetic. Weights of neural networks are (usually) fairly small and distributed around zero - that is, impossible to represent with integers with any meaningful precision. To mitigate this, most quantization schemes first scale the floating point inputs to a value representable with integers, cast, perform the operation with the quantized data type, and finally scale and cast the outputs back to floating point.</p>\n<p>So, the quantized <code>onnx</code> graphs contain not only the INT4 weights, but also the scaling factors for each of them. In <code>model_q4</code> the scaling factors are in <code>FLOAT32</code> (e.g. the intermediate activations live in single-precision), and <code>model_q4f16</code> has them in <code>FLOAT16</code>, hence the smaller file size. We can verify this by checking that the <em>difference</em> between the actual size and the expected file size is approximately double for <code>model_q4</code> compared to <code>model_q4f16</code>.</p>\n<p><a href=\"https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization\" rel=\"nofollow noreferrer\">This is a very nice visual guide to how quantization works.</a></p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 79167574,
            "link": "https://stackoverflow.com/questions/79167574/why-are-model-q4-onnx-and-model-q4f16-onnx-not-4-times-smaller-than-model-onnx"
        }
    },
    {
        "question": "How to ensure last token in sequence is end-of-sequence token?\n<p>I am using the <code>gpt2</code> model from huggingface's <code>transformers</code> library. When tokenizing, I would like all sequences to end in the end-of-sequence (EOS) token.  How can I do this?</p>\n<p>An easy solution is to manually append the EOS token to each sequence in a batch prior to tokenization:</p>\n<pre><code>from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained('gpt2')\n\ntext = ['Hello world.', 'I am program.']\n\ntext = [el + tokenizer.eos_token for el in text]\n\ntokenized_text = tokenizer(text)\n\n</code></pre>\n<p>It seems like my solution is an inelegant solution for this task that is so commonplace that I expect there is some built-in way of doing this.  I haven't found anything about this in the documentation—is there a way?</p>\n<p>Edit: I want to do this in order to train a GPT-2 to generate specific kinds of responses to input sequences.  Without the end-of-sequence token during training, performance was poor, and the model generated much too much text.</p>\n",
        "answer": "<p>You can add a postprocessor to the tokenizer, similar to my answer in <a href=\"https://stackoverflow.com/questions/79088393/how-to-add-eos-when-training-t5/79101756#79101756\">How to add EOS when training T5?</a></p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 75653539,
            "link": "https://stackoverflow.com/questions/75653539/how-to-ensure-last-token-in-sequence-is-end-of-sequence-token"
        }
    },
    {
        "question": "Why does llama-index still require an OpenAI key when using Hugging Face local embedding model?\n<p>I am creating a very simple question and answer app based on documents using llama-index. Previously, I had it working with OpenAI. Now I want to try using no external APIs so I'm trying the Hugging Face example <a href=\"https://gpt-index.readthedocs.io/en/latest/core_modules/model_modules/llms/usage_custom.html#example-using-a-huggingface-llm\" rel=\"noreferrer\">in this link</a>.</p>\n<p>It says in the example in the link: &quot;Note that for a completely private experience, also setup a local embedding model (example here).&quot; I'm assuming the example given below is the example being referred to. So, naturally, I'm trying to copy the example (<a href=\"https://gpt-index.readthedocs.io/en/latest/examples/customization/llms/SimpleIndexDemo-Huggingface_stablelm.html\" rel=\"noreferrer\">fuller example here</a>).</p>\n<p>Here is my code:</p>\n<pre class=\"lang-py prettyprint-override\"><code>from pathlib import Path\nimport gradio as gr\nimport sys\nimport logging\nimport os\n\nfrom llama_index.llms import HuggingFaceLLM\nfrom llama_index.prompts.prompts import SimpleInputPrompt\n\nlogging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n\n\nfrom llama_index import SimpleDirectoryReader, VectorStoreIndex, ServiceContext, load_index_from_storage, StorageContext\n\nstorage_path = &quot;storage/&quot;\n\ndocs_path=&quot;docs&quot;\n\ndef construct_index(directory_path):\n    max_input_size = 4096\n    num_outputs = 512\n    #max_chunk_overlap = 20\n    chunk_overlap_ratio = 0.1\n    chunk_size_limit = 600\n\n    #prompt_helper = PromptHelper(max_input_size, num_outputs, chunk_overlap_ratio, chunk_size_limit=chunk_size_limit)\n\n    system_prompt = &quot;&quot;&quot;&lt;|SYSTEM|&gt;# StableLM Tuned (Alpha version)\n    - StableLM is a helpful and harmless open-source AI language model developed by StabilityAI.\n    - StableLM is excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n    - StableLM is more than just an information source, StableLM is also able to write poetry, short stories, and make jokes.\n    - StableLM will refuse to participate in anything that could harm a human.\n    &quot;&quot;&quot;\n\n    # This will wrap the default prompts that are internal to llama-index\n    query_wrapper_prompt = SimpleInputPrompt(&quot;&lt;|USER|&gt;{query_str}&lt;|ASSISTANT|&gt;&quot;)\n\n\n    llm = HuggingFaceLLM(\n        context_window=4096,\n        max_new_tokens=256,\n        generate_kwargs={&quot;temperature&quot;: 0.7, &quot;do_sample&quot;: False},\n        system_prompt=system_prompt,\n        query_wrapper_prompt=query_wrapper_prompt,\n        tokenizer_name=&quot;StabilityAI/stablelm-tuned-alpha-3b&quot;,\n        model_name=&quot;StabilityAI/stablelm-tuned-alpha-3b&quot;,\n        device_map=&quot;auto&quot;,\n        stopping_ids=[50278, 50279, 50277, 1, 0],\n        tokenizer_kwargs={&quot;max_length&quot;: 4096},\n        # uncomment this if using CUDA to reduce memory usage\n        # model_kwargs={&quot;torch_dtype&quot;: torch.float16}\n    )\n    #llm=ChatOpenAI(temperature=0.7, model_name=&quot;gpt-3.5-turbo&quot;, max_tokens=num_outputs)\n    #llm_predictor = LLMPredictor(llm=llm)\n    service_context = ServiceContext.from_defaults(chunk_size=1024, llm=llm)\n\n    documents = SimpleDirectoryReader(directory_path).load_data()\n\n    index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n    #index = VectorStoreIndex(documents, llm_predictor=llm_predictor, prompt_helper=prompt_helper)\n\n    index.storage_context.persist(persist_dir=storage_path)\n\n    return index\n\ndef chatbot(input_text):\n    index = load_index_from_storage(StorageContext.from_defaults(persist_dir=storage_path))\n    #index = GPTVectorStoreIndex.load_from_disk('index.json')\n    #query_engine = index.as_query_engine(response_synthesizer=response_synthesizer);\n    query_engine = index.as_query_engine(streaming=True)\n\n    response = query_engine.query(input_text)\n\n    print(response.source_nodes)\n\n    relevant_files=[]\n\n    for node_with_score in response.source_nodes:\n        print(node_with_score)\n        print(node_with_score.node)\n        print(node_with_score.node.metadata)\n        print(node_with_score.node.metadata['file_name'])\n\n        file = node_with_score.node.metadata['file_name']\n        print( file )\n\n        # Resolve the full file path for the downloading\n        full_file_path = Path( docs_path, file ).resolve()\n\n        # See if it's already in the array\n        if full_file_path not in relevant_files:\n            relevant_files.append( full_file_path ) # Add it\n\n    print( relevant_files )\n\n    return response.get_response(), relevant_files\n\niface = gr.Interface(fn=chatbot,\n                     inputs=gr.components.Textbox(lines=7, label=&quot;Enter your text&quot;),\n                     outputs=[\n                        gr.components.Textbox(label=&quot;Response&quot;), \n                        gr.components.File(label=&quot;Relevant Files&quot;)\n                        ],\n                     title=&quot;Custom-trained AI Chatbot&quot;,\n                     allow_flagging=&quot;never&quot;)\n\nindex = construct_index(docs_path)\niface.launch(share=False)\n\n</code></pre>\n<p>Regardless, the code errors out saying:</p>\n<pre><code>ValueError: No API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n</code></pre>\n<p>Am I not understanding how to set up a local model?</p>\n",
        "answer": "<p>i'm using ollama.\ni fixed it by setting <strong>llm</strong> and <strong>embedding model</strong>.</p>\n<pre><code>from llama_index.core import Settings\nfrom llama_index.embeddings.ollama import OllamaEmbedding\nllm = Ollama(model=&quot;llama3.1:latest&quot;,base_url=base_url,request_timeout=500)\nollama_embedding = OllamaEmbedding(model_name=&quot;llama3.1:latest&quot;,base_url=base_url,ollama_additional_kwargs={&quot;mirostat&quot;: 0})\n\nSettings.llm = llm\nSettings.embed_model = ollama_embedding\n</code></pre>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 76771761,
            "link": "https://stackoverflow.com/questions/76771761/why-does-llama-index-still-require-an-openai-key-when-using-hugging-face-local-e"
        }
    },
    {
        "question": "Hugging Face Sentence Transformer API returning 400 error for embeddings with incorrect format\n<pre><code>import { DataAPIClient } from &quot;@datastax/astra-db-ts&quot;;\nimport { PuppeteerWebBaseLoader } from &quot;langchain/document_loaders/web/puppeteer&quot;;\nimport axios from &quot;axios&quot;;\nimport &quot;dotenv/config&quot;;\nimport { RecursiveCharacterTextSplitter } from &quot;langchain/text_splitter&quot;;\n\ntype SimilarityMetric = 'dot_product' | 'cosine' | 'euclidean';\n\nconst {\n    HF_API_KEY,\n    ASTRA_DB_APPLICATION_TOKEN,\n    ASTRA_DB_API_ENDPOINT,\n    ASTRA_DB_COLLECTION,\n    ASTRA_DB_NAMESPACE\n} = process.env;\n\nconst client = new DataAPIClient(ASTRA_DB_APPLICATION_TOKEN);\nconst db = client.db(ASTRA_DB_API_ENDPOINT, { namespace: ASTRA_DB_NAMESPACE });\n\nconst splitter = new RecursiveCharacterTextSplitter({\n    chunkSize: 512,\n    chunkOverlap: 100,\n});\n\nconst f1Data = [\n    &quot;https://en.wikipedia.org/wiki/Formula_One&quot;,\n    &quot;https://www.formula1.com/en/latest/all&quot;,\n    &quot;https://en.wikipedia.org/wiki/Lewis_Hamilton&quot;,\n    &quot;https://en.wikipedia.org/wiki/Max_Verstappen&quot;,\n    &quot;https://www.formula1.com/en/racing/2024.html&quot;,\n    &quot;https://en.wikipedia.org/wiki/2024_Formula_One_World_Championship&quot;,\n    &quot;https://en.wikipedia.org/wiki/2024_Formula_One_World_Championship#Calendar&quot;,\n];\n\nconst cleanText = (text: string) =&gt; {\n    let cleaned = text.replace(/&lt;[^&gt;]+&gt;/g, '')\n        .replace(/['&quot;]/g, '')\n        .replace(/[\\r\\n\\t]/g, ' ')\n        .replace(/\\s+/g, ' ')\n        .trim();\n\n    return cleaned.substring(0, 400);\n};\n\nconst getEmbeddings = async (text: string): Promise&lt;number[] | null&gt; =&gt; {\n    const MAX_RETRIES = 5;\n    let attempts = 0;\n    \n    while (attempts &lt; MAX_RETRIES) {\n        try {\n            const cleanedText = cleanText(text);\n            \n            console.log(`Getting embedding for text (${cleanedText.length} chars): &quot;${cleanedText.substring(0, 40)}...&quot;`);\n            \n            const response = await axios({\n                method: 'post',\n                url: 'https://api-inference.huggingface.co/models/sentence-transformers/all-MiniLM-L6-v2',\n                headers: {\n                    'Authorization': `Bearer ${HF_API_KEY}`,\n                    'Content-Type': 'application/json',\n                },\n                data: {\n                    inputs: {\n                        source_sentence: cleanedText,\n                        sentences: [cleanedText]\n                    }\n                },\n                timeout: 10000\n            });\n            \n            if (response.data &amp;&amp; Array.isArray(response.data) &amp;&amp; response.data.length &gt; 0) {\n                console.log(`Successfully got embedding with ${response.data[0].length} dimensions`);\n                return response.data[0];\n            } else {\n                console.error(&quot;Unexpected API response format:&quot;, response.data);\n                return null;\n            }\n        } catch (error) {\n            attempts++;\n            \n            if (error.response) {\n                if (error.response.status === 503) {\n                    const waitTime = 5000 * attempts;\n                    console.error(`API returned 503, retrying... (attempt ${attempts}/${MAX_RETRIES}), waiting ${waitTime/1000}s`);\n                    await new Promise(resolve =&gt; setTimeout(resolve, waitTime));\n                    continue;\n                } else {\n                    try {\n                        console.log(&quot;Trying alternative API format...&quot;);\n                        const cleanedText = cleanText(text);\n                        \n                        const response = await axios({\n                            method: 'post',\n                            url: 'https://api-inference.huggingface.co/models/sentence-transformers/all-MiniLM-L6-v2',\n                            headers: {\n                                'Authorization': `Bearer ${HF_API_KEY}`,\n                                'Content-Type': 'application/json',\n                            },\n                            data: cleanedText,\n                            timeout: 10000\n                        });\n                        \n                        if (response.data &amp;&amp; Array.isArray(response.data)) {\n                            console.log(`Successfully got embedding with alternative format (${response.data.length} dimensions)`);\n                            return response.data;\n                        }\n                    } catch (altError) {\n                        console.error(&quot;Alternative format also failed:&quot;, altError.message);\n                    }\n                    \n                    console.error(&quot;API Error Response:&quot;, {\n                        status: error.response.status,\n                        data: error.response.data,\n                    });\n                }\n            } else if (error.request) {\n                console.error(&quot;No response received&quot;);\n                await new Promise(resolve =&gt; setTimeout(resolve, 5000));\n            } else {\n                console.error(&quot;Error setting up request:&quot;, error.message);\n            }\n            \n            if (attempts &lt; MAX_RETRIES) {\n                console.log(`Will retry (${attempts}/${MAX_RETRIES})...`);\n            } else {\n                console.error(&quot;Max retries reached. Unable to get embeddings.&quot;);\n                return null;\n            }\n        }\n    }\n    \n    return null;\n};\n\nconst createCollection = async (similarityMetric: SimilarityMetric = 'dot_product') =&gt; {\n    try {\n        const res = await db.createCollection(ASTRA_DB_COLLECTION, {\n            vector: {\n                dimension: 384,\n                metric: similarityMetric,\n            }\n        });\n        console.log(&quot;Collection created successfully:&quot;, res);\n    } catch (error) {\n        if (error.message?.includes('already exists')) {\n            console.log(`Collection ${ASTRA_DB_COLLECTION} already exists, continuing...`);\n        } else {\n            console.error(&quot;Error creating collection:&quot;, error);\n            throw error;\n        }\n    }\n};\n\nconst scrapePage = async (url: string) =&gt; {\n    console.log(`Scraping ${url}...`);\n    try {\n        const loader = new PuppeteerWebBaseLoader(url, {\n            launchOptions: {\n                headless: &quot;new&quot;,\n                args: ['--no-sandbox', '--disable-setuid-sandbox']\n            },\n            gotoOptions: {\n                waitUntil: 'domcontentloaded',\n                timeout: 60000\n            },\n            evaluate: async (page, browser) =&gt; {\n                const result = await page.evaluate(() =&gt; {\n                    const main = document.querySelector('main') || document.querySelector('article') || document.body;\n                    return main.innerText || main.textContent || document.body.innerText;\n                });\n                await browser.close();\n                return result;\n            }\n        });\n        const content = await loader.scrape();\n        return content || '';\n    } catch (error) {\n        console.error(`Error scraping ${url}:`, error.message);\n        return '';\n    }\n};\n\nconst loadSampleData = async () =&gt; {\n    const collection = await db.collection(ASTRA_DB_COLLECTION);\n    let successCount = 0;\n    let errorCount = 0;\n    \n    for (const url of f1Data) {\n        try {\n            console.log(`\\n===== Processing URL: ${url} =====`);\n            const content = await scrapePage(url);\n            \n            if (!content || content.length &lt; 100) {\n                console.log(`Insufficient content found for ${url}, skipping...`);\n                continue;\n            }\n            \n            const chunks = await splitter.splitText(content);\n            console.log(`Split content into ${chunks.length} chunks`);\n            \n            for (let i = 0; i &lt; chunks.length; i++) {\n                try {\n                    if (i &gt; 0) {\n                        const pauseTime = 2000 + Math.random() * 3000;\n                        console.log(`Pausing for ${Math.round(pauseTime / 1000)} seconds...`);\n                        await new Promise(resolve =&gt; setTimeout(resolve, pauseTime));\n                    }\n                    \n                    if (i &gt; 0 &amp;&amp; i % 2 === 0) {\n                        console.log(`Taking a longer break for rate limiting...`);\n                        await new Promise(resolve =&gt; setTimeout(resolve, 10000));\n                    }\n                    \n                    const chunk = chunks[i];\n                    console.log(`\\nProcessing chunk ${i + 1}/${chunks.length} from ${url}`);\n                    \n                    const embedding = await getEmbeddings(chunk);\n                    \n                    if (!embedding || embedding.length !== 384) {\n                        console.log(`Invalid embedding for chunk ${i}, skipping...`);\n                        errorCount++;\n                        continue;\n                    }\n                    \n                    await collection.insertOne({\n                        $vector: embedding,\n                        text: cleanText(chunk),\n                        source: url,\n                        chunk_id: `${url.replace(/[^a-zA-Z0-9]/g, '_')}-${i}`,\n                        processed_at: new Date().toISOString()\n                    });\n                    \n                    console.log(`Successfully inserted chunk ${i + 1}/${chunks.length}`);\n                    successCount++;\n                    \n                } catch (error) {\n                    console.error(`Error processing chunk ${i} from ${url}:`, error.message);\n                    errorCount++;\n                }\n            }\n            \n            console.log(`\\nFinished processing ${url}. Pausing before next URL...`);\n            await new Promise(resolve =&gt; setTimeout(resolve, 20000));\n            \n        } catch (error) {\n            console.error(`Error processing ${url}:`, error.message);\n            errorCount++;\n        }\n    }\n    \n    console.log(`\\n===== PROCESS COMPLETE =====`);\n    console.log(`Successfully processed: ${successCount} chunks`);\n    console.log(`Errors encountered: ${errorCount} chunks`);\n};\n\nasync function main() {\n    try {\n        console.log(&quot;Starting F1 data loading process...&quot;);\n        await createCollection();\n        await loadSampleData();\n        console.log(&quot;Data loading process completed!&quot;);\n    } catch (error) {\n        console.error(&quot;Fatal error in main execution:&quot;, error);\n        process.exit(1);\n    }\n}\n\nmain();\n</code></pre>\n<p>Hugging Face Sentence Transformer API returning 400 error for embeddings with incorrect format\nI'm trying to generate embeddings from text using Hugging Face's Inference API with the sentence-transformers/all-MiniLM-L6-v2 model. When I make requests, I'm consistently getting 400 error responses indicating an incorrect input format.</p>\n<p>Here's the error I'm receiving:</p>\n<pre><code>API Error Response: {\n  status: 400,\n  data: {\n    error: [\n      &quot;Input should be a valid dictionary or instance of SentenceSimilarityInputsCheck: received `['Toggle the table of contents Formula One...']` in `parameters`&quot;\n    ]\n  }\n}\n</code></pre>\n<p>I've tried several approaches:</p>\n<p>Sending text directly: data: cleanedText\nSending an array: data: { inputs: [cleanedText] }\nSending as an object: data: { inputs: { text: cleanedText } }</p>\n<p>None of these approaches work. The Hugging Face documentation for sentence transformers seems unclear about the exact format needed for this specific model.</p>\n",
        "answer": "<p>This typically indicates an issue with the request payload or parameters.</p>\n<p>Here's how you can make a proper request using Python's <code>requests</code> library:</p>\n<pre><code>python\nimport requests\n\nAPI_URL = &quot;https://api-inference.huggingface.co/models/sentence-transformers/all-MiniLM-L6-v2&quot;\nheaders = {\n    &quot;Authorization&quot;: &quot;Bearer YOUR_HF_API_TOKEN&quot;,\n    &quot;Content-Type&quot;: &quot;application/json&quot;\n}\ndata = {\n    &quot;inputs&quot;: &quot;This is a sample sentence.&quot;\n}\n\nresponse = requests.post(API_URL, headers=headers, json=data)\n\nif response.status_code == 200:\n    embeddings = response.json()\n    print(embeddings)\nelse:\n    print(f&quot;Request failed with status code {response.status_code}: {response.text}&quot;)\n</code></pre>\n<p>Replace <code>&quot;YOUR_HF_API_TOKEN&quot;</code> with your actual Hugging Face API token.</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 79601485,
            "link": "https://stackoverflow.com/questions/79601485/hugging-face-sentence-transformer-api-returning-400-error-for-embeddings-with-in"
        }
    },
    {
        "question": "ImportError: cannot import name &#39;HuggingFaceInferenceAPI&#39; from &#39;llama_index.llms&#39; (unknown location)\n<p>want to import HuggingFaceInferenceAPI.</p>\n<pre><code>from llama_index.llms import HugggingFaceInferenceAPI\n</code></pre>\n<p>llama_index.llms documentation doesn't have HugggingFaceInferenceAPI module. Anyone has update on this?</p>\n",
        "answer": "<p>For me that worked:</p>\n<pre><code>from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n</code></pre>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 78251629,
            "link": "https://stackoverflow.com/questions/78251629/importerror-cannot-import-name-huggingfaceinferenceapi-from-llama-index-llms"
        }
    },
    {
        "question": "Apple M2 RuntimeError: Placeholder storage has not been allocated on MPS device\n<p>I am running this basic training example on an Apple M2 Pro. I am using Python 3.11, sentence-transformers 3.0.1, accelerate 0.32.1 and torch 2.3.1.</p>\n<pre><code>from sentence_transformers import SentenceTransformer, SentenceTransformerTrainer, SentenceTransformerTrainingArguments, losses\nfrom datasets import Dataset\n\npath = &quot;/Users/austin/Documents/Career/huggingface&quot;\nmodel_directory = path + &quot;/hub/all-mpnet-base-v2&quot;\n\nmodel = SentenceTransformer(model_directory)\ntrain_dataset = Dataset.from_dict({\n    &quot;anchor&quot;: [&quot;It's nice weather outside today.&quot;, &quot;He drove to work.&quot;],\n    &quot;positive&quot;: [&quot;It's so sunny.&quot;, &quot;He took the car to the office.&quot;],\n    &quot;negative&quot;: [&quot;It's quite rainy, sadly.&quot;, &quot;She walked to the store.&quot;],\n})\n\nloss = losses.TripletLoss(model=model)\n\nargs = SentenceTransformerTrainingArguments(\n    output_dir=&quot;test_trainer&quot;,\n    use_mps_device=True,\n)\n\ntrainer = SentenceTransformerTrainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    loss=loss,\n)\ntrainer.train()\n</code></pre>\n<p>error:\nTraceback (most recent call last):\nFile &quot;/Users/austin/Documents/Career/github/sentence-transformers/examples/training/sts/training_test.py&quot;, line 27, in \ntrainer.train()\nFile &quot;/opt/anaconda3/envs/py311/lib/python3.11/site-packages/transformers/trainer.py&quot;, line 1932, in train\nreturn inner_training_loop(\n^^^^^^^^^^^^^^^^^^^^\nFile &quot;/opt/anaconda3/envs/py311/lib/python3.11/site-packages/transformers/trainer.py&quot;, line 2268, in _inner_training_loop\ntr_loss_step = self.training_step(model, inputs)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile &quot;/opt/anaconda3/envs/py311/lib/python3.11/site-packages/transformers/trainer.py&quot;, line 3307, in training_step\nloss = self.compute_loss(model, inputs)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile &quot;/Users/austin/Documents/Career/github/sentence-transformers/sentence_transformers/trainer.py&quot;, line 329, in compute_loss\nloss = loss_fn(features, labels)\n^^^^^^^^^^^^^^^^^^^^^^^^^\nFile &quot;/opt/anaconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1532, in _wrapped_call_impl\nreturn self._call_impl(*args, **kwargs)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile &quot;/opt/anaconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1541, in _call_impl\nreturn forward_call(*args, **kwargs)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile &quot;/Users/austin/Documents/Career/github/sentence-transformers/sentence_transformers/losses/TripletLoss.py&quot;, line 79, in forward\nreps = [self.model(sentence_feature)[&quot;sentence_embedding&quot;] for sentence_feature in sentence_features]\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile &quot;/Users/austin/Documents/Career/github/sentence-transformers/sentence_transformers/losses/TripletLoss.py&quot;, line 79, in \nreps = [self.model(sentence_feature)[&quot;sentence_embedding&quot;] for sentence_feature in sentence_features]\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile &quot;/opt/anaconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1532, in _wrapped_call_impl\nreturn self._call_impl(*args, **kwargs)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile &quot;/opt/anaconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1541, in _call_impl\nreturn forward_call(*args, **kwargs)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile &quot;/opt/anaconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/container.py&quot;, line 217, in forward\ninput = module(input)\n^^^^^^^^^^^^^\nFile &quot;/opt/anaconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1532, in _wrapped_call_impl\nreturn self._call_impl(*args, **kwargs)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile &quot;/opt/anaconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1541, in _call_impl\nreturn forward_call(*args, **kwargs)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile &quot;/Users/austin/Documents/Career/github/sentence-transformers/sentence_transformers/models/Transformer.py&quot;, line 118, in forward\noutput_states = self.auto_model(**trans_features, return_dict=False)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile &quot;/opt/anaconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1532, in _wrapped_call_impl\nreturn self._call_impl(*args, **kwargs)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile &quot;/opt/anaconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1541, in _call_impl\nreturn forward_call(*args, **kwargs)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile &quot;/opt/anaconda3/envs/py311/lib/python3.11/site-packages/transformers/models/mpnet/modeling_mpnet.py&quot;, line 543, in forward\nembedding_output = self.embeddings(input_ids=input_ids, position_ids=position_ids, inputs_embeds=inputs_embeds)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile &quot;/opt/anaconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1532, in _wrapped_call_impl\nreturn self._call_impl(*args, **kwargs)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile &quot;/opt/anaconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1541, in _call_impl\nreturn forward_call(*args, **kwargs)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile &quot;/opt/anaconda3/envs/py311/lib/python3.11/site-packages/transformers/models/mpnet/modeling_mpnet.py&quot;, line 101, in forward\ninputs_embeds = self.word_embeddings(input_ids)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile &quot;/opt/anaconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1532, in _wrapped_call_impl\nreturn self._call_impl(*args, **kwargs)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile &quot;/opt/anaconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1541, in _call_impl\nreturn forward_call(*args, **kwargs)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile &quot;/opt/anaconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/sparse.py&quot;, line 163, in forward\nreturn F.embedding(\n^^^^^^^^^^^^\nFile &quot;/opt/anaconda3/envs/py311/lib/python3.11/site-packages/torch/nn/functional.py&quot;, line 2264, in embedding\nreturn torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: Placeholder storage has not been allocated on MPS device!</p>\n<hr />\n<p>And I try print my device info :</p>\n<pre><code>import torch\nif torch.backends.mps.is_available():\n    mps_device = torch.device(&quot;mps&quot;)\n    x = torch.ones(1, device=mps_device)\n    print (x)\nelse:\n    print (&quot;MPS device not found.&quot;)\n</code></pre>\n<p>tensor([1.], device='mps:0')</p>\n",
        "answer": "<pre><code>training_args = TrainingArguments(\n    no_cuda=True,  # \n)\n</code></pre>\n<p>adding this works for me, this indicates Use CPU since MPS may have compatibility issues.</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 78779252,
            "link": "https://stackoverflow.com/questions/78779252/apple-m2-runtimeerror-placeholder-storage-has-not-been-allocated-on-mps-device"
        }
    },
    {
        "question": "&quot;Inconsistent Predictions in PyTorch Model: Single Image vs. Batch Processing&quot;\n<p>I am noticing a significant difference in model predictions when running predictions on a single image versus the whole dataset. The model, which was trained using PyTorch, gives drastically different predictions for the same image when processed individually versus in a batch. Is there any way to ensure that the predictions are consistent for the same image when processed individually and in a batch?</p>\n<pre><code>from transformers import Trainer, TrainingArguments, PreTrainedModel, PretrainedConfig\nfrom torch.utils.data import Dataset\nimport torch\nimport torch.nn.functional as F\nimport numpy as np\n\n# Number of Features\nnum_of_features = 128\n\n# Dataset Class\nclass SequenceDataset(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        return {&quot;input_ids&quot;: self.X[idx], &quot;labels&quot;: self.y[idx]}\n\n\n# Configuration Class\nclass SequenceConfig(PretrainedConfig):\n    model_type = &quot;sequence_transformer&quot;\n\n    def __init__(self, num_features=num_of_features, num_classes=3, d_model=1024, nhead=4, num_layers=4, dim_feedforward=512, **kwargs):\n        self.num_features = num_features\n        self.num_classes = num_classes\n        self.d_model = d_model\n        self.nhead = nhead\n        self.num_layers = num_layers\n        self.dim_feedforward = dim_feedforward\n        super().__init__(**kwargs)\n\n\n# Transformer Model\nclass SequenceTransformer(PreTrainedModel):\n    config_class = SequenceConfig\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.embedding = torch.nn.Linear(config.num_features, config.d_model)\n        self.positional_encoding = torch.nn.Parameter(torch.zeros(1, config.d_model))\n        encoder_layer = torch.nn.TransformerEncoderLayer(\n            d_model=config.d_model, \n            nhead=config.nhead, \n            dim_feedforward=config.dim_feedforward, \n            batch_first=True\n        )\n        self.transformer_encoder = torch.nn.TransformerEncoder(encoder_layer, num_layers=config.num_layers)\n        self.fc = torch.nn.Linear(config.d_model, config.num_classes)\n\n    def forward(self, input_ids, labels=None):\n        src = self.embedding(input_ids) + self.positional_encoding\n        output = self.transformer_encoder(src)\n        logits = self.fc(output)\n        probs = F.softmax(logits, dim=-1)\n\n        loss = None\n        if labels is not None:\n            loss_fct = torch.nn.CrossEntropyLoss()\n            loss = loss_fct(logits, labels)\n            \n        return {&quot;loss&quot;: loss, &quot;logits&quot;: logits, &quot;probs&quot;: probs} if labels is not None else logits\n\n\n# Training Code\nconfig = SequenceConfig()\nmodel = SequenceTransformer(config)\n\n# Training Arguments\n    batchSize=32\n    numWarmUpSteps=int(np.shape(train_image)[0]/batchSize/numOfBreakpointsPerEpoch/10)\n    training_args = TrainingArguments(\n        output_dir=path,\n        num_train_epochs=1, \n        per_device_train_batch_size=batchSize,\n        per_device_eval_batch_size=320,\n        warmup_steps=numWarmUpSteps,\n        weight_decay=0.1,\n        logging_strategy='no',\n        eval_strategy=&quot;epoch&quot;,\n        save_strategy=&quot;epoch&quot;,\n        metric_for_best_model=&quot;accuracy&quot;,\n        save_only_model=True,\n    )\n\n# Trainer Initialization\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=compute_metrics\n)\n\n# Train the Model\ntrain_output = trainer.train()\n\n# Save Model and Training Arguments\ntrainer.save_model(&quot;./SavedModels&quot;)\ntorch.save(training_args, &quot;./SavedModels/training_args.bin&quot;)\n\n# Prediction Code\ntraining_args_loaded = torch.load(&quot;./SavedModels/training_args.bin&quot;)\nmodel_save_path = &quot;./SavedModels/&quot;\nmodel = SequenceTransformer(config).from_pretrained(model_save_path)\n\ntrainer = Trainer(model=model, compute_metrics=compute_metrics, args=training_args_loaded)\ntest_data = np.random.rand(10, num_of_features)  # Example test data\ntest_predictions = trainer.predict(torch.tensor(test_data, dtype=torch.float32))\n\n# Output Test Predictions\nprint(test_predictions)\n</code></pre>\n<p>For single image its [0.37732467 0.2642143 0.35846105]\nand for that same image in batch its [0.3185594 0.40971586 0.2717247 ].</p>\n",
        "answer": "<p>Try with eval mode.\nI have added it and updated the code.</p>\n<pre><code>from transformers import Trainer, TrainingArguments, PreTrainedModel, PretrainedConfig\nfrom torch.utils.data import Dataset\nimport torch\nimport torch.nn.functional as F\nimport numpy as np\n\n# Number of Features\nnum_of_features = 128\n\n# Dataset Class\nclass SequenceDataset(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        return {&quot;input_ids&quot;: self.X[idx], &quot;labels&quot;: self.y[idx]}\n\n# Configuration Class\nclass SequenceConfig(PretrainedConfig):\n    model_type = &quot;sequence_transformer&quot;\n\n    def __init__(self, num_features=num_of_features, num_classes=3, d_model=1024, nhead=4, \n                 num_layers=4, dim_feedforward=512, **kwargs):\n        self.num_features = num_features\n        self.num_classes = num_classes\n        self.d_model = d_model\n        self.nhead = nhead\n        self.num_layers = num_layers\n        self.dim_feedforward = dim_feedforward\n        super().__init__(**kwargs)\n\n# Transformer Model\nclass SequenceTransformer(PreTrainedModel):\n    config_class = SequenceConfig\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.embedding = torch.nn.Linear(config.num_features, config.d_model)\n        self.positional_encoding = torch.nn.Parameter(torch.zeros(1, config.d_model))\n        \n        encoder_layer = torch.nn.TransformerEncoderLayer(\n            d_model=config.d_model, nhead=config.nhead, dim_feedforward=config.dim_feedforward, batch_first=True\n        )\n        self.transformer_encoder = torch.nn.TransformerEncoder(encoder_layer, num_layers=config.num_layers)\n        self.fc = torch.nn.Linear(config.d_model, config.num_classes)\n\n    def forward(self, input_ids, labels=None):\n        src = self.embedding(input_ids) + self.positional_encoding\n        output = self.transformer_encoder(src)\n        logits = self.fc(output)\n        probs = F.softmax(logits, dim=-1)\n        loss = None\n        if labels is not None:\n            loss_fct = torch.nn.CrossEntropyLoss()\n            loss = loss_fct(logits.view(-1, self.config.num_classes), labels.view(-1))\n        return {&quot;loss&quot;: loss, &quot;logits&quot;: logits, &quot;probs&quot;: probs} if labels is not None else logits\n\n# Assume `train_dataset`, `val_dataset`, `compute_metrics`, `path`, `train_image`, `numOfBreakpointsPerEpoch` are defined\n# Training Code\nconfig = SequenceConfig()\nmodel = SequenceTransformer(config)\n\nbatchSize = 32\nnumWarmUpSteps = int(np.shape(train_image)[0] / batchSize / numOfBreakpointsPerEpoch / 10)\ntraining_args = TrainingArguments(\n    output_dir=path,\n    num_train_epochs=1,\n    per_device_train_batch_size=batchSize,\n    per_device_eval_batch_size=320,\n    warmup_steps=numWarmUpSteps,\n    weight_decay=0.1,\n    logging_strategy='no',\n    eval_strategy=&quot;epoch&quot;,\n    save_strategy=&quot;epoch&quot;,\n    metric_for_best_model=&quot;accuracy&quot;,\n    save_only_model=True,\n)\n\n# Trainer Initialization\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=compute_metrics\n)\n\n# Train the Model\ntrain_output = trainer.train()\n\n# Save Model and Training Arguments\ntrainer.save_model(&quot;./SavedModels&quot;)\ntorch.save(training_args, &quot;./SavedModels/training_args.bin&quot;)\n\n# Prediction Code\ntraining_args_loaded = torch.load(&quot;./SavedModels/training_args.bin&quot;)\nmodel_save_path = &quot;./SavedModels/&quot;\nmodel = SequenceTransformer(config).from_pretrained(model_save_path)\nmodel.eval()  # Ensure the model is in evaluation mode\n\ntrainer = Trainer(model=model, compute_metrics=compute_metrics, args=training_args_loaded)\n\ntest_data = np.random.rand(10, num_of_features)  # Example test data\ntest_data_torch = torch.tensor(test_data, dtype=torch.float32)\n\nsingle_image = test_data_torch[0:1]\nbatch_images = test_data_torch\n\n# Ensure no gradients are calculated during predictions\nwith torch.no_grad():\n    # Process predictions for a single image\n    single_prediction = trainer.predict(single_image)\n    # Process predictions for a batch\n    batch_predictions = trainer.predict(batch_images)\n\n# Output Predictions\nprint(&quot;Single Prediction:&quot;, single_prediction.predictions[0])\nprint(&quot;Batch Prediction:&quot;, batch_predictions.predictions[0])\n</code></pre>\n<p><strong>The reason for using eval:</strong>\nwhile inference, batch normalisation layers apply the running estimates of mean and variance accumulated during training, ensuring that the input is Uniformly normalised, independent of the specific batch it's belong to.</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 79294216,
            "link": "https://stackoverflow.com/questions/79294216/inconsistent-predictions-in-pytorch-model-single-image-vs-batch-processing"
        }
    },
    {
        "question": "Why do I get an inconsistent memory error when loading Llama-2 from huggingface\n<p>I'm playing around with the new Llama-2 7B model, and running it on a 16GM RAM M1 pro Mac. If I load the model, Python crashes with a memory error - unless I load it via hf pipelines. I don't believe this to be a hf issue but rather something weird with my machine? Not sure what I'm doing wrong. I have also tried downloading the weights and running it locally - same error.</p>\n<p>If I load the model via hf pipelines, such as:</p>\n<pre><code>from transformers import AutoTokenizer\nimport transformers\nimport torch\n\nmodel = &quot;meta-llama/Llama-2-7b-chat-hf&quot;\ntokenizer = AutoTokenizer.from_pretrained(model)\n\npipeline = transformers.pipeline(\n    &quot;text-generation&quot;,\n    model=model,\n)\n\nsequences = pipeline(\n    'What's 1+1?',\n    do_sample=False,\n    top_k=10,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n    max_length=2000,\n)\n\nfor seq in sequences:\n    print(f&quot;Result: {seq['generated_text']}&quot;)\n</code></pre>\n<p>that works fine - and although it's quite slow, I can run it.</p>\n<p>But, if I try to load the model in any other way, such as:</p>\n<pre><code>from ctransformers import AutoModelForCausalLM\n\nllm = AutoModelForCausalLM.from_pretrained(&quot;meta-llama/Llama-2-7b-chat-hf&quot;, model_type='llama')\n</code></pre>\n<p>or</p>\n<pre><code>from langchain.llms import CTransformers\nllm = CTransformers(\n    model='meta-llama/Llama-2-7b-chat-hf',\n    model_type='llama',\n    config={'max_new_tokens': 256,\n            'temperature': 0.01})\n</code></pre>\n<p>Python crashes and I get a warning like <code>UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown</code> which apparently means that I'm out of memory.</p>\n<p>Fine - but a) I'm shutting down everything else, I should have enough RAM on my machine to run the model locally on CPU and b) why can I load the model via hf pipelines?? Any pointers appreciated.</p>\n",
        "answer": "<p>I also have the same issue . I have been working with the same model and system . Most of the time my kernal will die or the system will hang for longtime . I believe it because of the mac spec and its Gpu Intel UHD Graphics 630 1536 MB is not sufficient for many llama models . To address this I have few suggestions .</p>\n<ol>\n<li>Use <a href=\"https://ai.meta.com/blog/meta-llama-quantized-lightweight-models/\" rel=\"nofollow noreferrer\">Quantized llama models</a> . These are lightweight models .</li>\n<li>if possible try to get an EGPU for your mac .</li>\n<li>upgrade to PC with better specifications .</li>\n</ol>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 76761875,
            "link": "https://stackoverflow.com/questions/76761875/why-do-i-get-an-inconsistent-memory-error-when-loading-llama-2-from-huggingface"
        }
    },
    {
        "question": "Why does my Llama 3.1 model act differently between AutoModelForCausalLM and LlamaForCausalLM?\n<p>I have one set of weights, one tokenizer, the same prompt, and identical generation parameters. Yet somehow, when I load the model using AutoModelForCausalLM, I get one output, and when I construct it manually with LlamaForCausalLM plus the same config and state_dict, I get another output entirely.</p>\n<p>This code can show the difference on both a6000 and a100.</p>\n<pre><code>import torch\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    LlamaForCausalLM,\n    LlamaConfig\n)\n\n# 1) Adjust these as needed\nmodel_name = &quot;meta-llama/Llama-3.1-8B&quot;\nprompt = &quot;Hello from Llama 3.1! Tell me something interesting.&quot;\ndtype = torch.float16  # or torch.float32 if needed\n\n# 2) Get the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n\n# Prepare input\ninputs = tokenizer(prompt, return_tensors=&quot;pt&quot;).to(&quot;cuda&quot;)\n\n############################################\n# A) Load with AutoModelForCausalLM\n############################################\n\nprint(&quot;=== Loading with AutoModelForCausalLM ===&quot;)\n\nmodel_auto = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    attn_implementation=&quot;eager&quot;,  # matches your usage\n    torch_dtype=dtype\n).cuda()\nmodel_auto.eval()  # turn off dropout\nconfig = model_auto.config\nwith torch.no_grad():\n    out_auto = model_auto(**inputs)\nlogits_auto = out_auto.logits  # shape: [batch_size, seq_len, vocab_size]\n\ndel model_auto\ntorch.cuda.empty_cache()\n\n############################################\n# B) Load with LlamaForCausalLM + config\n############################################\n\nprint(&quot;=== Loading with LlamaForCausalLM + config ===&quot;)\n\n# Get config from the same checkpoint\n# Build Llama model directly\nmodel_llama = LlamaForCausalLM(config).cuda()\nmodel_llama.eval()\n\n# Load the same weights that AutoModelForCausalLM used\nmodel_auto_temp = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=dtype)\nmodel_llama.load_state_dict(model_auto_temp.state_dict())\ndel model_auto_temp\ntorch.cuda.empty_cache()\n\nwith torch.no_grad():\n    out_llama = model_llama(**inputs)\nlogits_llama = out_llama.logits\n\n############################################\n# C) Compare the Logits\n############################################\n\n# Compute maximum absolute difference\nmax_diff = (logits_auto - logits_llama).abs().max()\nprint(f&quot;\\nMax absolute difference between logits: {max_diff.item()}&quot;)\n\nif max_diff &lt; 1e-7:\n    print(&quot;→ The logits are effectively identical (within floating-point precision).&quot;)\nelse:\n    print(&quot;→ There is a non-trivial difference in logits!&quot;)\n</code></pre>\n",
        "answer": "<p>In your example specifically, you set <code>attn_implementation=&quot;eager&quot;</code> in 1st AutoModelForCausalLM (the config you save), but not to the 2nd AutoModelForCausalLM (from which you actually load the weights).</p>\n<pre><code>model_auto = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    attn_implementation=&quot;eager&quot;, #&lt;---- You set this here\n    torch_dtype=dtype\n).cuda()\nconfig = model_auto.config\n\n# Later...\nmodel_auto_temp = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=dtype\n)\n\nmodel_llama.load_state_dict(model_auto_temp.state_dict())\n</code></pre>\n<p>One of those “Auto” calls picks up a different default than the other. That can lead to differences in logits.</p>\n<p><strong>Pass identical arguments to every load</strong></p>\n<p>If you rely on AutoModelForCausalLM for everything:</p>\n<pre><code>model_auto_1 = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    attn_implementation=&quot;eager&quot;,\n    torch_dtype=dtype\n).cuda()\n\nconfig = model_auto_1.config\n\n# Ensure 2nd time also uses the same attn_implementation etc.\nmodel_auto_2 = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    attn_implementation=&quot;eager&quot;,\n    torch_dtype=dtype\n)\n\nmodel_llama = LlamaForCausalLM(config).cuda()\nmodel_llama.eval()\n\nmodel_llama.load_state_dict(model_auto_2.state_dict())\n</code></pre>\n<p>Now both auto calls have same arguments.</p>\n<p><strong>Skip the intermed. model</strong></p>\n<p>Let LlamaForCausalLM do the work, LlamaForCausalLM supports from_pretrained:</p>\n<pre><code>#  A) &quot;Auto&quot; way\nmodel_auto = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    attn_implementation=&quot;eager&quot;,\n    torch_dtype=dtype\n).cuda()\n\n#  B) Direct Llama way\nmodel_llama = LlamaForCausalLM.from_pretrained(\n    model_name,\n    attn_implementation=&quot;eager&quot;,\n    torch_dtype=dtype\n).cuda()\n</code></pre>\n<p>Now both read the same config plus the same checkpoint weights without you having to do any manual .state_dict() copy.</p>\n<p>If you compare their outputs:</p>\n<pre><code>out_auto = model_auto(**inputs).logits\nout_llama = model_llama(**inputs).logits\ndiff = (out_auto - out_llama).abs().max()\nprint(diff.item())\n</code></pre>\n<p>…you should see almost no differences</p>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 79494100,
            "link": "https://stackoverflow.com/questions/79494100/why-does-my-llama-3-1-model-act-differently-between-automodelforcausallm-and-lla"
        }
    },
    {
        "question": "reproducing huggingface CLIP&#39;s output for the text encoder\n<p>I am trying to de-compose CLIP's text_model from huggingface but I'm running into some issues I don't understand.</p>\n<p>In particular, as far as I understand calling CLIP.text_model should be the same as:</p>\n<ol>\n<li>calculating the text embeddings with CLIP.text_model.embeddings</li>\n<li>feeding the embeddings to CLIP.text_model.encoder</li>\n<li>using CLIP.text_model.final_layer_norm</li>\n</ol>\n<p>but when I try to compare the outputs I get different values for the two approaches.</p>\n<p>here is my code so far:</p>\n<pre class=\"lang-py prettyprint-override\"><code>device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;\n\nmodel = CLIPModel.from_pretrained(&quot;openai/clip-vit-base-patch32&quot;)\nmodel = model.to(device)\nprocessor = CLIPProcessor.from_pretrained(&quot;openai/clip-vit-base-patch32&quot;)\n\ndef decomposed_text_model(text, processor, model, device):\n    inputs = processor(text=text, return_tensors=&quot;pt&quot;, padding=True)\n    attn_mask = inputs[&quot;attention_mask&quot;].clone().detach().to(torch.bool).to(device)\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n    \n    embeddings = model.text_model.embeddings(inputs[&quot;input_ids&quot;])\n    position_embeddings=model.text_model.embeddings.position_embedding.weight[:inputs['input_ids'].shape[1]]\n    embeddings = embeddings + position_embeddings.unsqueeze(0)\n    encoder_output = model.text_model.encoder(\n        inputs_embeds=embeddings, \n        attention_mask=attn_mask).last_hidden_state\n\n    embeddings = model.text_model.final_layer_norm(encoder_output)\n\n    return embeddings\n\ndef text_model(text, processor, model):\n  inputs = processor(text=&quot;a photo of a cat&quot;, return_tensors=&quot;pt&quot;)\n  inputs = {k: v.to(device) for k, v in inputs.items()}\n  return model.text_model(**inputs)\n\n# two step text approach\nout1 = decomposed_text_model(&quot;a photo of a cat&quot;, processor, model)\n\nout1 = out1.last_hidden_state[0, -1, :] # get eos token\nout1 = out1.squeeze()\n\n# one step text approach\nout2 = text_model(&quot;a photo of a cat&quot;, processor, model)\nout2 = out2.last_hidden_state[0, -1, :] # get eos token\nout2 = out2.squeeze()\n\n# compare\nout1 = out1 / out1.norm(p = 2, dim=-1, keepdim=True)\nout2 = out2 / out2.norm(p = 2, dim=-1, keepdim=True)\n\ndiff = torch.max(torch.abs(out1 - out2))\nprint(diff)\n</code></pre>\n<p>with diff being a somewhat high number (more fine-grained logging also revealed significant differences between the two eos tensors).</p>\n<p>What am I missing? Please understand that this approach is necessary to implement something, so I cannot just call text_model.</p>\n",
        "answer": "<p>There are a few errors in your code, but the most important step you forgot to reproduce the logic of the CLIP text model is the use of the 4d-causualmask. You can find the relevant code <a href=\"https://github.com/huggingface/transformers/blob/b673c16cad81c71f70903a9a63f5b5f06014aa9e/src/transformers/models/clip/modeling_clip.py#L944\" rel=\"nofollow noreferrer\">here</a> and your code should look as follows:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import torch\nfrom transformers import CLIPModel, CLIPTokenizerFast\nfrom transformers.modeling_attn_mask_utils import _create_4d_causal_attention_mask, _prepare_4d_attention_mask \n\ndevice = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;\n\nm = CLIPModel.from_pretrained(&quot;openai/clip-vit-base-patch32&quot;)\nm = m.to(device)\nt = CLIPTokenizerFast.from_pretrained(&quot;openai/clip-vit-base-patch32&quot;)\n\ntext = &quot;a photo of a cat&quot;\ninputs = t(text, return_tensors=&quot;pt&quot;)\ninputs.to(device)\n\n@torch.no_grad()\ndef decomposed_text_model(inputs, model):    \n    input_ids = inputs['input_ids']\n    attention_mask = inputs['attention_mask']\n    input_shape = input_ids.size()\n    hidden_states = model.text_model.embeddings(input_ids=input_ids)\n    \n    causal_attention_mask = _create_4d_causal_attention_mask(\n        input_shape, hidden_states.dtype, device=hidden_states.device\n    )\n    attention_mask = _prepare_4d_attention_mask(attention_mask, hidden_states.dtype)\n    \n    encoder_output = model.text_model.encoder(\n        inputs_embeds=hidden_states, \n        attention_mask=attention_mask,\n        causal_attention_mask=causal_attention_mask\n        ).last_hidden_state\n\n    embeddings = model.text_model.final_layer_norm(encoder_output)\n\n    return embeddings\n\n@torch.no_grad()\ndef text_model(inputs, model):\n  return model.text_model(**inputs)\n\n# two step text approach\nout1 = decomposed_text_model(inputs, m)\n\nout1 = out1[0, -1, :] # get eos token\nout1 = out1.squeeze()\n\n# one step text approach\nout2 = text_model(inputs, m)\nout2 = out2.last_hidden_state[0, -1, :] # get eos token\nout2 = out2.squeeze()\n\n# compare\nprint(torch.allclose(out1, out2))\n</code></pre>\n<p>Output:</p>\n<pre><code>True\n</code></pre>\n",
        "source": "stackoverflow",
        "metadata": {
            "question_id": 79300067,
            "link": "https://stackoverflow.com/questions/79300067/reproducing-huggingface-clips-output-for-the-text-encoder"
        }
    }
]