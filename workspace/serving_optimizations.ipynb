{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f57cb6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "! pip install transformers\n",
    "! pip install torch\n",
    "! pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be975ae",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Terminal: huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d6997c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import evaluate\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5176e3d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# === Load model and tokenizer ===\n",
    "# checkpoint_dir = \"trained_model\"\n",
    "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(base_model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "# Set device (GPU if available, otherwise CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c413f2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import evaluate\n",
    "\n",
    "# === Load QA data ===\n",
    "with open(\"qa_from_commits_formatted.json\", \"r\") as f:\n",
    "    all_data = json.load(f)\n",
    "qa_data = random.sample(all_data, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30244f74",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# === Initialize metric ===\n",
    "squad_metric = evaluate.load(\"squad\")\n",
    "\n",
    "# === Run inference ===\n",
    "latencies = []\n",
    "total_tokens = 0\n",
    "\n",
    "print(\"\\nRunning inference on QA pairs...\\n\")\n",
    "for idx, item in enumerate(tqdm(qa_data)):\n",
    "    context = item[\"context\"]\n",
    "    question = item[\"question\"]\n",
    "    expected = item[\"answer\"]\n",
    "    qid = str(idx)\n",
    "\n",
    "    # LLaMA 3 prompt format with extractive QA instruction\n",
    "    prompt = (\n",
    "        \"<|begin_of_text|>\"\n",
    "        \"<|start_header_id|>system<|end_header_id|>\\n\"\n",
    "        \"You are a concise assistant. Answer only using direct quotes from the context. Do not explain.\\n\"\n",
    "        \"<|eot_id|>\"\n",
    "        \"<|start_header_id|>user<|end_header_id|>\\n\"\n",
    "        f\"Context:\\n{context}\\n\\nQuestion: {question}\\n\"\n",
    "        \"<|eot_id|>\"\n",
    "        \"<|start_header_id|>assistant<|end_header_id|>\"\n",
    "    )\n",
    "\n",
    "    # Tokenize and move to device\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Generate\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=50,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Measure latency\n",
    "    latency = end_time - start_time\n",
    "    latencies.append(latency)\n",
    "\n",
    "    # Decode only newly generated tokens\n",
    "    input_len = inputs[\"input_ids\"].shape[-1]\n",
    "    generated_tokens = outputs[0][input_len:]\n",
    "    generated_answer = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n",
    "    total_tokens += generated_tokens.shape[-1]\n",
    "\n",
    "    # Print each QA pair\n",
    "    print(f\"\\n--- QA Pair {idx} ---\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Generated Answer: {generated_answer}\")\n",
    "    print(f\"Reference Answer: {expected}\")\n",
    "\n",
    "    # Format for squad metric\n",
    "    squad_metric.add(\n",
    "        prediction={\"id\": qid, \"prediction_text\": generated_answer},\n",
    "        reference={\"id\": qid, \"answers\": {\"text\": [expected], \"answer_start\": [0]}}\n",
    "    )\n",
    "\n",
    "# === Compute metrics ===\n",
    "results = squad_metric.compute()\n",
    "em_score = results[\"exact_match\"]\n",
    "f1_score = results[\"f1\"]\n",
    "avg_latency = sum(latencies) / len(latencies)\n",
    "p95_latency = sorted(latencies)[int(0.95 * len(latencies))]\n",
    "throughput = len(qa_data) / sum(latencies)\n",
    "token_throughput = total_tokens / sum(latencies)\n",
    "\n",
    "# === Print results ===\n",
    "print(f\"\\n--- Benchmark Results (n={len(qa_data)}) ---\")\n",
    "print(f\"Accuracy (EM):           {em_score:.2f}\")\n",
    "print(f\"F1 Score:                {f1_score:.2f}\")\n",
    "print(f\"Avg Latency:             {avg_latency:.3f} sec\")\n",
    "print(f\"P95 Latency:             {p95_latency:.3f} sec\")\n",
    "print(f\"Throughput:              {throughput:.2f} samples/sec\")\n",
    "print(f\"Token Throughput:        {token_throughput:.2f} tokens/sec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cdc429",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#### ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657eb3de",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "! python -m pip install onnx onnxruntime-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d2d340",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class Wrapper(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        return self.model(input_ids=input_ids, attention_mask=attention_mask).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f416c12",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Save as ONNX\n",
    "# import torch\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Disable caching to avoid DynamicCache errors\n",
    "model.config.use_cache = False\n",
    "\n",
    "# Define dummy input\n",
    "inputs = tokenizer(\"Hello\", return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "# Use a wrapper to isolate the exact outputs\n",
    "class Wrapper(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        return self.model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "\n",
    "# Export\n",
    "torch.onnx.export(\n",
    "    Wrapper(model),\n",
    "    (input_ids, attention_mask),\n",
    "    \"Llama-Instruct.onnx\",\n",
    "    input_names=[\"input_ids\", \"attention_mask\"],\n",
    "    output_names=[\"logits\"],\n",
    "    dynamic_axes={\n",
    "        \"input_ids\": {0: \"batch\", 1: \"seq\"},\n",
    "        \"attention_mask\": {0: \"batch\", 1: \"seq\"},\n",
    "        \"logits\": {0: \"batch\", 1: \"seq\"}\n",
    "    },\n",
    "    opset_version=14\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40ea489",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def onnx_generate(prompt, max_new_tokens=50):\n",
    "    encoded = tokenizer(prompt, return_tensors=\"np\")\n",
    "    input_ids = encoded[\"input_ids\"].astype(np.int64)\n",
    "    attention_mask = encoded[\"attention_mask\"].astype(np.int64)\n",
    "\n",
    "    generated_ids = input_ids\n",
    "    generated_mask = attention_mask\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        outputs = session.run([\"logits\"], {\n",
    "            \"input_ids\": generated_ids,\n",
    "            \"attention_mask\": generated_mask\n",
    "        })\n",
    "        logits = outputs[0]\n",
    "\n",
    "        # Greedy decode: take highest scoring token from last position\n",
    "        next_token_logits = logits[:, -1, :]\n",
    "        next_token_id = np.argmax(next_token_logits, axis=-1).reshape(1, 1)\n",
    "\n",
    "        # Append to inputs\n",
    "        generated_ids = np.concatenate([generated_ids, next_token_id], axis=1)\n",
    "        next_mask = np.ones_like(next_token_id)\n",
    "        generated_mask = np.concatenate([generated_mask, next_mask], axis=1)\n",
    "\n",
    "        if next_token_id.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "    return tokenizer.decode(generated_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d09f8c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import onnxruntime\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "onnx_path = \"Llama-Instruct.onnx\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# session = onnxruntime.InferenceSession(onnx_path)\n",
    "session = onnxruntime.InferenceSession(onnx_path, providers=['CUDAExecutionProvider'])\n",
    "\n",
    "output_text = onnx_generate(\"Once upon a time\", 20)\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d0dc44",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "onnxruntime.get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941e4a61",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "onnxruntime.get_available_providers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74eda9eb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## USE CUDAExecutionProvider\n",
    " \n",
    "\n",
    "# === Initialize metric ===\n",
    "squad_metric = evaluate.load(\"squad\")\n",
    "\n",
    "# === Initialize ONNX session with CUDAExecutionProvider ===\n",
    "import onnxruntime as ort\n",
    "session = ort.InferenceSession(\"Llama-3.1-8B-Instruct.onnx\", providers=[\"CUDAExecutionProvider\"])\n",
    "\n",
    "# === Run inference ===\n",
    "latencies = []\n",
    "total_tokens = 0\n",
    "\n",
    "print(\"\\nRunning inference on QA pairs using ONNX model...\\n\")\n",
    "for idx, item in enumerate(tqdm(qa_data)):\n",
    "    context = item[\"context\"]\n",
    "    question = item[\"question\"]\n",
    "    expected = item[\"answer\"]\n",
    "    qid = str(idx)\n",
    "\n",
    "    # LLaMA 3 extractive QA prompt\n",
    "    prompt = (\n",
    "        \"<|begin_of_text|>\"\n",
    "        \"<|start_header_id|>system<|end_header_id|>\\n\"\n",
    "        \"You are a concise assistant. Answer only using direct quotes from the context. Do not explain.\\n\"\n",
    "        \"<|eot_id|>\"\n",
    "        \"<|start_header_id|>user<|end_header_id|>\\n\"\n",
    "        f\"Context:\\n{context}\\n\\nQuestion: {question}\\n\"\n",
    "        \"<|eot_id|>\"\n",
    "        \"<|start_header_id|>assistant<|end_header_id|>\"\n",
    "    )\n",
    "\n",
    "    # Tokenize input to track input length\n",
    "    inputs = tokenizer(prompt, return_tensors=\"np\")\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "    input_length = input_ids.shape[-1]\n",
    "\n",
    "    # Inference using CUDAExecutionProvider\n",
    "    start_time = time.time()\n",
    "    logits = session.run([\"logits\"], {\"input_ids\": input_ids, \"attention_mask\": attention_mask})[0]\n",
    "    next_token_id = logits[0, -1].argmax()\n",
    "    output_ids = np.append(input_ids[0], next_token_id)\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Extract only generated token\n",
    "    generated_ids = output_ids[input_length:]\n",
    "    generated_answer = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "    latency = end_time - start_time\n",
    "    latencies.append(latency)\n",
    "    total_tokens += len(generated_ids)\n",
    "\n",
    "    # Print QA pair\n",
    "    print(f\"\\n--- QA Pair {idx} ---\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Generated Answer: {generated_answer}\")\n",
    "    print(f\"Reference Answer: {expected}\")\n",
    "\n",
    "    # Evaluate\n",
    "    squad_metric.add(\n",
    "        prediction={\"id\": qid, \"prediction_text\": generated_answer},\n",
    "        reference={\"id\": qid, \"answers\": {\"text\": [expected], \"answer_start\": [0]}}\n",
    "    )\n",
    "\n",
    "# === Compute metrics ===\n",
    "results = squad_metric.compute()\n",
    "em_score = results[\"exact_match\"]\n",
    "f1_score = results[\"f1\"]\n",
    "avg_latency = sum(latencies) / len(latencies)\n",
    "p95_latency = sorted(latencies)[int(0.95 * len(latencies))]\n",
    "throughput = len(qa_data) / sum(latencies)\n",
    "token_throughput = total_tokens / sum(latencies)\n",
    "\n",
    "# === Print results ===\n",
    "print(f\"\\n--- ONNX Benchmark Results (n={len(qa_data)}) ---\")\n",
    "print(f\"Accuracy (EM):           {em_score:.2f}\")\n",
    "print(f\"F1 Score:                {f1_score:.2f}\")\n",
    "print(f\"Avg Latency:             {avg_latency:.3f} sec\")\n",
    "print(f\"P95 Latency:             {p95_latency:.3f} sec\")\n",
    "print(f\"Throughput:              {throughput:.2f} samples/sec\")\n",
    "print(f\"Token Throughput:        {token_throughput:.2f} tokens/sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e0a37c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
